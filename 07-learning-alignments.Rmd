# Learning alignments, an interesting perspective

Recently, machine learning methods have been increasingly applied to the process of alignment. Learning an alignment method through machine learning could result in methods with less design biases and with data-driven insights.

## Deep learning and sequences

As many of these techniques are based on deep learning, which I did not introduce in Chapter \@ref(learning-from-sequences-and-alignments), I will first introduce deep learning very shortly. I will then introduce the concept of learned sequence embeddings which have become very useful for machine sequence alignment.

### Intro to deep learning

Deep learning is the process of learning using neural networks. Neural all started in 1958 when Rosenblatt proposed the *perceptron* [@rosenblattPerceptronProbabilisticModel1958]. This learning algorithm was loosely inspired by biological neurons, which led to the name: *neural networks.* The perceptron takes as input $n$ values, these are used in a weighted sum that is then fed through an *activation function.* The output of this function is the output of the perceptron. Originally, to replicate biological neurons, the activation function was a step function where, the perceptron has an output only if the weighted sum crosses a given threshold. This structure is often represented through a computational graph like in Figure \@ref(fig:perceptron). By tweaking the weights of the inputs, the perceptron can be used to solve linear separation problems.

```{r, perceptronCap}
perceptronCaption <- "**Computational graph of a perceptron.**  
Here, $n$ inputs are passed into the perceptron where they are summed, weighted by $w_1,\\ldots,w_n$. This sum is then fed through the perceptron's activation function (here a binary step function) which is the output of the perceptron. Often the sum is implicitely considered part of the activation function."
```

```{r, perceptron, label="perceptron", fig.cap=perceptronCaption, eval=knitr::is_html_output(), out.width="60%"}
knitr::include_graphics("figures/Learn-alignments/perceptron.png")
```

```{=tex}
\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{figures/Learn-alignments/perceptron.png}
  \extcaption{Computational graph of a perceptron.}{
Here, $n$ inputs are passed into the perceptron where they are summed, weighted by $w_1,\ldots,w_n$. This sum is then fed through the perceptron's activation function (here a binary step function) which is the output of the perceptron. Often the sum is implicitely considered part of the activation function.}
  \label{fig:perceptron}
\end{figure}
```
While the perceptron could be useful, some other methods could solve more complex problems. However it was discovered that by linking several perceptrons together, as in a biological brain, some complex problems could also be solved. These structure, called *multilayer perceptrons* (MLP), are organized in layers, where the outputs of perceptrons on a layer are used as inputs by perceptrons is the next layer (c.f. Figure \@ref(fig:mlp)). The perceptrons, when in this form, are often called *neurons*, and the MLP a *neural network* (NN). These neural networks are organized in layers, with an input and output layer on either end, and hidden layers in the middle. With the large number of weights to tune, these models were very difficult to train and therefore not practically useful.

```{r, mlpCap}
mlpCaption <- "**Computational graph of a multilayer perceptron.**  
This MLP, also called feedforward neural network, has $n$ inputs represented as the input layer, 2 hidden layers of 3 neurons each and an output layer of 2 neurons (e.g. suitable to binary classification). It is fully connected meaning that each node of a given layer is used as input for every neuron of the following layer. Each edge in this graph is paired to a weight, these are the tunable parameters during the training process."
```

```{r, mlp, label="mlp", eval=knitr::is_html_output(), fig.cap=mlpCaption, out.width="60%"}
knitr::include_graphics("figures/Learn-alignments/mlp.png")
```

```{=tex}
\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{figures/Learn-alignments/mlp.png}
  \extcaption{Computational graph of a multilayer perceptron.}{  
This MLP, also called feedforward neural network, has $n$ inputs represented as the input layer, 2 hidden layers of 3 neurons each and an output layer of 2 neurons (e.g. suitable to binary classification). It is fully connected meaning that each node of a given layer is used as input for every neuron of the following layer. Each edge in this graph is paired to a weight, these are the tunable parameters during the training process.}
  \label{fig:mlp}
\end{figure}
```
There was a great resurgence of these models in the nineties due to the invention of *backpropagation* [@rumelhartLearningRepresentationsBackpropagating1986]. By replacing the step functions of neurons with continuous, differentiable activation functions like sigmoid or hyperbolic tangent functions. With backpropagation, a gradient of the output could be computed w.r.t to each weight, enabling gradient descent procedures for automatically learning the optimal weights from data as described in Section \@ref(supervised-learning). With this, method, neural networks could be efficiently trained on complex classification and regression problems [@murtaghMultilayerPerceptronsClassification1991]. It was also proven that with hidden layers, neural networks are universal function approximators [@cybenkoApproximationSuperpositionsSigmoidal1989; @hornikMultilayerFeedforwardNetworks1989; @hornikApproximationCapabilitiesMultilayer1991a], suitable for all types of tasks. One notable caveat for neural networks is that, due to the large amount of weights to tune, they require large amounts of training data, which also explained their low usage before the internet and resulting data collection.

In the following years, neural networks saw an explosion in usage, with more complex architectures like convolutional neural networks (CNN) achieving state of the art result in computer vision tasks [@lecunBackpropagationAppliedHandwritten1989; @lecunGradientbasedLearningApplied1998]. By representing an input variable as a linear combination of its neighbors some form of contextual information can be passed to the NN and improve performance. CNNs can also have good results in non computer-vision tasks like: drug resistance prediction [@steinerDrugResistancePrediction2020a], protein subcellular localization [@weiPredictionHumanProtein2018], or epidemiological model parameter estimation [@voznicaDeepLearningPhylogenies2022].

More recently, as computational power and the amount of training data grew, larger and deeper (i.e. more hidden layers) architecture were able to be trained and achieved state of the art performance in many fields: image recognition with deep CNNs like `Alexnet` [@krizhevskyImageNetClassificationDeep2017] or `Resnet` [@heDeepResidualLearning2016], translation with Recurrent NNs [@bahdanauNeuralMachineTranslation2016] and Transformers [@vaswaniAttentionAllYou2017] (more on that in Section \@ref(learned-sequence-embeddings)) or protein structure prediction with `Alphafold2` [@jumperHighlyAccurateProtein2021]

### Learned sequence embeddings

An area that in which deep learning has recently proved particularly useful is the creation relevant learned embeddings. These embeddings, similarly to the encodings discussed in Section \@ref(preprocessing-the-alignment-for-machine-learning), transform a sequence of tokens in a numerical vector which can then be used in other machine learning tasks. By learning these embeddings, the hope is that the resulting vector will retain the most important information in the sequence and keep some context.

#### $x$-`2vec`

Learned embeddings were principally developed in the field of natural language processing (NLP), where machine learning algorithms use text in languages such as English or French as input. In these contexts, simple encodings like OHE are not very practical because of the very high dimensionality of a language. For example, Merriam-Webster English dictionary contains 470,000 words [@HowManyWords] so to encode a single word as a One-Hot encoded vector would result in a 470,000-dimensional sparse vector. Encoding a whole text or even a single sentence is wildly unpractical. Therefore, as a field, NLP needed to come up with ways of efficiently representing words in lower-dimensional vectors than naive encoding methods, while retaining semantic meaning.

One of the early methods for sequence embedding was called `word2vec` [@mikolovEfficientEstimationWord2013; @mikolovDistributedRepresentationsWords2013], proposed by researchers at Google, that learns an word-embedding method on a particular text corpus. This method is designed to make embeddings that contain semantically relevant information, and example of the article is that the vector corresponding to $vec(Madrid) - vec(Spain)$ should be very similar to the vector $vec(Paris)$, and that similar words result in similar vectors.

The way this method works is by considering a word within its context, i.e. a window of length $k$ centered around the word. In a corpus of words (i.e. our training data), each word is encoded as a One Hot Vector, which is possible since the corpus contains only a subset of the words in the English language. A neural is then trained on one one of two tasks [@goldbergWord2vecExplainedDeriving2014]:

-   Continuous Bag of words: where the word is predicted given the context of the word as input

-   Skip-gram: where the context is predicted given the encoded word vector

After having sufficiently trained the neural network on the corpus on one of these tasks, one of the hidden layers of the network can be extracted and used as a vector representation of the input word, this results in an embedding method that is specific to a given corpus and the embedded vectors can be used in downstream learning tasks.

`word2vec` was very successful and widely used in the field of NLP, it is perhaps no surprise that the ideas behind it were adapted and reused in the field of bioinformatics. `dna2vec` [@ngDna2vecConsistentVector2017] uses similar ideas and was used to embed $k$-mers, and predict methylation sites on DNA sequences [@liangHyb4mCHybridDNA2vecbased2022]. Similar embedding methods like `seq2vec` [@kimothiDistributedRepresentationsBiological2016] as well as `bioVec` (including the protein specific `protVec`) [@asgariContinuousDistributedRepresentation2015] were also developed to embed whole biological sequences. They were successfully used in biological sequence classification problems [@kimothiMetricLearningBiological2017].

#### The attention revolution

While `word2vec` was widely used for many NLP tasks where word embeddings were needed, a lot of interesting developments on word embeddings were made in the field of automated machine translation. In this application, the desired embedding characteristics are slightly different. While semantic relevance is useful, in machine translation the embedding method needs to be able to capture dependencies, e.g. within a sentence where the link between the subject and the verb must be captured even though they are not necessarily next to each other. This was initially done by using recurrent neural networks, but they were hard to train and had trouble properly capturing long-range dependencies [@songPretrainingModelBiological2021].

One of the most successful methods developed for this task it the transformer [@vaswaniAttentionAllYou2017], also created by Google researchers. The main mechanisms of the transformer is the *self-attention* mechanisms: each input token, usually encoded as a One-Hot vector, is represented as a weighted sum of all the other tokens in a sequence *(here a token is a word and the sequence is a sentence)*. The weights of this sum are trained along with the rest of this network. by stacking several of these self-attention blocks, transformers can learn to represent and leverage long-range dependencies. This mechanism, and the transformer in general have had very successful application in machine translation, while being easier to train than recurrent networks [@wangProgressMachineTranslation2021].

This architecture was used to create very large pre-trained language models, that is to say models that perform word embedding. These models like `GPT-3` [@brownLanguageModelsAre2020] or `BERT` [@devlinBERTPretrainingDeep2019] are huge, with millions or even billions of learned weights, and have been trained on huge quantities of data in order to produce word embeddings useful in a wide range of contexts. `BERT` was trained using Masked Language Modelling (MLM), where some percentage of the tokens *(words)* in an input sequence *(sentence)* are replaced by a special `[MASK]` token, and the model is trained to predict the whole sentence, effectively guessing what tokens are missing based on the context of the whole sequence. This process allows the model to learn relevant dependencies between tokens in the training data.

As was the case with `word2vec`, these methods have been adapted to bioinformatics tasks with state of the art results, proving the versatility of the transformer model. Several *protein language models* similar to `BERT` were trained on various training sets of protein data. Some of these are `ProGen` [@madaniProGenLanguageModeling2020], `ProGen2` [@eriknijkampProGen2ExploringBoundaries2022] and `ProtBERT` [@elnaggarProtTransCrackingLanguage2021]. These large protein language models have been studied and interesting properties have been observed [@beplerLearningProteinLanguage2021], and some specific characteristics of proteins can be inferred from these models without specifying them in the training step. For example, protein language models seem to learn some information about the protein structure and attention maps can be used to infer residue contact maps [@raoTransformerProteinLanguage2020; @rivesBiologicalStructureFunction2019; @bhattacharyaSingleLayersAttention2020]. Similarly these models capture some information about protein function [@huExploringEvolutionbasedFree2022], mutational effects [@meierLanguageModelsEnable2021], evolutionary characteristics [@hieEvolutionaryVelocityProtein2022] and can even be used to generate new protein with desired properties [@madaniProGenLanguageModeling2020]. Some large language models have also been trained on DNA sequences like `DNABert` [@jiDNABERTPretrainedBidirectional2021] and also seem to capture some information without explicit specification during training, like variant effects [@benegasDNALanguageModels2022].

While, these protein language models have shown very useful for embedding sequences, some developments have been made to embed multiple sequence alignments as learning inputs. In some cases this is done by including information on the alignment in the tokens and then using a regular language model to embed them [@caiGenomewidePredictionSmall2020]. In the case of the MSA transformer [@raoMSATransformer2021], the attention mechanism was extended to include a weighted sum between aligned sequences effectively taking the alignment into account when embedding sequences. An attention-like mechanism was also used to train a protein structural model directly on MSAs [@sercuNeuralPottsModel2021]. Similarly, by pre-training language models on profiles derived from MSAs, some information about the alignment can also be included in the resulting embeddings [@sturmfelsProfilePredictionAlignmentBased2020]. Finally aligned sequences can be used as inputs in a regular transformer as was done `DeepConsensus` [@baidDeepConsensusImprovesAccuracy2022], a transformer-based polisher to improve PacBio HiFi reads even further. Finally the EvoFormer model included in `AlphaFold2` [@jumperHighlyAccurateProtein2021], which embeds MSAs to predict protein structure, is partly responsible for the leap in performance between the two generations of the `AlphaFold` model.

It is important to note that while these transformer models are very powerful and useful in practice, their complexity and size makes it very hard to study and understand what the model actually learns. There is work to peer inside this "black box", notably by interpreting the learn attention maps [@vigBERTologyMeetsBiology2021] to decipher biologically relevant information contained within.

## Learning sequence alignment

### Predicting a substitution matrix

One approach is to learn a substitution matrix (specific position/position scoring matrix) and plug it in a differentiable SW or NW algorithm (for end-to-end learning):

-   SAdLSA [@gaoNovelSequenceAlignment2021]

    -   sequences are encoded as PSI blast profiles

    -   Fed through a deep CNN

    -   predict a scoring matrix

    -   No differentiable alignment algorithm -\> cross entropy between alignment and structural alignment.

-   DeepBLAST [@mortonProteinStructuralAlignments2020]

    -   Embed sequences with LSTM-based language model (trained on PFAM)

    -   predict substitution/gap score

    -   differentiable NW (not to learn parameters but only to backpropagate the error)

-   DEDAL [@llinares-lopezDeepEmbeddingAlignment2022]

    -   2 seqs are embedded with encoder-only transformer

    -   Trained on TPUs with a fast differentiable algo (SW ?)

    -   Training set: parwise alignments extracted from PFAM

    -   predicts substitution, gap open and extend scoring matrices (position per position)

    -   improves alignment for remote homologies

-   The Learned Alignement module [@pettiEndtoendLearningMultiple2022]

    -   Learns a "context specific scoring matrix", i.e. a 20x20 matrix for a window around a given position.

    -   Differentiable SW

    -   Uses convolutional NN

    -   Learns an "MSA", actually outputs all to one pairwise alignments.

    -   Used to as plugin to alphafold2 and improved some metrics

-   Prediction of PSSM with RNN + LSTM [@guoComprehensiveStudyEnhancing2021] , not directly used on alignment but structure prediction.

### predicting an alignment

-   BetaAlign [@dotanHarnessingMachineTranslation2022]

    -   Unaligned sequences = a "language"

    -   Aligned sequences = another "language"

    -   So use transformers to translate one into the other (tried several ways to represent these "languages")

    -   Tested with both DNA and protein

    -   limitations:

        -   length of sequences

        -   Training / testing set

-   Another direction could be to predict the state from 2 residues (like a PSSM but directly match/indel).

<!-- -->

-   To counter the space limitations (i.e. sequence length limitations) induced by attention, other types of transformers used:

    -   with linear scale attention maps not quadratic [@choromanskiMaskedLanguageModeling2020]

    -   single layer attention or factored attention [@bhattacharyaInterpretingPottsTransformer2021] which lowers the number of parameters to estimate but keeps relevant information.

-   This is not a problem limited to bioinformatics, other fields have tried to come up with solutions:

    -   adaptive attention span [@sukhbaatarAdaptiveAttentionSpan2019]

    -   Long-Short range attention [@wuLiteTransformerLongShort2020]

    -   sparse transformers [@childGeneratingLongSequences2019; @correiaAdaptivelySparseTransformers2019]

    -   Reformer replace dot product to reduce memory from quadratic to linear [@kitaevReformerEfficientTransformer2020]

For learning DNA alignment several challenges:

-   longer sequences

-   less information in a single residue than a single nucleotide

-   In mapping size discrepancy between sequences.

### Learning seeds

Some work has been done in learning seeding procedures

-   Learn index structures on a specific reference (not necessarily DL), although fairly recent developments already thought of in 2018 [@kraskaCaseLearnedIndex2018]:

    -   BWA-MEME [@jungBWAMEMEBWAMEMEmulated2022] predicts the position in a suffix array, lowering query time and no need to compute the whole suffix array

    -   Sapling [@kirscheSaplingAcceleratingSuffix2021] same thing

    -   LISA [@hoLISALearnedIndexes2021] predict position in FM-index

<!-- -->

-   DeepMinimizer [@hoangDifferentiableLearningSequenceSpecific2022]:

    -   Train a neural network to select minimizers

    -   Results in a better density, seeds are spread out evenly accross sequences

-   Select candidate alignment sites in mRNA-miRNA pairs with DL: TargetNet [@minTargetNetFunctionalMicroRNA2022]

Final note, we could also learn a pre-processing function as in Chapter \@ref(HPC-paper) in an end to end fashion: either by learning the connections in MSRs or by learning transformations with sequence to sequence models (like transformers). This is still a little abstract and would need a differentiable read mapping algo for end-to-end learning (SW is possible but seeding -\> deepminimizer ?).

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

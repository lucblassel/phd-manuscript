# Learning alignments, an interesting perspective

## Learning sequence embeddings

-   (Variational) Auto-encoders:

    -   Bottlneck in deep neural neck, task is to predict input. Add noise in the hidden layers -\> remove noise or regularize to have smooth latent space and get embeddings
    -   Used for ancestral sequence reconstruction [@moretaAncestralProteinSequence2022] and estimating evolutionary distances [@corsoNeuralDistanceEmbeddings2021]
    -   VAEs used for sequence design as well [@wuProteinSequenceDesign2021; @stantonAcceleratingBayesianOptimization2022]

NLP:

-   From the field of natural language processing where very high dimensionality (470,000 words in the Merriam-Webster English dictionary [@HowManyWords], so naive one hot is out of the question), we need other ways to transform words into sequences.

-   Method of pre-training embedding methods

-   Word2Vec derivatives:

    -   word2Vec [@mikolovEfficientEstimationWord2013; @mikolovDistributedRepresentationsWords2013], take in a large corpus of text and learns a vector space from it. Then each word in the corpus can be assigned a vector, constraints mean that similar words have similar vectors (i.e. low distance in the vector space). And that the embeddings make sense grammatically (e.g. of the Paper $vec(Madrid) - vec(Spain)$ should be close to $vec(Paris)$ in the learned space.

        -   Context of a word = window of $k$ words centered around it

        -   The model is a neural network and the hidden layer corresponds to the embedding (similar to auto-encoders)

        -   2 ways to train it [@goldbergWord2vecExplainedDeriving2014]:

            -   CBOW (continuous bag of words) = predict word from context

            -   skip-gram = predict context from word

    -   dna2vec [@ngDna2vecConsistentVector2017]

        -   Used to predict methylation sites [@liangHyb4mCHybridDNA2vecbased2022]

    -   seq2vec [@kimothiDistributedRepresentationsBiological2016]

    -   BioVec/ProtVec/GeneVec [@asgariContinuousDistributedRepresentation2015]

        -   Seq2vec and ProtVec both used in classification [@kimothiMetricLearningBiological2017]

-   Transformers / NN-based language models:

    -   Also from NLP, more recent development,

        -   Some have seen a lot of success like BERT [@devlinBERTPretrainingDeep2019] and GPT-3 [@brownLanguageModelsAre2020]
        -   Based on the very popular Transformer architecture [@vaswaniAttentionAllYou2017], with attention maps. Embed features as a linear weighted sum of other features (learn weights).
        -   Allows for long range dependencies to be captured efficiently
        -   LLMs trained with MLM
        -   Replaced methods based on RNNs / LSTMs which have trouble capturing long range dependencies [@songPretrainingModelBiological2021].

    -   protein language models have been developed from this with the same idea.

        -   ProGen [@madaniProGenLanguageModeling2020] and ProGen2 [@eriknijkampProGen2ExploringBoundaries2022]

        -   ProtBERT [@elnaggarProtTransCrackingLanguage2021]

        -   DNABert [@jiDNABERTPretrainedBidirectional2021]

        -   They have interesting properties [@beplerLearningProteinLanguage2021]:

            -   Intuitively learn structure of proteins [@raoTransformerProteinLanguage2020; @rivesBiologicalStructureFunction2019]

            -   Learn mutational effects [@meierLanguageModelsEnable2021]

            -   Evolutionary characteristics [@hieEvolutionaryVelocityProtein2022]

        -   To counter the space limitations (i.e. sequence length limitations) induced by attention, other types of transformers used, with linear scale attention maps not quadratic [@choromanskiMaskedLanguageModeling2020]

    -   Include information from MSA directly in embedding[@caiGenomewidePredictionSmall2020]: transform aligned sequence in to tokens -\> use ALBERT to embed tokens

    -   MSA Transformer [@raoMSATransformer2021] that extends attention to include aligned residues from an input MSA as well.

        -   Similarly: learn on profiles derived from MSAs [@sturmfelsProfilePredictionAlignmentBased2020] as a pre-training task for protein language models

        -   Learn a protein structure model (potts model) directly on the MSA with a mechanism similar to attention [@sercuNeuralPottsModel2021]

-   Powerful but hard to interpret what the model actually learns. i.e. "black box" but some work is being done to interpret attention maps [@vigBERTologyMeetsBiology2021]

## Learning pairwise alignment

### DEDAL

-   reference to transformer embedding

-   Predict substitution matrix

-   Reference other similar works

-   drawback: only on proteins

### predicting an alignment

-   Transformer models can also predict tokens -\> predict "CIGAR string" or a an aligned sequence.

-   Challenges:

    -   Longer sequences in DNA

    -   Size difference in the case of mapping

    -   Less information in a single nucleotide token than in proteins....

## What else could we learn ?

### Learn to predict seeds or starting positions

-   DeepMinimizer

-   predict start position given a pair of sequences

### Learn pre-processing functions

i.e. either connections in MSR graph or sequence 2 sequence models

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

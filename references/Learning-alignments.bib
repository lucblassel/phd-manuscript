
@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2022-09-23},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf;/Users/lucblassel/Zotero/storage/NHZ2KVKD/1409.html}
}

@article{baidDeepConsensusImprovesAccuracy2022,
  title = {{{DeepConsensus}} Improves the Accuracy of Sequences with a Gap-Aware Sequence Transformer},
  author = {Baid, Gunjan and Cook, Daniel E. and Shafin, Kishwar and Yun, Taedong and Llinares-López, Felipe and Berthet, Quentin and Belyaeva, Anastasiya and Töpfer, Armin and Wenger, Aaron M. and Rowell, William J. and Yang, Howard and Kolesnikov, Alexey and Ammar, Waleed and Vert, Jean-Philippe and Vaswani, Ashish and McLean, Cory Y. and Nattestad, Maria and Chang, Pi-Chuan and Carroll, Andrew},
  date = {2022-09-01},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/s41587-022-01435-7},
  url = {https://www.nature.com/articles/s41587-022-01435-7},
  urldate = {2022-09-26},
  abstract = {Circular consensus sequencing with Pacific Biosciences (PacBio) technology generates long (10–25\,kilobases), accurate ‘HiFi’ reads by combining serial observations of a DNA molecule into a consensus sequence. The standard approach to consensus generation, pbccs, uses a hidden Markov model. We introduce DeepConsensus, which uses an alignment-based loss to train a gap-aware transformer–encoder for sequence correction. Compared to pbccs, DeepConsensus reduces read errors by 42\%. This increases the yield of PacBio HiFi reads at Q20 by 9\%, at Q30 by 27\% and at Q40 by 90\%. With two SMRT Cells of HG003, reads from DeepConsensus improve hifiasm assembly contiguity (NG50 4.9\,megabases (Mb) to 17.2\,Mb), increase gene completeness (94\% to 97\%), reduce the false gene duplication rate (1.1\% to 0.5\%), improve assembly base accuracy (Q43 to Q45) and reduce variant-calling errors by 24\%. DeepConsensus models could be trained to the general problem of analyzing the alignment of other types of sequences, such as unique molecular identifiers or genome assemblies.},
  langid = {english},
  keywords = {Genome assembly algorithms,Machine learning,Software},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Baid et al_2022_DeepConsensus improves the accuracy of sequences with a gap-aware sequence.pdf;/Users/lucblassel/Zotero/storage/TJNTPAGY/s41587-022-01435-7.html}
}

@misc{benegasDNALanguageModels2022,
  title = {{{DNA}} Language Models Are Powerful Zero-Shot Predictors of Non-Coding Variant Effects},
  author = {Benegas, Gonzalo and Batra, Sanjit Singh and Song, Yun S.},
  date = {2022-08-23},
  pages = {2022.08.22.504706},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.08.22.504706},
  url = {https://www.biorxiv.org/content/10.1101/2022.08.22.504706v1},
  urldate = {2022-09-26},
  abstract = {Variant effect prediction has traditionally focused on training supervised models on labeled data. Motivated by recent advances in natural language processing that have demonstrated substantial gains on diverse tasks by pre-training on large unlabeled data, however, unsupervised pre-training on massive databases of protein sequences has proven to be an effective approach to extracting complex information about proteins. Such models have been shown to learn variant effects in coding regions in a zero-shot manner. In a similar vein, we here introduce GPN (Genomic Pre-trained Network) which can learn variant effects in non-coding DNA using unsupervised pre-training on genomic DNA sequence alone. Our model is also able to learn gene structure and DNA motifs without any supervision. We demonstrate the utility of GPN by showing that it outperforms supervised deep learning models such as DeepSEA trained on vast amounts of functional genomics data in Arabidopsis thaliana, a model organism for plant biology. Additionally, GPN trained on a single genome outperforms popular conservation scores such as phyloP and PhastCons, which are computed using aligned genomes from multiple species and can be used to predict the pathogenicity of variants that perturb highly conserved positions. We provide code (https://github.com/songlab-cal/gpn) to train GPN for any given species using its DNA sequence alone and learn corresponding zero-shot variant effects genome-wide.},
  langid = {english},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Benegas et al_2022_DNA language models are powerful zero-shot predictors of non-coding variant.pdf;/Users/lucblassel/Zotero/storage/593HIIUI/2022.08.22.504706v1.html}
}

@misc{beplerLearningProteinSequence2019,
  title = {Learning Protein Sequence Embeddings Using Information from Structure},
  author = {Bepler, Tristan and Berger, Bonnie},
  date = {2019-10-16},
  number = {arXiv:1902.08661},
  eprint = {1902.08661},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.08661},
  url = {http://arxiv.org/abs/1902.08661},
  urldate = {2022-09-28},
  abstract = {Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings --- one per amino acid position --- that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrary-length sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Bepler_Berger_2019_Learning protein sequence embeddings using information from structure.pdf;/Users/lucblassel/Zotero/storage/QSRLYHGI/1902.html}
}

@article{bermanWorldwideProteinData2007,
  title = {The Worldwide {{Protein Data Bank}} ({{wwPDB}}): Ensuring a Single, Uniform Archive of {{PDB}} Data},
  shorttitle = {The Worldwide {{Protein Data Bank}} ({{wwPDB}})},
  author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki and Markley, John L.},
  date = {2007-01-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Research},
  volume = {35},
  pages = {D301-D303},
  issn = {0305-1048},
  doi = {10.1093/nar/gkl971},
  url = {https://doi.org/10.1093/nar/gkl971},
  urldate = {2022-09-28},
  abstract = {The worldwide Protein Data Bank (wwPDB) is the international collaboration that manages the deposition, processing and distribution of the PDB archive. The online PDB archive is a repository for the coordinates and related information for more than 38\,000 structures, including proteins, nucleic acids and large macromolecular complexes that have been determined using X-ray crystallography, NMR and electron microscopy techniques. The founding members of the wwPDB are RCSB PDB (USA), MSD-EBI (Europe) and PDBj (Japan) [H.M. Berman, K. Henrick and H. Nakamura (2003) Nature Struct. Biol., 10, 980]. The BMRB group (USA) joined the wwPDB in 2006. The mission of the wwPDB is to maintain a single archive of macromolecular structural data that are freely and publicly available to the global community. Additionally, the wwPDB provides a variety of services to a broad community of users. The wwPDB website at  provides information about services provided by the individual member organizations and about projects undertaken by the wwPDB.},
  issue = {suppl\_1},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Berman et al_2007_The worldwide Protein Data Bank (wwPDB).pdf;/Users/lucblassel/Zotero/storage/YRQR7YJS/1115659.html}
}

@incollection{bhattacharyaInterpretingPottsTransformer2021,
  title = {Interpreting {{Potts}} and {{Transformer Protein Models Through}} the {{Lens}} of {{Simplified Attention}}},
  booktitle = {Biocomputing 2022},
  author = {Bhattacharya, Nicholas and Thomas, Neil and Rao, Roshan and Dauparas, Justas and Koo, Peter K. and Baker, David and Song, Yun S. and Ovchinnikov, Sergey},
  date = {2021-09-21},
  pages = {34--45},
  publisher = {{WORLD SCIENTIFIC}},
  doi = {10.1142/9789811250477_0004},
  url = {https://www.worldscientific.com/doi/abs/10.1142/9789811250477_0004},
  urldate = {2022-09-26},
  isbn = {9789811250460},
  keywords = {Attention,BERT,Contact Prediction,Language Modeling,Markov Random Fields,Potts Models,Representation Learning,Self-supervised learning,Transformer},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Bhattacharya et al_2021_Interpreting Potts and Transformer Protein Models Through the Lens of.pdf}
}

@misc{bhattacharyaSingleLayersAttention2020,
  title = {Single {{Layers}} of {{Attention Suffice}} to {{Predict Protein Contacts}}},
  author = {Bhattacharya, Nicholas and Thomas, Neil and Rao, Roshan and Dauparas, Justas and Koo, Peter K. and Baker, David and Song, Yun S. and Ovchinnikov, Sergey},
  date = {2020-12-22},
  pages = {2020.12.21.423882},
  publisher = {{bioRxiv}},
  doi = {10.1101/2020.12.21.423882},
  url = {https://www.biorxiv.org/content/10.1101/2020.12.21.423882v2},
  urldate = {2022-09-26},
  abstract = {The established approach to unsupervised protein contact prediction estimates co-evolving positions using undirected graphical models. This approach trains a Potts model on a Multiple Sequence Alignment, then predicts that the edges with highest weight correspond to contacts in the 3D structure. On the other hand, increasingly large Transformers are being pretrained on protein sequence databases but have demonstrated mixed results for downstream tasks, including contact prediction. This has sparked discussion about the role of scale and attention-based models in unsupervised protein representation learning. We argue that attention is a principled model of protein interactions, grounded in real properties of protein family data. We introduce a simplified attention layer, factored attention, and show that it achieves comparable performance to Potts models, while sharing parameters both within and across families. Further, we extract contacts from the attention maps of a pretrained Transformer and show they perform competitively with the other two approaches. This provides evidence that large-scale pretraining can learn meaningful protein features when presented with unlabeled and unaligned data. We contrast factored attention with the Transformer to indicate that the Transformer leverages hierarchical signal in protein family databases not captured by our single-layer models. This raises the exciting possibility for the development of powerful structured models of protein family databases.1},
  langid = {english},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Bhattacharya et al_2020_Single Layers of Attention Suffice to Predict Protein Contacts.pdf;/Users/lucblassel/Zotero/storage/76S3SGU5/2020.12.21.html}
}

@misc{childGeneratingLongSequences2019,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  date = {2019-04-23},
  number = {arXiv:1904.10509},
  eprint = {1904.10509},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.10509},
  url = {http://arxiv.org/abs/1904.10509},
  urldate = {2022-09-26},
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Child et al_2019_Generating Long Sequences with Sparse Transformers.pdf;/Users/lucblassel/Zotero/storage/QBAA334P/1904.html}
}

@article{corpetMultipleSequenceAlignment1988,
  title = {Multiple Sequence Alignment with Hierarchical Clustering},
  author = {Corpet, Florence},
  date = {1988-11-25},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Research},
  volume = {16},
  number = {22},
  pages = {10881--10890},
  issn = {0305-1048},
  doi = {10.1093/nar/16.22.10881},
  url = {https://doi.org/10.1093/nar/16.22.10881},
  urldate = {2022-09-07},
  abstract = {An algorithm is presented for the multiple alignment of sequences, either proteins or nucleic acids, that is both accurate and easy to use on microcomputers. The approach is based on the conventional dynamic-programming method of pairwise alignment. Initially, a hierarchical clustering of the sequences is performed using the matrix of the pairwise alignment scores. The closest sequences are aligned creating groups of aligned sequences. Then close groups are aligned until all sequences are aligned in one group. The pairwise alignments included in the multiple alignment form a new matrix that is used to produce a hierarchical clustering. If it is different from the first one, iteration of the process can be performed. The method is illustrated by an example : a global alignment of 39 sequences of cytochrome c.},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Corpet_1988_Multiple sequence alignment with hierarchical clustering.pdf;/Users/lucblassel/Zotero/storage/ZRFD4TKR/2378678.html}
}

@inproceedings{correiaAdaptivelySparseTransformers2019,
  title = {Adaptively {{Sparse Transformers}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Correia, Gonçalo M. and Niculae, Vlad and Martins, André F. T.},
  date = {2019-11},
  pages = {2174--2184},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1223},
  url = {https://aclanthology.org/D19-1223},
  urldate = {2022-09-26},
  abstract = {Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter – which controls the shape and sparsity of alpha-entmax – allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.},
  eventtitle = {{{EMNLP-IJCNLP}} 2019},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Correia et al_2019_Adaptively Sparse Transformers.pdf}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12-01},
  journaltitle = {Mathematics of Control, Signals and Systems},
  shortjournal = {Math. Control Signal Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  url = {https://doi.org/10.1007/BF02551274},
  urldate = {2022-09-23},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english},
  keywords = {Approximation,Completeness,Neural networks},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Cybenko_1989_Approximation by superpositions of a sigmoidal function.pdf}
}

@misc{dotanHarnessingMachineTranslation2022,
  title = {Harnessing Machine Translation Methods for Sequence Alignment},
  author = {Dotan, Edo and Belinkov, Yonatan and Avram, Oren and Wygoda, Elya and Ecker, Noa and Alburquerque, Michael and Keren, Omri and Loewenthal, Gil and Pupko, Tal},
  date = {2022-07-23},
  pages = {2022.07.22.501063},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.07.22.501063},
  url = {https://www.biorxiv.org/content/10.1101/2022.07.22.501063v1},
  urldate = {2022-09-26},
  abstract = {The sequence alignment problem is one of the most fundamental problems in bioinformatics and a plethora of methods were devised to tackle it. Here we introduce BetaAlign, a novel methodology for aligning sequences using a natural language processing (NLP) approach. BetaAlign accounts for the possible variability of the evolutionary process among different datasets by using an ensemble of transformers, each trained on millions of samples generated from a different evolutionary model. Our approach leads to outstanding alignment accuracy, often outperforming commonly used methods, such as MAFFT, DIALIGN, ClustalW, T-Coffee, and MUSCLE. Notably, the utilization of deep-learning techniques for the sequence alignment problem brings additional advantages, such as automatic feature extraction that can be leveraged for a variety of downstream analysis tasks.},
  langid = {english},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Dotan et al_2022_Harnessing machine translation methods for sequence alignment.pdf;/Users/lucblassel/Zotero/storage/S4J8N429/2022.07.22.501063v1.html}
}

@article{gaoNovelSequenceAlignment2021,
  title = {A Novel Sequence Alignment Algorithm Based on Deep Learning of the Protein Folding Code},
  author = {Gao, Mu and Skolnick, Jeffrey},
  date = {2021-02-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {37},
  number = {4},
  pages = {490--496},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btaa810},
  url = {https://doi.org/10.1093/bioinformatics/btaa810},
  urldate = {2022-09-26},
  abstract = {From evolutionary interference, function annotation to structural prediction, protein sequence comparison has provided crucial biological insights. While many sequence alignment algorithms have been developed, existing approaches often cannot detect hidden structural relationships in the ‘twilight zone’ of low sequence identity. To address this critical problem, we introduce a computational algorithm that performs protein Sequence Alignments from deep-Learning of Structural Alignments (SAdLSA, silent ‘d’). The key idea is to implicitly learn the protein folding code from many thousands of structural alignments using experimentally determined protein structures.To demonstrate that the folding code was learned, we first show that SAdLSA trained on pure α-helical proteins successfully recognizes pairs of structurally related pure β-sheet protein domains. Subsequent training and benchmarking on larger, highly challenging datasets show significant improvement over established approaches. For challenging cases, SAdLSA is ∼150\% better than HHsearch for generating pairwise alignments and ∼50\% better for identifying the proteins with the best alignments in a sequence library. The time complexity of SAdLSA is O(N) thanks to GPU acceleration.Datasets and source codes of SAdLSA are available free of charge for academic users at http://sites.gatech.edu/cssb/sadlsa/.Supplementary data are available at Bioinformatics online.},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Gao_Skolnick_2021_A novel sequence alignment algorithm based on deep learning of the protein.pdf;/Users/lucblassel/Zotero/storage/KABZB3EH/5909984.html}
}

@article{guoComprehensiveStudyEnhancing2021,
  title = {Comprehensive {{Study}} on {{Enhancing Low-Quality Position-Specific Scoring Matrix}} with {{Deep Learning}} for {{Accurate Protein Structure Property Prediction}}: {{Using Bagging Multiple Sequence Alignment Learning}}},
  shorttitle = {Comprehensive {{Study}} on {{Enhancing Low-Quality Position-Specific Scoring Matrix}} with {{Deep Learning}} for {{Accurate Protein Structure Property Prediction}}},
  author = {Guo, Yuzhi and Wu, Jiaxiang and Ma, Hehuan and Wang, Sheng and Huang, Junzhou},
  date = {2021-04},
  journaltitle = {Journal of Computational Biology},
  volume = {28},
  number = {4},
  pages = {346--361},
  publisher = {{Mary Ann Liebert, Inc., publishers}},
  doi = {10.1089/cmb.2020.0416},
  url = {https://www.liebertpub.com/doi/10.1089/cmb.2020.0416},
  urldate = {2022-09-26},
  abstract = {Accurate predictions of protein structure properties, for example, secondary structure and solvent accessibility, are essential in analyzing the structure and function of a protein. Position-specific scoring matrix (PSSM) features are widely used in the structure property prediction. However, some proteins may have low-quality PSSM features due to insufficient homologous sequences, leading to limited prediction accuracy. To address this limitation, we propose an enhancing scheme for PSSM features. We introduce the “Bagging MSA” (multiple sequence alignment) method to calculate PSSM features used to train our model, adopt a convolutional network to capture local context features and bidirectional long short-term memory for long-term dependencies, and integrate them under an unsupervised framework. Structure property prediction models are then built upon such enhanced PSSM features for more accurate predictions. Moreover, we develop two frameworks to evaluate the effectiveness of the enhanced PSSM features, which also bring proposed method into real-world scenarios. Empirical evaluation of CB513, CASP11, and CASP12 data sets indicates that our unsupervised enhancing scheme indeed generates more informative PSSM features for structure property prediction.},
  keywords = {deep learning,enhancing PSSM,protein,protein solvent accessibility,secondary structure,unsupervised learning},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Guo et al_2021_Comprehensive Study on Enhancing Low-Quality Position-Specific Scoring Matrix.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2022-09-23},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/He et al_2016_Deep Residual Learning for Image Recognition.pdf;/Users/lucblassel/Zotero/storage/NWZXZAK9/He_Deep_Residual_Learning_CVPR_2016_paper.html}
}

@article{hoangDifferentiableLearningSequenceSpecific2022,
  title = {Differentiable {{Learning}} of {{Sequence-Specific Minimizer Schemes}} with {{DeepMinimizer}}},
  author = {Hoang, Minh and Zheng, Hongyu and Kingsford, Carl},
  date = {2022-09-12},
  journaltitle = {Journal of Computational Biology},
  publisher = {{Mary Ann Liebert, Inc., publishers}},
  doi = {10.1089/cmb.2022.0275},
  url = {https://www.liebertpub.com/doi/full/10.1089/cmb.2022.0275},
  urldate = {2022-09-26},
  abstract = {Minimizers are widely used to sample representative k-mers from biological sequences in many applications, such as read mapping and taxonomy prediction. In most scenarios, having the minimizer scheme select as few k-mer positions as possible (i.e., having a low density) is desirable to reduce computation and memory cost. Despite the growing interest in minimizers, learning an effective scheme with optimal density is still an open question, as it requires solving an apparently challenging discrete optimization problem on the permutation space of k-mer orderings. Most existing schemes are designed to work well in expectation over random sequences, which have limited applicability to many practical tools. On the other hand, several methods have been proposed to construct minimizer schemes for a specific target sequence. These methods, however, only approximate the original objective with likewise discrete surrogate tasks that are not able to significantly improve the density performance. This article introduces the first continuous relaxation of the density minimizing objective, DeepMinimizer, which employs a novel Deep Learning twin architecture to simultaneously ensure both validity and performance of the minimizer scheme. Our surrogate objective is fully differentiable and, therefore, amenable to efficient gradient-based optimization using GPU computing. Finally, we demonstrate that DeepMinimizer discovers minimizer schemes that significantly outperform state-of-the-art constructions on human genomic sequences.},
  keywords = {deep learning,optimization,sequence sketching},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Hoang et al_2022_Differentiable Learning of Sequence-Specific Minimizer Schemes with.pdf}
}

@misc{hoLISALearnedIndexes2021,
  title = {{{LISA}}: {{Learned Indexes}} for {{Sequence Analysis}}},
  shorttitle = {{{LISA}}},
  author = {Ho, Darryl and Kalikar, Saurabh and Misra, Sanchit and Ding, Jialin and Md, Vasimuddin and Tatbul, Nesime and Li, Heng and Kraska, Tim},
  date = {2021-07-13},
  pages = {2020.12.22.423964},
  publisher = {{bioRxiv}},
  doi = {10.1101/2020.12.22.423964},
  url = {https://www.biorxiv.org/content/10.1101/2020.12.22.423964v2},
  urldate = {2022-09-26},
  abstract = {Background Next-generation sequencing (NGS) technologies have enabled affordable sequencing of billions of short DNA fragments at high throughput, paving the way for population-scale genomics. Genomics data analytics at this scale requires overcoming performance bottlenecks, such as searching for short DNA sequences over long reference sequences. Results In this paper, we introduce LISA (Learned Indexes for Sequence Analysis), a novel learning-based approach to DNA sequence search. We focus on accelerating two of the most essential flavors of DNA sequence search—exact search and super-maximal exact match (SMEM) search. LISA builds on and extends FM-index, which is the state-of-the-art technique widely deployed in genomics tools. Experiments with human, animal, and plant genome datasets indicate that LISA achieves up to 2.2 and 10.8 speedups over the state-of-the-art FM-index based implementations for exact search and super-maximal exact match (SMEM) search, respectively. Code availability https://github.com/IntelLabs/Trans-Omics-Acceleration-Library/tree/master/LISA.},
  langid = {english},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Ho et al_2021_LISA.pdf;/Users/lucblassel/Zotero/storage/BMLL7E9C/2020.12.22.html}
}

@article{hornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  author = {Hornik, Kurt},
  date = {1991-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {4},
  number = {2},
  pages = {251--257},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(91)90009-T},
  url = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
  urldate = {2019-08-19},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
  keywords = {() approximation,Activation function,Input environment measure,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
  file = {/Users/lucblassel/Google Drive/Zotero_papers/hornik_1991_approximation_capabilities_of_multilayer.pdf;/Users/lucblassel/Zotero/storage/5V9NTA2N/089360809190009T.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2022-09-23},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  langid = {english},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Hornik et al_1989_Multilayer feedforward networks are universal approximators.pdf;/Users/lucblassel/Zotero/storage/QKS4R2RA/0893608089900208.html}
}

@misc{huExploringEvolutionbasedFree2022,
  title = {Exploring Evolution-Based \& -Free Protein Language Models as Protein Function Predictors},
  author = {Hu, Mingyang and Yuan, Fajie and Yang, Kevin K. and Ju, Fusong and Su, Jin and Wang, Hui and Yang, Fei and Ding, Qiuyang},
  date = {2022-06-13},
  number = {arXiv:2206.06583},
  eprint = {2206.06583},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.06583},
  url = {http://arxiv.org/abs/2206.06583},
  urldate = {2022-09-26},
  abstract = {Large-scale Protein Language Models (PLMs) have improved performance in protein prediction tasks, ranging from 3D structure prediction to various function predictions. In particular, AlphaFold, a ground-breaking AI system, could potentially reshape structural biology. However, the utility of the PLM module in AlphaFold, Evoformer, has not been explored beyond structure prediction. In this paper, we investigate the representation ability of three popular PLMs: ESM-1b (single sequence), MSA-Transformer (multiple sequence alignment) and Evoformer (structural), with a special focus on Evoformer. Specifically, we aim to answer the following key questions: (i) Does the Evoformer trained as part of AlphaFold produce representations amenable to predicting protein function? (ii) If yes, can Evoformer replace ESM-1b and MSA-Transformer? (iii) How much do these PLMs rely on evolution-related protein data? In this regard, are they complementary to each other? We compare these models by empirical study along with new insights and conclusions. Finally, we release code and datasets for reproducibility.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Quantitative Methods},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Hu et al_2022_Exploring evolution-based & -free protein language models as protein function.pdf;/Users/lucblassel/Zotero/storage/V64UKWL7/Hu et al. - 2022 - Exploring evolution-based & -free protein language.pdf;/Users/lucblassel/Zotero/storage/36KKNJPI/2206.html}
}

@article{jungBWAMEMEBWAMEMEmulated2022,
  title = {{{BWA-MEME}}: {{BWA-MEM}} Emulated with a Machine Learning Approach},
  shorttitle = {{{BWA-MEME}}},
  author = {Jung, Youngmok and Han, Dongsu},
  date = {2022-05-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {38},
  number = {9},
  pages = {2404--2413},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btac137},
  url = {https://doi.org/10.1093/bioinformatics/btac137},
  urldate = {2022-09-26},
  abstract = {The growing use of next-generation sequencing and enlarged sequencing throughput require efficient short-read alignment, where seeding is one of the major performance bottlenecks. The key challenge in the seeding phase is searching for exact matches of substrings of short reads in the reference DNA sequence. Existing algorithms, however, present limitations in performance due to their frequent memory accesses.This article presents BWA-MEME, the first full-fledged short read alignment software that leverages learned indices for solving the exact match search problem for efficient seeding. BWA-MEME is a practical and efficient seeding algorithm based on a suffix array search algorithm that solves the challenges in utilizing learned indices for SMEM search which is extensively used in the seeding phase. Our evaluation shows that BWA-MEME achieves up to 3.45× speedup in seeding throughput over BWA-MEM2 by reducing the number of instructions by 4.60×, memory accesses by 8.77× and LLC misses by 2.21×, while ensuring the identical SAM output to BWA-MEM2.The source code and test scripts are available for academic use at https://github.com/kaist-ina/BWA-MEME/.Supplementary data are available at Bioinformatics online.},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Jung_Han_2022_BWA-MEME.pdf;/Users/lucblassel/Zotero/storage/R3EMVIVC/6543607.html}
}

@article{kirscheSaplingAcceleratingSuffix2021,
  title = {Sapling: Accelerating Suffix Array Queries with Learned Data Models},
  shorttitle = {Sapling},
  author = {Kirsche, Melanie and Das, Arun and Schatz, Michael C},
  date = {2021-03-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {37},
  number = {6},
  pages = {744--749},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btaa911},
  url = {https://doi.org/10.1093/bioinformatics/btaa911},
  urldate = {2022-09-26},
  abstract = {As genomic data becomes more abundant, efficient algorithms and data structures for sequence alignment become increasingly important. The suffix array is a widely used data structure to accelerate alignment, but the binary search algorithm used to query, it requires widespread memory accesses, causing a large number of cache misses on large datasets.Here, we present Sapling, an algorithm for sequence alignment, which uses a learned data model to augment the suffix array and enable faster queries. We investigate different types of data models, providing an analysis of different neural network models as well as providing an open-source aligner with a compact, practical piecewise linear model. We show that Sapling outperforms both an optimized binary search approach and multiple widely used read aligners on a diverse collection of genomes, including human, bacteria and plants, speeding up the algorithm by more than a factor of two while adding \&lt;1\% to the suffix array’s memory footprint.The source code and tutorial are available open-source at https://github.com/mkirsche/sapling.Supplementary data are available at Bioinformatics online.},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Kirsche et al_2021_Sapling.pdf;/Users/lucblassel/Zotero/storage/4TU8JITV/5941464.html}
}

@misc{kitaevReformerEfficientTransformer2020,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  date = {2020-02-18},
  number = {arXiv:2001.04451},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.04451},
  url = {http://arxiv.org/abs/2001.04451},
  urldate = {2022-09-26},
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L\^2\$) to O(\$L\textbackslash log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Kitaev et al_2020_Reformer.pdf;/Users/lucblassel/Zotero/storage/R4M9CDPZ/2001.html}
}

@inproceedings{kraskaCaseLearnedIndex2018,
  title = {The {{Case}} for {{Learned Index Structures}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
  date = {2018-05-27},
  series = {{{SIGMOD}} '18},
  pages = {489--504},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3183713.3196909},
  url = {https://doi.org/10.1145/3183713.3196909},
  urldate = {2022-09-26},
  abstract = {Indexes are models: a \textbackslash btree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term \textbackslash em learned indexes. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show that our learned indexes can have significant advantages over traditional indexes. More importantly, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work provides just a glimpse of what might be possible.},
  isbn = {978-1-4503-4703-7},
  keywords = {b-tree,bloom-filter,cdf,hash-map,index structures,learned data structures,learned index,learned index structure,linear regression,mixture of experts,neural net},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Kraska et al_2018_The Case for Learned Index Structures.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782},
  doi = {10.1145/3065386},
  url = {https://doi.org/10.1145/3065386},
  urldate = {2022-09-23},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Krizhevsky et al_2017_ImageNet classification with deep convolutional neural networks.pdf}
}

@article{lecunBackpropagationAppliedHandwritten1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  date = {1989-12},
  journaltitle = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.4.541},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  eventtitle = {Neural {{Computation}}},
  file = {/Users/lucblassel/Zotero/storage/TFU34LRU/6795724.html}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Lecun et al_1998_Gradient-based learning applied to document recognition.pdf;/Users/lucblassel/Zotero/storage/IVH5GFHP/726791.html}
}

@inproceedings{liConvergenceAnalysisTwolayer2017,
  title = {Convergence {{Analysis}} of {{Two-layer Neural Networks}} with {{ReLU Activation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Yuanzhi and Yuan, Yang},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/a96b65a721e561e1e3de768ac819ffbb-Abstract.html},
  urldate = {2022-09-23},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Li_Yuan_2017_Convergence Analysis of Two-layer Neural Networks with ReLU Activation.pdf}
}

@misc{llinares-lopezDeepEmbeddingAlignment2022,
  title = {Deep Embedding and Alignment of Protein Sequences},
  author = {Llinares-López, Felipe and Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Vert, Jean-Philippe},
  date = {2022-07-01},
  pages = {2021.11.15.468653},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.11.15.468653},
  url = {https://www.biorxiv.org/content/10.1101/2021.11.15.468653v2},
  urldate = {2022-09-23},
  abstract = {Protein sequence alignment is a key component of most bioinformatics pipelines to study the structures and functions of proteins. Aligning highly divergent sequences remains, however, a difficult task that current algorithms often fail to perform accurately, leaving many proteins or open reading frames poorly annotated. Here, we leverage recent advances in deep learning for language modelling and differentiable programming to propose DEDAL, a flexible model to align protein sequences and detect homologs. DEDAL is a machine learning-based model that learns to align sequences by observing large datasets of raw protein sequences and of correct alignments. Once trained, we show that DEDAL improves by up to two- or three-fold the alignment correctness over existing methods on remote homologs, and better discriminates remote homologs from evolutionarily unrelated sequences, paving the way to improvements on many downstream tasks relying on sequence alignment in structural and functional genomics.},
  langid = {english},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Llinares-López et al_2022_Deep embedding and alignment of protein sequences.pdf;/Users/lucblassel/Zotero/storage/YDNDSENJ/2021.11.15.468653v2.html}
}

@article{minTargetNetFunctionalMicroRNA2022,
  title = {{{TargetNet}}: Functional {{microRNA}} Target Prediction with Deep Neural Networks},
  shorttitle = {{{TargetNet}}},
  author = {Min, Seonwoo and Lee, Byunghan and Yoon, Sungroh},
  date = {2022-02-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {38},
  number = {3},
  pages = {671--677},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btab733},
  url = {https://doi.org/10.1093/bioinformatics/btab733},
  urldate = {2022-09-26},
  abstract = {MicroRNAs (miRNAs) play pivotal roles in gene expression regulation by binding to target sites of messenger RNAs (mRNAs). While identifying functional targets of miRNAs is of utmost importance, their prediction remains a great challenge. Previous computational algorithms have major limitations. They use conservative candidate target site (CTS) selection criteria mainly focusing on canonical site types, rely on laborious and time-consuming manual feature extraction, and do not fully capitalize on the information underlying miRNA–CTS interactions.In this article, we introduce TargetNet, a novel deep learning-based algorithm for functional miRNA target prediction. To address the limitations of previous approaches, TargetNet has three key components: (i) relaxed CTS selection criteria accommodating irregularities in the seed region, (ii) a novel miRNA–CTS sequence encoding scheme incorporating extended seed region alignments and (iii) a deep residual network-based prediction model. The proposed model was trained with miRNA–CTS pair datasets and evaluated with miRNA–mRNA pair datasets. TargetNet advances the previous state-of-the-art algorithms used in functional miRNA target classification. Furthermore, it demonstrates great potential for distinguishing high-functional miRNA targets. The codes and pre-trained models are available at https://github.com/mswzeus/TargetNet.},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Min et al_2022_TargetNet.pdf;/Users/lucblassel/Zotero/storage/ZBNLLYYG/6408435.html}
}

@article{mistryPfamProteinFamilies2021,
  title = {Pfam: {{The}} Protein Families Database in 2021},
  shorttitle = {Pfam},
  author = {Mistry, Jaina and Chuguransky, Sara and Williams, Lowri and Qureshi, Matloob and Salazar, Gustavo~A and Sonnhammer, Erik L L and Tosatto, Silvio C E and Paladin, Lisanna and Raj, Shriya and Richardson, Lorna J and Finn, Robert D and Bateman, Alex},
  date = {2021-01-08},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Research},
  volume = {49},
  number = {D1},
  pages = {D412-D419},
  issn = {0305-1048},
  doi = {10.1093/nar/gkaa913},
  url = {https://doi.org/10.1093/nar/gkaa913},
  urldate = {2022-09-28},
  abstract = {The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Mistry et al_2021_Pfam.pdf;/Users/lucblassel/Zotero/storage/HQNMUCNU/5943818.html}
}

@misc{mortonProteinStructuralAlignments2020,
  title = {Protein {{Structural Alignments From Sequence}}},
  author = {Morton, James T. and Strauss, Charlie E. M. and Blackwell, Robert and Berenberg, Daniel and Gligorijevic, Vladimir and Bonneau, Richard},
  date = {2020-11-04},
  pages = {2020.11.03.365932},
  publisher = {{bioRxiv}},
  doi = {10.1101/2020.11.03.365932},
  url = {https://www.biorxiv.org/content/10.1101/2020.11.03.365932v1},
  urldate = {2022-09-26},
  abstract = {Computing sequence similarity is a fundamental task in biology, with alignment forming the basis for the annotation of genes and genomes and providing the core data structures for evolutionary analysis. Standard approaches are a mainstay of modern molecular biology and rely on variations of edit distance to obtain explicit alignments between pairs of biological sequences. However, sequence alignment algorithms struggle with remote homology tasks and cannot identify similarities between many pairs of proteins with similar structures and likely homology. Recent work suggests that using machine learning language models can improve remote homology detection. To this end, we introduce DeepBLAST, that obtains explicit alignments from residue embeddings learned from a protein language model integrated into an end-to-end differentiable alignment framework. This approach can be accelerated on the GPU architectures and outperforms conventional sequence alignment techniques in terms of both speed and accuracy when identifying structurally similar proteins.},
  langid = {english},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Morton et al_2020_Protein Structural Alignments From Sequence.pdf;/Users/lucblassel/Zotero/storage/E5UZQ6M3/2020.11.03.365932v1.html}
}

@article{murtaghMultilayerPerceptronsClassification1991,
  title = {Multilayer Perceptrons for Classification and Regression},
  author = {Murtagh, Fionn},
  date = {1991-07-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {2},
  number = {5},
  pages = {183--197},
  issn = {0925-2312},
  doi = {10.1016/0925-2312(91)90023-5},
  url = {https://www.sciencedirect.com/science/article/pii/0925231291900235},
  urldate = {2022-09-23},
  abstract = {We review the theory and practice of the multilayer perceptron. We aim at addressing a range of issues which are important from the point of view of applying this approach to practical problems. A number of examples are given, illustrating how the multilayer perceptron compares to alternative, conventional approaches. The application fields of classification and regression are especially considered. Questions of implementation, i.e. of multilayer perceptron architecture, dynamics, and related aspects, are discussed. Recent studies, which are particularly relevant to the areas of discriminant analysis, and function mapping, are cited.},
  langid = {english},
  keywords = {discriminant analysis,function approximation,Multilayer perceptron,regression,supervised classification},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Murtagh_1991_Multilayer perceptrons for classification and regression.pdf;/Users/lucblassel/Zotero/storage/V3P8ISHG/0925231291900235.html}
}

@article{ourmazdStructuralBiologySolved2022,
  title = {Structural Biology Is Solved — Now What?},
  author = {Ourmazd, Abbas and Moffat, Keith and Lattman, Eaton Edward},
  date = {2022-01},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {19},
  number = {1},
  pages = {24--26},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01357-3},
  url = {https://www.nature.com/articles/s41592-021-01357-3},
  urldate = {2022-09-29},
  abstract = {The splendid computational success of AlphaFold and RoseTTAFold in solving the 60-year-old problem of protein folding raises an obvious question: what new avenues should structural biology explore? We propose a strong pivot toward the goal of reading mechanism and function directly from the amino acid sequence. This ambitious goal will require new data analytical tools and an extensive database of the atomic-level structural trajectories traced out on energy landscapes as proteins perform their function.},
  issue = {1},
  langid = {english},
  keywords = {Protein function predictions,Protein structure predictions,Proteins},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Ourmazd et al_2022_Structural biology is solved — now what.pdf;/Users/lucblassel/Zotero/storage/A6GCKZDW/s41592-021-01357-3.html}
}

@misc{pettiEndtoendLearningMultiple2022,
  title = {End-to-End Learning of Multiple Sequence Alignments with Differentiable {{Smith-Waterman}}},
  author = {Petti, Samantha and Bhattacharya, Nicholas and Rao, Roshan and Dauparas, Justas and Thomas, Neil and Zhou, Juannan and Rush, Alexander M. and Koo, Peter K. and Ovchinnikov, Sergey},
  date = {2022-04-18},
  pages = {2021.10.23.465204},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.10.23.465204},
  url = {https://www.biorxiv.org/content/10.1101/2021.10.23.465204v2},
  urldate = {2022-09-07},
  abstract = {Multiple Sequence Alignments (MSAs) of homologous sequences contain information on structural and functional constraints and their evolutionary histories. Despite their importance for many downstream tasks, such as structure prediction, MSA generation is often treated as a separate pre-processing step, without any guidance from the application it will be used for. Here, we implement a smooth and differentiable version of the Smith-Waterman pairwise alignment algorithm that enables jointly learning an MSA and a downstream machine learning system in an end-to-end fashion. To demonstrate its utility, we introduce SMURF (Smooth Markov Unaligned Random Field), a new method that jointly learns an alignment and the parameters of a Markov Random Field for unsupervised contact prediction. We find that SMURF learns MSAs that mildly improve contact prediction on a diverse set of protein and RNA families. As a proof of concept, we demonstrate that by connecting our differentiable alignment module to AlphaFold and maximizing predicted confidence, we can learn MSAs that improve structure predictions over the initial MSAs. Interestingly, the alignments that improve AlphaFold predictions are self-inconsistent and can be viewed as adversarial. This work highlights the potential of differentiable dynamic programming to improve neural network pipelines that rely on an alignment and the potential dangers of relying on black-box methods for optimizing predictions of protein sequences.},
  langid = {english},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Petti et al_2022_End-to-end learning of multiple sequence alignments with differentiable.pdf;/Users/lucblassel/Zotero/storage/D8NQQB3X/2021.10.23.465204v2.html}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  date = {1958},
  journaltitle = {Psychological Review},
  volume = {65},
  pages = {386--408},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1471},
  doi = {10.1037/h0042519},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Brain,Cognition,Memory,Nervous System},
  file = {/Users/lucblassel/Zotero/storage/EWBZ4SRX/1959-09865-001.html}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2022-09-22},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  issue = {6088},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Rumelhart et al_1986_Learning representations by back-propagating errors.pdf;/Users/lucblassel/Zotero/storage/4PMKWZJV/323533a0.html}
}

@article{smirnovMAGUSMultipleSequence2021,
  title = {{{MAGUS}}: {{Multiple}} Sequence {{Alignment}} Using {{Graph clUStering}}},
  shorttitle = {{{MAGUS}}},
  author = {Smirnov, Vladimir and Warnow, Tandy},
  date = {2021-06-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {37},
  number = {12},
  pages = {1666--1672},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btaa992},
  url = {https://doi.org/10.1093/bioinformatics/btaa992},
  urldate = {2022-09-07},
  abstract = {The estimation of large multiple sequence alignments (MSAs) is a basic bioinformatics challenge. Divide-and-conquer is a useful approach that has been shown to improve the scalability and accuracy of MSA estimation in established methods such as SATé and PASTA. In these divide-and-conquer strategies, a sequence dataset is divided into disjoint subsets, alignments are computed on the subsets using base MSA methods (e.g. MAFFT), and then merged together into an alignment on the full dataset.We present MAGUS, Multiple sequence Alignment using Graph clUStering, a new technique for computing large-scale alignments. MAGUS is similar to PASTA in that it uses nearly the same initial steps (starting tree, similar decomposition strategy, and MAFFT to compute subset alignments), but then merges the subset alignments using the Graph Clustering Merger, a new method for combining disjoint alignments that we present in this study. Our study, on a heterogeneous collection of biological and simulated datasets, shows that MAGUS produces improved accuracy and is faster than PASTA on large datasets, and matches it on smaller datasets.MAGUS: https://github.com/vlasmirnov/MAGUSSupplementary data are available at Bioinformatics online.},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Smirnov_Warnow_2021_MAGUS.pdf;/Users/lucblassel/Zotero/storage/5GTPTXZ2/6012350.html}
}

@misc{sukhbaatarAdaptiveAttentionSpan2019,
  title = {Adaptive {{Attention Span}} in {{Transformers}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  date = {2019-08-08},
  number = {arXiv:1905.07799},
  eprint = {1905.07799},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.07799},
  url = {http://arxiv.org/abs/1905.07799},
  urldate = {2022-09-26},
  abstract = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Sukhbaatar et al_2019_Adaptive Attention Span in Transformers.pdf;/Users/lucblassel/Zotero/storage/GKGX5IH6/1905.html}
}

@article{suzekUniRefClustersComprehensive2015,
  title = {{{UniRef}} Clusters: A Comprehensive and Scalable Alternative for Improving Sequence Similarity Searches},
  shorttitle = {{{UniRef}} Clusters},
  author = {Suzek, Baris E. and Wang, Yuqi and Huang, Hongzhan and McGarvey, Peter B. and Wu, Cathy H. and {the UniProt Consortium}},
  date = {2015-03-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {31},
  number = {6},
  pages = {926--932},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btu739},
  url = {https://doi.org/10.1093/bioinformatics/btu739},
  urldate = {2022-09-28},
  abstract = {Motivation: UniRef databases provide full-scale clustering of UniProtKB sequences and are utilized for a broad range of applications, particularly similarity-based functional annotation. Non-redundancy and intra-cluster homogeneity in UniRef were recently improved by adding a sequence length overlap threshold. Our hypothesis is that these improvements would enhance the speed and sensitivity of similarity searches and improve the consistency of annotation within clusters.Results: Intra-cluster molecular function consistency was examined by analysis of Gene Ontology terms. Results show that UniRef clusters bring together proteins of identical molecular function in more than 97\% of the clusters, implying that clusters are useful for annotation and can also be used to detect annotation inconsistencies. To examine coverage in similarity results, BLASTP searches against UniRef50 followed by expansion of the hit lists with cluster members demonstrated advantages compared with searches against UniProtKB sequences; the searches are concise (∼7 times shorter hit list before expansion), faster (∼6 times) and more sensitive in detection of remote similarities (\&gt;96\% recall at e-value \&lt;0.0001). Our results support the use of UniRef clusters as a comprehensive and scalable alternative to native sequence databases for similarity searches and reinforces its reliability for use in functional annotation.Availability and implementation: Web access and file download from UniProt website at http://www.uniprot.org/uniref and ftp://ftp.uniprot.org/pub/databases/uniprot/uniref. BLAST searches against UniRef are available at http://www.uniprot.org/blast/Contact:huang@dbi.udel.edu},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Suzek et al_2015_UniRef clusters.pdf;/Users/lucblassel/Zotero/storage/GYYA6QFC/214968.html}
}

@article{voznicaDeepLearningPhylogenies2022,
  title = {Deep Learning from Phylogenies to Uncover the Epidemiological Dynamics of Outbreaks},
  author = {Voznica, J. and Zhukova, A. and Boskova, V. and Saulnier, E. and Lemoine, F. and Moslonka-Lefebvre, M. and Gascuel, O.},
  date = {2022-07-06},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {3896},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-31511-0},
  url = {https://www.nature.com/articles/s41467-022-31511-0},
  urldate = {2022-09-23},
  abstract = {Widely applicable, accurate and fast inference methods in phylodynamics are needed to fully profit from the richness of genetic data in uncovering the dynamics of epidemics. Standard methods, including maximum-likelihood and Bayesian approaches, generally rely on complex mathematical formulae and approximations, and do not scale with dataset size. We develop a likelihood-free, simulation-based approach, which combines deep learning with (1) a large set of summary statistics measured on phylogenies or (2) a complete and compact representation of trees, which avoids potential limitations of summary statistics and applies to any phylodynamics model. Our method enables both model selection and estimation of epidemiological parameters from very large phylogenies. We demonstrate its speed and accuracy on simulated data, where it performs better than the state-of-the-art methods. To illustrate its applicability, we assess the dynamics induced by superspreading individuals in an HIV dataset of men-having-sex-with-men in Zurich. Our tool PhyloDeep is available on github.com/evolbioinfo/phylodeep.},
  issue = {1},
  langid = {english},
  keywords = {Machine learning,Phylogeny,Statistical methods,Viral infection},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Voznica et al_2022_Deep learning from phylogenies to uncover the epidemiological dynamics of.pdf;/Users/lucblassel/Zotero/storage/SVI55Y2V/s41467-022-31511-0.html}
}

@misc{wangLinformerSelfAttentionLinear2020,
  title = {Linformer: {{Self-Attention}} with {{Linear Complexity}}},
  shorttitle = {Linformer},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  date = {2020-06-14},
  number = {arXiv:2006.04768},
  eprint = {2006.04768},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.04768},
  url = {http://arxiv.org/abs/2006.04768},
  urldate = {2022-09-28},
  abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n\^2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n\^2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the \textbackslash textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Wang et al_2020_Linformer.pdf;/Users/lucblassel/Zotero/storage/U238FC5L/2006.html}
}

@article{wangProgressMachineTranslation2021,
  title = {Progress in {{Machine Translation}}},
  author = {Wang, Haifeng and Wu, Hua and He, Zhongjun and Huang, Liang and Ward Church, Kenneth},
  date = {2021-07-14},
  journaltitle = {Engineering},
  shortjournal = {Engineering},
  issn = {2095-8099},
  doi = {10.1016/j.eng.2021.03.023},
  url = {https://www.sciencedirect.com/science/article/pii/S2095809921002745},
  urldate = {2022-09-27},
  abstract = {After more than 70 years of evolution, great achievements have been made in machine translation. Especially in recent years, translation quality has been greatly improved with the emergence of neural machine translation (NMT). In this article, we first review the history of machine translation from rule-based machine translation to example-based machine translation and statistical machine translation. We then introduce NMT in more detail, including the basic framework and the current dominant framework, Transformer, as well as multilingual translation models to deal with the data sparseness problem. In addition, we introduce cutting-edge simultaneous translation methods that achieve a balance between translation quality and latency. We then describe various products and applications of machine translation. At the end of this article, we briefly discuss challenges and future research directions in this field.},
  langid = {english},
  keywords = {Machine translation,Neural machine translation,Simultaneous translation},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Wang et al_2021_Progress in Machine Translation.pdf;/Users/lucblassel/Zotero/storage/R24Q3K4C/S2095809921002745.html}
}

@misc{wuLiteTransformerLongShort2020,
  title = {Lite {{Transformer}} with {{Long-Short Range Attention}}},
  author = {Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  date = {2020-04-24},
  number = {arXiv:2004.11886},
  eprint = {2004.11886},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.11886},
  url = {http://arxiv.org/abs/2004.11886},
  urldate = {2022-09-26},
  abstract = {Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Wu et al_2020_Lite Transformer with Long-Short Range Attention.pdf;/Users/lucblassel/Zotero/storage/PH757RA4/2004.html}
}

@article{xiongNystromformerNystrombasedAlgorithm2021,
  title = {Nyströmformer: {{A Nyström-based Algorithm}} for {{Approximating Self-Attention}}},
  shorttitle = {Nyströmformer},
  author = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {16},
  pages = {14138--14148},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i16.17664},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17664},
  urldate = {2022-09-28},
  abstract = {Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nyströmformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention with O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.},
  issue = {16},
  langid = {english},
  keywords = {Language Models},
  file = {/Users/lucblassel/Library/CloudStorage/GoogleDrive-luc.blassel@gmail.com/My Drive/Zotero_papers/Xiong et al_2021_Nyströmformer.pdf}
}

@inproceedings{yanOptimizingAccuracyRandomized2022,
  title = {Optimizing the {{Accuracy}} of {{Randomized Embedding}} for {{Sequence Alignment}}},
  booktitle = {2022 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Yan, Yiqing and Chaturvedi, Nimisha and Appuswamy, Raja},
  date = {2022-05},
  pages = {144--151},
  doi = {10.1109/IPDPSW55747.2022.00036},
  abstract = {Gapped alignment of sequenced data to a reference genome has traditionally been a computationally-intensive task due to the use of edit distance for dealing with indels and mismatches introduced by sequencing. In prior work, we developed Accel-Align [1], a Seed-Embed-Extend (SEE) sequence aligner that uses randomized embedding algorithms to quickly identify optimal candidate locations using Hamming distance rather than edit distance. While Accel-Align provides up to an order of magnitude improvement over state-of-the-art aligners, the randomized nature of embedding can lead to alignment errors resulting in lower precision and recall with downstream variant callers. In this work, we propose several techniques for improving the accuracy of randomized embedding-based sequence alignment. We provide an efficient implementation of these techniques in Accel-Align, and use it to present a comparative evaluation that demonstrates that the accuracy improvements can be achieved without sacrificing performance. Code is accessible in github.com/raja-appuswamy/accel-ali2n-release.},
  eventtitle = {2022 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  keywords = {alignment,Codes,Conferences,Distributed processing,embedding,Error analysis,Genomics,Hamming distance,mapping,Sequential analysis},
  file = {/Users/lucblassel/Zotero/storage/ZLGL2C37/9835534.html}
}



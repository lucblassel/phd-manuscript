
@misc{bahdanauNeuralMachineTranslation2016,
  abstract      = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  author        = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date          = {2016-05-19},
  doi           = {10.48550/arXiv.1409.0473},
  eprint        = {1409.0473},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  number        = {arXiv:1409.0473},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  title         = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  url           = {http://arxiv.org/abs/1409.0473},
  urldate       = {2022-09-23}
}

@article{baidDeepConsensusImprovesAccuracy2022,
  abstract     = {Circular consensus sequencing with Pacific Biosciences (PacBio) technology generates long (10–25\,kilobases), accurate ‘HiFi’ reads by combining serial observations of a DNA molecule into a consensus sequence. The standard approach to consensus generation, pbccs, uses a hidden Markov model. We introduce DeepConsensus, which uses an alignment-based loss to train a gap-aware transformer–encoder for sequence correction. Compared to pbccs, DeepConsensus reduces read errors by 42\%. This increases the yield of PacBio HiFi reads at Q20 by 9\%, at Q30 by 27\% and at Q40 by 90\%. With two SMRT Cells of HG003, reads from DeepConsensus improve hifiasm assembly contiguity (NG50 4.9\,megabases (Mb) to 17.2\,Mb), increase gene completeness (94\% to 97\%), reduce the false gene duplication rate (1.1\% to 0.5\%), improve assembly base accuracy (Q43 to Q45) and reduce variant-calling errors by 24\%. DeepConsensus models could be trained to the general problem of analyzing the alignment of other types of sequences, such as unique molecular identifiers or genome assemblies.},
  author       = {Baid, Gunjan and Cook, Daniel E. and Shafin, Kishwar and Yun, Taedong and Llinares-López, Felipe and Berthet, Quentin and Belyaeva, Anastasiya and Töpfer, Armin and Wenger, Aaron M. and Rowell, William J. and Yang, Howard and Kolesnikov, Alexey and Ammar, Waleed and Vert, Jean-Philippe and Vaswani, Ashish and McLean, Cory Y. and Nattestad, Maria and Chang, Pi-Chuan and Carroll, Andrew},
  date         = {2022-09-01},
  doi          = {10.1038/s41587-022-01435-7},
  issn         = {1546-1696},
  journaltitle = {Nature Biotechnology},
  keywords     = {Genome assembly algorithms,Machine learning,Software},
  langid       = {english},
  pages        = {1--7},
  publisher    = {{Nature Publishing Group}},
  shortjournal = {Nat Biotechnol},
  title        = {{{DeepConsensus}} Improves the Accuracy of Sequences with a Gap-Aware Sequence Transformer},
  url          = {https://www.nature.com/articles/s41587-022-01435-7},
  urldate      = {2022-09-26}
}

@misc{benegasDNALanguageModels2022,
  abstract  = {Variant effect prediction has traditionally focused on training supervised models on labeled data. Motivated by recent advances in natural language processing that have demonstrated substantial gains on diverse tasks by pre-training on large unlabeled data, however, unsupervised pre-training on massive databases of protein sequences has proven to be an effective approach to extracting complex information about proteins. Such models have been shown to learn variant effects in coding regions in a zero-shot manner. In a similar vein, we here introduce GPN (Genomic Pre-trained Network) which can learn variant effects in non-coding DNA using unsupervised pre-training on genomic DNA sequence alone. Our model is also able to learn gene structure and DNA motifs without any supervision. We demonstrate the utility of GPN by showing that it outperforms supervised deep learning models such as DeepSEA trained on vast amounts of functional genomics data in Arabidopsis thaliana, a model organism for plant biology. Additionally, GPN trained on a single genome outperforms popular conservation scores such as phyloP and PhastCons, which are computed using aligned genomes from multiple species and can be used to predict the pathogenicity of variants that perturb highly conserved positions. We provide code (https://github.com/songlab-cal/gpn) to train GPN for any given species using its DNA sequence alone and learn corresponding zero-shot variant effects genome-wide.},
  author    = {Benegas, Gonzalo and Batra, Sanjit Singh and Song, Yun S.},
  date      = {2022-08-23},
  doi       = {10.1101/2022.08.22.504706},
  langid    = {english},
  pages     = {2022.08.22.504706},
  publisher = {{bioRxiv}},
  title     = {{{DNA}} Language Models Are Powerful Zero-Shot Predictors of Non-Coding Variant Effects},
  url       = {https://www.biorxiv.org/content/10.1101/2022.08.22.504706v1},
  urldate   = {2022-09-26}
}

@misc{beplerLearningProteinSequence2019,
  abstract      = {Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings --- one per amino acid position --- that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrary-length sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction.},
  archiveprefix = {arXiv},
  author        = {Bepler, Tristan and Berger, Bonnie},
  date          = {2019-10-16},
  doi           = {10.48550/arXiv.1902.08661},
  eprint        = {1902.08661},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  number        = {arXiv:1902.08661},
  primaryclass  = {cs, q-bio, stat},
  publisher     = {{arXiv}},
  title         = {Learning Protein Sequence Embeddings Using Information from Structure},
  url           = {http://arxiv.org/abs/1902.08661},
  urldate       = {2022-09-28}
}

@article{bermanWorldwideProteinData2007,
  abstract     = {The worldwide Protein Data Bank (wwPDB) is the international collaboration that manages the deposition, processing and distribution of the PDB archive. The online PDB archive is a repository for the coordinates and related information for more than 38\,000 structures, including proteins, nucleic acids and large macromolecular complexes that have been determined using X-ray crystallography, NMR and electron microscopy techniques. The founding members of the wwPDB are RCSB PDB (USA), MSD-EBI (Europe) and PDBj (Japan) [H.M. Berman, K. Henrick and H. Nakamura (2003) Nature Struct. Biol., 10, 980]. The BMRB group (USA) joined the wwPDB in 2006. The mission of the wwPDB is to maintain a single archive of macromolecular structural data that are freely and publicly available to the global community. Additionally, the wwPDB provides a variety of services to a broad community of users. The wwPDB website at  provides information about services provided by the individual member organizations and about projects undertaken by the wwPDB.},
  author       = {Berman, Helen and Henrick, Kim and Nakamura, Haruki and Markley, John L.},
  date         = {2007-01-01},
  doi          = {10.1093/nar/gkl971},
  issn         = {0305-1048},
  issue        = {suppl\_1},
  journaltitle = {Nucleic Acids Research},
  pages        = {D301-D303},
  shortjournal = {Nucleic Acids Research},
  shorttitle   = {The Worldwide {{Protein Data Bank}} ({{wwPDB}})},
  title        = {The Worldwide {{Protein Data Bank}} ({{wwPDB}}): Ensuring a Single, Uniform Archive of {{PDB}} Data},
  url          = {https://doi.org/10.1093/nar/gkl971},
  urldate      = {2022-09-28},
  volume       = {35}
}

@incollection{bhattacharyaInterpretingPottsTransformer2021,
  author    = {Bhattacharya, Nicholas and Thomas, Neil and Rao, Roshan and Dauparas, Justas and Koo, Peter K. and Baker, David and Song, Yun S. and Ovchinnikov, Sergey},
  booktitle = {Biocomputing 2022},
  date      = {2021-09-21},
  doi       = {10.1142/9789811250477_0004},
  isbn      = {9789811250460},
  keywords  = {Attention,BERT,Contact Prediction,Language Modeling,Markov Random Fields,Potts Models,Representation Learning,Self-supervised learning,Transformer},
  pages     = {34--45},
  publisher = {{WORLD SCIENTIFIC}},
  title     = {Interpreting {{Potts}} and {{Transformer Protein Models Through}} the {{Lens}} of {{Simplified Attention}}},
  url       = {https://www.worldscientific.com/doi/abs/10.1142/9789811250477_0004},
  urldate   = {2022-09-26}
}

@misc{bhattacharyaSingleLayersAttention2020,
  abstract  = {The established approach to unsupervised protein contact prediction estimates co-evolving positions using undirected graphical models. This approach trains a Potts model on a Multiple Sequence Alignment, then predicts that the edges with highest weight correspond to contacts in the 3D structure. On the other hand, increasingly large Transformers are being pretrained on protein sequence databases but have demonstrated mixed results for downstream tasks, including contact prediction. This has sparked discussion about the role of scale and attention-based models in unsupervised protein representation learning. We argue that attention is a principled model of protein interactions, grounded in real properties of protein family data. We introduce a simplified attention layer, factored attention, and show that it achieves comparable performance to Potts models, while sharing parameters both within and across families. Further, we extract contacts from the attention maps of a pretrained Transformer and show they perform competitively with the other two approaches. This provides evidence that large-scale pretraining can learn meaningful protein features when presented with unlabeled and unaligned data. We contrast factored attention with the Transformer to indicate that the Transformer leverages hierarchical signal in protein family databases not captured by our single-layer models. This raises the exciting possibility for the development of powerful structured models of protein family databases.1},
  author    = {Bhattacharya, Nicholas and Thomas, Neil and Rao, Roshan and Dauparas, Justas and Koo, Peter K. and Baker, David and Song, Yun S. and Ovchinnikov, Sergey},
  date      = {2020-12-22},
  doi       = {10.1101/2020.12.21.423882},
  langid    = {english},
  pages     = {2020.12.21.423882},
  publisher = {{bioRxiv}},
  title     = {Single {{Layers}} of {{Attention Suffice}} to {{Predict Protein Contacts}}},
  url       = {https://www.biorxiv.org/content/10.1101/2020.12.21.423882v2},
  urldate   = {2022-09-26}
}

@misc{childGeneratingLongSequences2019,
  abstract      = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archiveprefix = {arXiv},
  author        = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  date          = {2019-04-23},
  doi           = {10.48550/arXiv.1904.10509},
  eprint        = {1904.10509},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Machine Learning,Statistics - Machine Learning},
  number        = {arXiv:1904.10509},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  title         = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  url           = {http://arxiv.org/abs/1904.10509},
  urldate       = {2022-09-26}
}

@article{corpetMultipleSequenceAlignment1988,
  abstract     = {An algorithm is presented for the multiple alignment of sequences, either proteins or nucleic acids, that is both accurate and easy to use on microcomputers. The approach is based on the conventional dynamic-programming method of pairwise alignment. Initially, a hierarchical clustering of the sequences is performed using the matrix of the pairwise alignment scores. The closest sequences are aligned creating groups of aligned sequences. Then close groups are aligned until all sequences are aligned in one group. The pairwise alignments included in the multiple alignment form a new matrix that is used to produce a hierarchical clustering. If it is different from the first one, iteration of the process can be performed. The method is illustrated by an example : a global alignment of 39 sequences of cytochrome c.},
  author       = {Corpet, Florence},
  date         = {1988-11-25},
  doi          = {10.1093/nar/16.22.10881},
  issn         = {0305-1048},
  journaltitle = {Nucleic Acids Research},
  number       = {22},
  pages        = {10881--10890},
  shortjournal = {Nucleic Acids Research},
  title        = {Multiple Sequence Alignment with Hierarchical Clustering},
  url          = {https://doi.org/10.1093/nar/16.22.10881},
  urldate      = {2022-09-07},
  volume       = {16}
}

@inproceedings{correiaAdaptivelySparseTransformers2019,
  abstract   = {Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter – which controls the shape and sparsity of alpha-entmax – allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.},
  author     = {Correia, Gonçalo M. and Niculae, Vlad and Martins, André F. T.},
  booktitle  = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  date       = {2019-11},
  doi        = {10.18653/v1/D19-1223},
  eventtitle = {{{EMNLP-IJCNLP}} 2019},
  location   = {{Hong Kong, China}},
  pages      = {2174--2184},
  publisher  = {{Association for Computational Linguistics}},
  title      = {Adaptively {{Sparse Transformers}}},
  url        = {https://aclanthology.org/D19-1223},
  urldate    = {2022-09-26}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  abstract     = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  author       = {Cybenko, G.},
  date         = {1989-12-01},
  doi          = {10.1007/BF02551274},
  issn         = {1435-568X},
  journaltitle = {Mathematics of Control, Signals and Systems},
  keywords     = {Approximation,Completeness,Neural networks},
  langid       = {english},
  number       = {4},
  pages        = {303--314},
  shortjournal = {Math. Control Signal Systems},
  title        = {Approximation by Superpositions of a Sigmoidal Function},
  url          = {https://doi.org/10.1007/BF02551274},
  urldate      = {2022-09-23},
  volume       = {2}
}

@misc{dotanHarnessingMachineTranslation2022,
  abstract  = {The sequence alignment problem is one of the most fundamental problems in bioinformatics and a plethora of methods were devised to tackle it. Here we introduce BetaAlign, a novel methodology for aligning sequences using a natural language processing (NLP) approach. BetaAlign accounts for the possible variability of the evolutionary process among different datasets by using an ensemble of transformers, each trained on millions of samples generated from a different evolutionary model. Our approach leads to outstanding alignment accuracy, often outperforming commonly used methods, such as MAFFT, DIALIGN, ClustalW, T-Coffee, and MUSCLE. Notably, the utilization of deep-learning techniques for the sequence alignment problem brings additional advantages, such as automatic feature extraction that can be leveraged for a variety of downstream analysis tasks.},
  author    = {Dotan, Edo and Belinkov, Yonatan and Avram, Oren and Wygoda, Elya and Ecker, Noa and Alburquerque, Michael and Keren, Omri and Loewenthal, Gil and Pupko, Tal},
  date      = {2022-07-23},
  doi       = {10.1101/2022.07.22.501063},
  langid    = {english},
  pages     = {2022.07.22.501063},
  publisher = {{bioRxiv}},
  title     = {Harnessing Machine Translation Methods for Sequence Alignment},
  url       = {https://www.biorxiv.org/content/10.1101/2022.07.22.501063v1},
  urldate   = {2022-09-26}
}

@article{gaoNovelSequenceAlignment2021,
  abstract     = {From evolutionary interference, function annotation to structural prediction, protein sequence comparison has provided crucial biological insights. While many sequence alignment algorithms have been developed, existing approaches often cannot detect hidden structural relationships in the ‘twilight zone’ of low sequence identity. To address this critical problem, we introduce a computational algorithm that performs protein Sequence Alignments from deep-Learning of Structural Alignments (SAdLSA, silent ‘d’). The key idea is to implicitly learn the protein folding code from many thousands of structural alignments using experimentally determined protein structures.To demonstrate that the folding code was learned, we first show that SAdLSA trained on pure α-helical proteins successfully recognizes pairs of structurally related pure β-sheet protein domains. Subsequent training and benchmarking on larger, highly challenging datasets show significant improvement over established approaches. For challenging cases, SAdLSA is ∼150\% better than HHsearch for generating pairwise alignments and ∼50\% better for identifying the proteins with the best alignments in a sequence library. The time complexity of SAdLSA is O(N) thanks to GPU acceleration.Datasets and source codes of SAdLSA are available free of charge for academic users at http://sites.gatech.edu/cssb/sadlsa/.Supplementary data are available at Bioinformatics online.},
  author       = {Gao, Mu and Skolnick, Jeffrey},
  date         = {2021-02-15},
  doi          = {10.1093/bioinformatics/btaa810},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {4},
  pages        = {490--496},
  shortjournal = {Bioinformatics},
  title        = {A Novel Sequence Alignment Algorithm Based on Deep Learning of the Protein Folding Code},
  url          = {https://doi.org/10.1093/bioinformatics/btaa810},
  urldate      = {2022-09-26},
  volume       = {37}
}

@article{guoComprehensiveStudyEnhancing2021,
  abstract     = {Accurate predictions of protein structure properties, for example, secondary structure and solvent accessibility, are essential in analyzing the structure and function of a protein. Position-specific scoring matrix (PSSM) features are widely used in the structure property prediction. However, some proteins may have low-quality PSSM features due to insufficient homologous sequences, leading to limited prediction accuracy. To address this limitation, we propose an enhancing scheme for PSSM features. We introduce the “Bagging MSA” (multiple sequence alignment) method to calculate PSSM features used to train our model, adopt a convolutional network to capture local context features and bidirectional long short-term memory for long-term dependencies, and integrate them under an unsupervised framework. Structure property prediction models are then built upon such enhanced PSSM features for more accurate predictions. Moreover, we develop two frameworks to evaluate the effectiveness of the enhanced PSSM features, which also bring proposed method into real-world scenarios. Empirical evaluation of CB513, CASP11, and CASP12 data sets indicates that our unsupervised enhancing scheme indeed generates more informative PSSM features for structure property prediction.},
  author       = {Guo, Yuzhi and Wu, Jiaxiang and Ma, Hehuan and Wang, Sheng and Huang, Junzhou},
  date         = {2021-04},
  doi          = {10.1089/cmb.2020.0416},
  journaltitle = {Journal of Computational Biology},
  keywords     = {deep learning,enhancing PSSM,protein,protein solvent accessibility,secondary structure,unsupervised learning},
  number       = {4},
  pages        = {346--361},
  publisher    = {{Mary Ann Liebert, Inc., publishers}},
  shorttitle   = {Comprehensive {{Study}} on {{Enhancing Low-Quality Position-Specific Scoring Matrix}} with {{Deep Learning}} for {{Accurate Protein Structure Property Prediction}}},
  title        = {Comprehensive {{Study}} on {{Enhancing Low-Quality Position-Specific Scoring Matrix}} with {{Deep Learning}} for {{Accurate Protein Structure Property Prediction}}: {{Using Bagging Multiple Sequence Alignment Learning}}},
  url          = {https://www.liebertpub.com/doi/10.1089/cmb.2020.0416},
  urldate      = {2022-09-26},
  volume       = {28}
}


@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2022-09-23},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
}

@article{hoangDifferentiableLearningSequenceSpecific2022,
  abstract     = {Minimizers are widely used to sample representative k-mers from biological sequences in many applications, such as read mapping and taxonomy prediction. In most scenarios, having the minimizer scheme select as few k-mer positions as possible (i.e., having a low density) is desirable to reduce computation and memory cost. Despite the growing interest in minimizers, learning an effective scheme with optimal density is still an open question, as it requires solving an apparently challenging discrete optimization problem on the permutation space of k-mer orderings. Most existing schemes are designed to work well in expectation over random sequences, which have limited applicability to many practical tools. On the other hand, several methods have been proposed to construct minimizer schemes for a specific target sequence. These methods, however, only approximate the original objective with likewise discrete surrogate tasks that are not able to significantly improve the density performance. This article introduces the first continuous relaxation of the density minimizing objective, DeepMinimizer, which employs a novel Deep Learning twin architecture to simultaneously ensure both validity and performance of the minimizer scheme. Our surrogate objective is fully differentiable and, therefore, amenable to efficient gradient-based optimization using GPU computing. Finally, we demonstrate that DeepMinimizer discovers minimizer schemes that significantly outperform state-of-the-art constructions on human genomic sequences.},
  author       = {Hoang, Minh and Zheng, Hongyu and Kingsford, Carl},
  date         = {2022-09-12},
  doi          = {10.1089/cmb.2022.0275},
  journaltitle = {Journal of Computational Biology},
  keywords     = {deep learning,optimization,sequence sketching},
  publisher    = {{Mary Ann Liebert, Inc., publishers}},
  title        = {Differentiable {{Learning}} of {{Sequence-Specific Minimizer Schemes}} with {{DeepMinimizer}}},
  url          = {https://www.liebertpub.com/doi/full/10.1089/cmb.2022.0275},
  urldate      = {2022-09-26}
}

@misc{hoLISALearnedIndexes2021,
  abstract   = {Background Next-generation sequencing (NGS) technologies have enabled affordable sequencing of billions of short DNA fragments at high throughput, paving the way for population-scale genomics. Genomics data analytics at this scale requires overcoming performance bottlenecks, such as searching for short DNA sequences over long reference sequences. Results In this paper, we introduce LISA (Learned Indexes for Sequence Analysis), a novel learning-based approach to DNA sequence search. We focus on accelerating two of the most essential flavors of DNA sequence search—exact search and super-maximal exact match (SMEM) search. LISA builds on and extends FM-index, which is the state-of-the-art technique widely deployed in genomics tools. Experiments with human, animal, and plant genome datasets indicate that LISA achieves up to 2.2 and 10.8 speedups over the state-of-the-art FM-index based implementations for exact search and super-maximal exact match (SMEM) search, respectively. Code availability https://github.com/IntelLabs/Trans-Omics-Acceleration-Library/tree/master/LISA.},
  author     = {Ho, Darryl and Kalikar, Saurabh and Misra, Sanchit and Ding, Jialin and Md, Vasimuddin and Tatbul, Nesime and Li, Heng and Kraska, Tim},
  date       = {2021-07-13},
  doi        = {10.1101/2020.12.22.423964},
  langid     = {english},
  pages      = {2020.12.22.423964},
  publisher  = {{bioRxiv}},
  shorttitle = {{{LISA}}},
  title      = {{{LISA}}: {{Learned Indexes}} for {{Sequence Analysis}}},
  url        = {https://www.biorxiv.org/content/10.1101/2020.12.22.423964v2},
  urldate    = {2022-09-26}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  abstract     = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  author       = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date         = {1989-01-01},
  doi          = {10.1016/0893-6080(89)90020-8},
  issn         = {0893-6080},
  journaltitle = {Neural Networks},
  keywords     = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  langid       = {english},
  number       = {5},
  pages        = {359--366},
  shortjournal = {Neural Networks},
  title        = {Multilayer Feedforward Networks Are Universal Approximators},
  url          = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate      = {2022-09-23},
  volume       = {2}
}

@misc{huExploringEvolutionbasedFree2022,
  abstract      = {Large-scale Protein Language Models (PLMs) have improved performance in protein prediction tasks, ranging from 3D structure prediction to various function predictions. In particular, AlphaFold, a ground-breaking AI system, could potentially reshape structural biology. However, the utility of the PLM module in AlphaFold, Evoformer, has not been explored beyond structure prediction. In this paper, we investigate the representation ability of three popular PLMs: ESM-1b (single sequence), MSA-Transformer (multiple sequence alignment) and Evoformer (structural), with a special focus on Evoformer. Specifically, we aim to answer the following key questions: (i) Does the Evoformer trained as part of AlphaFold produce representations amenable to predicting protein function? (ii) If yes, can Evoformer replace ESM-1b and MSA-Transformer? (iii) How much do these PLMs rely on evolution-related protein data? In this regard, are they complementary to each other? We compare these models by empirical study along with new insights and conclusions. Finally, we release code and datasets for reproducibility.},
  archiveprefix = {arXiv},
  author        = {Hu, Mingyang and Yuan, Fajie and Yang, Kevin K. and Ju, Fusong and Su, Jin and Wang, Hui and Yang, Fei and Ding, Qiuyang},
  date          = {2022-06-13},
  doi           = {10.48550/arXiv.2206.06583},
  eprint        = {2206.06583},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Artificial Intelligence,Quantitative Biology - Quantitative Methods},
  number        = {arXiv:2206.06583},
  primaryclass  = {cs, q-bio},
  publisher     = {{arXiv}},
  title         = {Exploring Evolution-Based \& -Free Protein Language Models as Protein Function Predictors},
  url           = {http://arxiv.org/abs/2206.06583},
  urldate       = {2022-09-26}
}

@article{jungBWAMEMEBWAMEMEmulated2022,
  abstract     = {The growing use of next-generation sequencing and enlarged sequencing throughput require efficient short-read alignment, where seeding is one of the major performance bottlenecks. The key challenge in the seeding phase is searching for exact matches of substrings of short reads in the reference DNA sequence. Existing algorithms, however, present limitations in performance due to their frequent memory accesses.This article presents BWA-MEME, the first full-fledged short read alignment software that leverages learned indices for solving the exact match search problem for efficient seeding. BWA-MEME is a practical and efficient seeding algorithm based on a suffix array search algorithm that solves the challenges in utilizing learned indices for SMEM search which is extensively used in the seeding phase. Our evaluation shows that BWA-MEME achieves up to 3.45× speedup in seeding throughput over BWA-MEM2 by reducing the number of instructions by 4.60×, memory accesses by 8.77× and LLC misses by 2.21×, while ensuring the identical SAM output to BWA-MEM2.The source code and test scripts are available for academic use at https://github.com/kaist-ina/BWA-MEME/.Supplementary data are available at Bioinformatics online.},
  author       = {Jung, Youngmok and Han, Dongsu},
  date         = {2022-05-01},
  doi          = {10.1093/bioinformatics/btac137},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {9},
  pages        = {2404--2413},
  shortjournal = {Bioinformatics},
  shorttitle   = {{{BWA-MEME}}},
  title        = {{{BWA-MEME}}: {{BWA-MEM}} Emulated with a Machine Learning Approach},
  url          = {https://doi.org/10.1093/bioinformatics/btac137},
  urldate      = {2022-09-26},
  volume       = {38}
}

@article{kirscheSaplingAcceleratingSuffix2021,
  abstract     = {As genomic data becomes more abundant, efficient algorithms and data structures for sequence alignment become increasingly important. The suffix array is a widely used data structure to accelerate alignment, but the binary search algorithm used to query, it requires widespread memory accesses, causing a large number of cache misses on large datasets.Here, we present Sapling, an algorithm for sequence alignment, which uses a learned data model to augment the suffix array and enable faster queries. We investigate different types of data models, providing an analysis of different neural network models as well as providing an open-source aligner with a compact, practical piecewise linear model. We show that Sapling outperforms both an optimized binary search approach and multiple widely used read aligners on a diverse collection of genomes, including human, bacteria and plants, speeding up the algorithm by more than a factor of two while adding \&lt;1\% to the suffix array’s memory footprint.The source code and tutorial are available open-source at https://github.com/mkirsche/sapling.Supplementary data are available at Bioinformatics online.},
  author       = {Kirsche, Melanie and Das, Arun and Schatz, Michael C},
  date         = {2021-03-15},
  doi          = {10.1093/bioinformatics/btaa911},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {6},
  pages        = {744--749},
  shortjournal = {Bioinformatics},
  shorttitle   = {Sapling},
  title        = {Sapling: Accelerating Suffix Array Queries with Learned Data Models},
  url          = {https://doi.org/10.1093/bioinformatics/btaa911},
  urldate      = {2022-09-26},
  volume       = {37}
}

@misc{kitaevReformerEfficientTransformer2020,
  abstract      = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L\^2\$) to O(\$L\textbackslash log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  archiveprefix = {arXiv},
  author        = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  date          = {2020-02-18},
  doi           = {10.48550/arXiv.2001.04451},
  eprint        = {2001.04451},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  number        = {arXiv:2001.04451},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  shorttitle    = {Reformer},
  title         = {Reformer: {{The Efficient Transformer}}},
  url           = {http://arxiv.org/abs/2001.04451},
  urldate       = {2022-09-26}
}

@inproceedings{kraskaCaseLearnedIndex2018,
  abstract  = {Indexes are models: a \textbackslash btree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term \textbackslash em learned indexes. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show that our learned indexes can have significant advantages over traditional indexes. More importantly, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work provides just a glimpse of what might be possible.},
  author    = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  date      = {2018-05-27},
  doi       = {10.1145/3183713.3196909},
  isbn      = {978-1-4503-4703-7},
  keywords  = {b-tree,bloom-filter,cdf,hash-map,index structures,learned data structures,learned index,learned index structure,linear regression,mixture of experts,neural net},
  location  = {{New York, NY, USA}},
  pages     = {489--504},
  publisher = {{Association for Computing Machinery}},
  series    = {{{SIGMOD}} '18},
  title     = {The {{Case}} for {{Learned Index Structures}}},
  url       = {https://doi.org/10.1145/3183713.3196909},
  urldate   = {2022-09-26}
}

@article{krizhevskyImageNetClassificationDeep2017,
  abstract     = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date         = {2017-05-24},
  doi          = {10.1145/3065386},
  issn         = {0001-0782},
  journaltitle = {Communications of the ACM},
  number       = {6},
  pages        = {84--90},
  shortjournal = {Commun. ACM},
  title        = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  url          = {https://doi.org/10.1145/3065386},
  urldate      = {2022-09-23},
  volume       = {60}
}

@article{lecunBackpropagationAppliedHandwritten1989,
  abstract     = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  author       = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  date         = {1989-12},
  doi          = {10.1162/neco.1989.1.4.541},
  eventtitle   = {Neural {{Computation}}},
  issn         = {0899-7667},
  journaltitle = {Neural Computation},
  number       = {4},
  pages        = {541--551},
  title        = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  volume       = {1}
}

@article{lecunGradientbasedLearningApplied1998,
  abstract     = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  author       = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date         = {1998-11},
  doi          = {10.1109/5.726791},
  eventtitle   = {Proceedings of the {{IEEE}}},
  issn         = {1558-2256},
  journaltitle = {Proceedings of the IEEE},
  keywords     = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  number       = {11},
  pages        = {2278--2324},
  title        = {Gradient-Based Learning Applied to Document Recognition},
  volume       = {86}
}

@inproceedings{liConvergenceAnalysisTwolayer2017,
  author    = {Li, Yuanzhi and Yuan, Yang},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  date      = {2017},
  publisher = {{Curran Associates, Inc.}},
  title     = {Convergence {{Analysis}} of {{Two-layer Neural Networks}} with {{ReLU Activation}}},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/a96b65a721e561e1e3de768ac819ffbb-Abstract.html},
  urldate   = {2022-09-23},
  volume    = {30}
}

@misc{llinares-lopezDeepEmbeddingAlignment2022,
  abstract  = {Protein sequence alignment is a key component of most bioinformatics pipelines to study the structures and functions of proteins. Aligning highly divergent sequences remains, however, a difficult task that current algorithms often fail to perform accurately, leaving many proteins or open reading frames poorly annotated. Here, we leverage recent advances in deep learning for language modelling and differentiable programming to propose DEDAL, a flexible model to align protein sequences and detect homologs. DEDAL is a machine learning-based model that learns to align sequences by observing large datasets of raw protein sequences and of correct alignments. Once trained, we show that DEDAL improves by up to two- or three-fold the alignment correctness over existing methods on remote homologs, and better discriminates remote homologs from evolutionarily unrelated sequences, paving the way to improvements on many downstream tasks relying on sequence alignment in structural and functional genomics.},
  author    = {Llinares-López, Felipe and Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Vert, Jean-Philippe},
  date      = {2022-07-01},
  doi       = {10.1101/2021.11.15.468653},
  langid    = {english},
  pages     = {2021.11.15.468653},
  publisher = {{bioRxiv}},
  title     = {Deep Embedding and Alignment of Protein Sequences},
  url       = {https://www.biorxiv.org/content/10.1101/2021.11.15.468653v2},
  urldate   = {2022-09-23}
}

@article{minTargetNetFunctionalMicroRNA2022,
  abstract     = {MicroRNAs (miRNAs) play pivotal roles in gene expression regulation by binding to target sites of messenger RNAs (mRNAs). While identifying functional targets of miRNAs is of utmost importance, their prediction remains a great challenge. Previous computational algorithms have major limitations. They use conservative candidate target site (CTS) selection criteria mainly focusing on canonical site types, rely on laborious and time-consuming manual feature extraction, and do not fully capitalize on the information underlying miRNA–CTS interactions.In this article, we introduce TargetNet, a novel deep learning-based algorithm for functional miRNA target prediction. To address the limitations of previous approaches, TargetNet has three key components: (i) relaxed CTS selection criteria accommodating irregularities in the seed region, (ii) a novel miRNA–CTS sequence encoding scheme incorporating extended seed region alignments and (iii) a deep residual network-based prediction model. The proposed model was trained with miRNA–CTS pair datasets and evaluated with miRNA–mRNA pair datasets. TargetNet advances the previous state-of-the-art algorithms used in functional miRNA target classification. Furthermore, it demonstrates great potential for distinguishing high-functional miRNA targets. The codes and pre-trained models are available at https://github.com/mswzeus/TargetNet.},
  author       = {Min, Seonwoo and Lee, Byunghan and Yoon, Sungroh},
  date         = {2022-02-01},
  doi          = {10.1093/bioinformatics/btab733},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {3},
  pages        = {671--677},
  shortjournal = {Bioinformatics},
  shorttitle   = {{{TargetNet}}},
  title        = {{{TargetNet}}: Functional {{microRNA}} Target Prediction with Deep Neural Networks},
  url          = {https://doi.org/10.1093/bioinformatics/btab733},
  urldate      = {2022-09-26},
  volume       = {38}
}

@article{mistryPfamProteinFamilies2021,
  abstract     = {The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.},
  author       = {Mistry, Jaina and Chuguransky, Sara and Williams, Lowri and Qureshi, Matloob and Salazar, Gustavo~A and Sonnhammer, Erik L L and Tosatto, Silvio C E and Paladin, Lisanna and Raj, Shriya and Richardson, Lorna J and Finn, Robert D and Bateman, Alex},
  date         = {2021-01-08},
  doi          = {10.1093/nar/gkaa913},
  issn         = {0305-1048},
  journaltitle = {Nucleic Acids Research},
  number       = {D1},
  pages        = {D412-D419},
  shortjournal = {Nucleic Acids Research},
  shorttitle   = {Pfam},
  title        = {Pfam: {{The}} Protein Families Database in 2021},
  url          = {https://doi.org/10.1093/nar/gkaa913},
  urldate      = {2022-09-28},
  volume       = {49}
}

@misc{mortonProteinStructuralAlignments2020,
  abstract  = {Computing sequence similarity is a fundamental task in biology, with alignment forming the basis for the annotation of genes and genomes and providing the core data structures for evolutionary analysis. Standard approaches are a mainstay of modern molecular biology and rely on variations of edit distance to obtain explicit alignments between pairs of biological sequences. However, sequence alignment algorithms struggle with remote homology tasks and cannot identify similarities between many pairs of proteins with similar structures and likely homology. Recent work suggests that using machine learning language models can improve remote homology detection. To this end, we introduce DeepBLAST, that obtains explicit alignments from residue embeddings learned from a protein language model integrated into an end-to-end differentiable alignment framework. This approach can be accelerated on the GPU architectures and outperforms conventional sequence alignment techniques in terms of both speed and accuracy when identifying structurally similar proteins.},
  author    = {Morton, James T. and Strauss, Charlie E. M. and Blackwell, Robert and Berenberg, Daniel and Gligorijevic, Vladimir and Bonneau, Richard},
  date      = {2020-11-04},
  doi       = {10.1101/2020.11.03.365932},
  langid    = {english},
  pages     = {2020.11.03.365932},
  publisher = {{bioRxiv}},
  title     = {Protein {{Structural Alignments From Sequence}}},
  url       = {https://www.biorxiv.org/content/10.1101/2020.11.03.365932v1},
  urldate   = {2022-09-26}
}

@article{murtaghMultilayerPerceptronsClassification1991,
  abstract     = {We review the theory and practice of the multilayer perceptron. We aim at addressing a range of issues which are important from the point of view of applying this approach to practical problems. A number of examples are given, illustrating how the multilayer perceptron compares to alternative, conventional approaches. The application fields of classification and regression are especially considered. Questions of implementation, i.e. of multilayer perceptron architecture, dynamics, and related aspects, are discussed. Recent studies, which are particularly relevant to the areas of discriminant analysis, and function mapping, are cited.},
  author       = {Murtagh, Fionn},
  date         = {1991-07-01},
  doi          = {10.1016/0925-2312(91)90023-5},
  issn         = {0925-2312},
  journaltitle = {Neurocomputing},
  keywords     = {discriminant analysis,function approximation,Multilayer perceptron,regression,supervised classification},
  langid       = {english},
  number       = {5},
  pages        = {183--197},
  shortjournal = {Neurocomputing},
  title        = {Multilayer Perceptrons for Classification and Regression},
  url          = {https://www.sciencedirect.com/science/article/pii/0925231291900235},
  urldate      = {2022-09-23},
  volume       = {2}
}

@article{ourmazdStructuralBiologySolved2022,
  abstract     = {The splendid computational success of AlphaFold and RoseTTAFold in solving the 60-year-old problem of protein folding raises an obvious question: what new avenues should structural biology explore? We propose a strong pivot toward the goal of reading mechanism and function directly from the amino acid sequence. This ambitious goal will require new data analytical tools and an extensive database of the atomic-level structural trajectories traced out on energy landscapes as proteins perform their function.},
  author       = {Ourmazd, Abbas and Moffat, Keith and Lattman, Eaton Edward},
  date         = {2022-01},
  doi          = {10.1038/s41592-021-01357-3},
  issn         = {1548-7105},
  issue        = {1},
  journaltitle = {Nature Methods},
  keywords     = {Protein function predictions,Protein structure predictions,Proteins},
  langid       = {english},
  number       = {1},
  pages        = {24--26},
  publisher    = {{Nature Publishing Group}},
  shortjournal = {Nat Methods},
  title        = {Structural Biology Is Solved — Now What?},
  url          = {https://www.nature.com/articles/s41592-021-01357-3},
  urldate      = {2022-09-29},
  volume       = {19}
}

@misc{pettiEndtoendLearningMultiple2022,
  abstract  = {Multiple Sequence Alignments (MSAs) of homologous sequences contain information on structural and functional constraints and their evolutionary histories. Despite their importance for many downstream tasks, such as structure prediction, MSA generation is often treated as a separate pre-processing step, without any guidance from the application it will be used for. Here, we implement a smooth and differentiable version of the Smith-Waterman pairwise alignment algorithm that enables jointly learning an MSA and a downstream machine learning system in an end-to-end fashion. To demonstrate its utility, we introduce SMURF (Smooth Markov Unaligned Random Field), a new method that jointly learns an alignment and the parameters of a Markov Random Field for unsupervised contact prediction. We find that SMURF learns MSAs that mildly improve contact prediction on a diverse set of protein and RNA families. As a proof of concept, we demonstrate that by connecting our differentiable alignment module to AlphaFold and maximizing predicted confidence, we can learn MSAs that improve structure predictions over the initial MSAs. Interestingly, the alignments that improve AlphaFold predictions are self-inconsistent and can be viewed as adversarial. This work highlights the potential of differentiable dynamic programming to improve neural network pipelines that rely on an alignment and the potential dangers of relying on black-box methods for optimizing predictions of protein sequences.},
  author    = {Petti, Samantha and Bhattacharya, Nicholas and Rao, Roshan and Dauparas, Justas and Thomas, Neil and Zhou, Juannan and Rush, Alexander M. and Koo, Peter K. and Ovchinnikov, Sergey},
  date      = {2022-04-18},
  doi       = {10.1101/2021.10.23.465204},
  langid    = {english},
  pages     = {2021.10.23.465204},
  publisher = {{bioRxiv}},
  title     = {End-to-End Learning of Multiple Sequence Alignments with Differentiable {{Smith-Waterman}}},
  url       = {https://www.biorxiv.org/content/10.1101/2021.10.23.465204v2},
  urldate   = {2022-09-07}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  abstract     = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  author       = {Rosenblatt, F.},
  date         = {1958},
  doi          = {10.1037/h0042519},
  issn         = {1939-1471},
  journaltitle = {Psychological Review},
  keywords     = {Brain,Cognition,Memory,Nervous System},
  location     = {{US}},
  pages        = {386--408},
  publisher    = {{American Psychological Association}},
  shorttitle   = {The Perceptron},
  title        = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain},
  volume       = {65}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  abstract     = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  author       = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date         = {1986-10},
  doi          = {10.1038/323533a0},
  issn         = {1476-4687},
  issue        = {6088},
  journaltitle = {Nature},
  keywords     = {Humanities and Social Sciences,multidisciplinary,Science},
  langid       = {english},
  number       = {6088},
  pages        = {533--536},
  publisher    = {{Nature Publishing Group}},
  title        = {Learning Representations by Back-Propagating Errors},
  url          = {https://www.nature.com/articles/323533a0},
  urldate      = {2022-09-22},
  volume       = {323}
}

@article{smirnovMAGUSMultipleSequence2021,
  abstract     = {The estimation of large multiple sequence alignments (MSAs) is a basic bioinformatics challenge. Divide-and-conquer is a useful approach that has been shown to improve the scalability and accuracy of MSA estimation in established methods such as SATé and PASTA. In these divide-and-conquer strategies, a sequence dataset is divided into disjoint subsets, alignments are computed on the subsets using base MSA methods (e.g. MAFFT), and then merged together into an alignment on the full dataset.We present MAGUS, Multiple sequence Alignment using Graph clUStering, a new technique for computing large-scale alignments. MAGUS is similar to PASTA in that it uses nearly the same initial steps (starting tree, similar decomposition strategy, and MAFFT to compute subset alignments), but then merges the subset alignments using the Graph Clustering Merger, a new method for combining disjoint alignments that we present in this study. Our study, on a heterogeneous collection of biological and simulated datasets, shows that MAGUS produces improved accuracy and is faster than PASTA on large datasets, and matches it on smaller datasets.MAGUS: https://github.com/vlasmirnov/MAGUSSupplementary data are available at Bioinformatics online.},
  author       = {Smirnov, Vladimir and Warnow, Tandy},
  date         = {2021-06-15},
  doi          = {10.1093/bioinformatics/btaa992},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {12},
  pages        = {1666--1672},
  shortjournal = {Bioinformatics},
  shorttitle   = {{{MAGUS}}},
  title        = {{{MAGUS}}: {{Multiple}} Sequence {{Alignment}} Using {{Graph clUStering}}},
  url          = {https://doi.org/10.1093/bioinformatics/btaa992},
  urldate      = {2022-09-07},
  volume       = {37}
}

@misc{sukhbaatarAdaptiveAttentionSpan2019,
  abstract      = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
  archiveprefix = {arXiv},
  author        = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  date          = {2019-08-08},
  doi           = {10.48550/arXiv.1905.07799},
  eprint        = {1905.07799},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Machine Learning,Statistics - Machine Learning},
  number        = {arXiv:1905.07799},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  title         = {Adaptive {{Attention Span}} in {{Transformers}}},
  url           = {http://arxiv.org/abs/1905.07799},
  urldate       = {2022-09-26}
}

@article{suzekUniRefClustersComprehensive2015,
  abstract     = {Motivation: UniRef databases provide full-scale clustering of UniProtKB sequences and are utilized for a broad range of applications, particularly similarity-based functional annotation. Non-redundancy and intra-cluster homogeneity in UniRef were recently improved by adding a sequence length overlap threshold. Our hypothesis is that these improvements would enhance the speed and sensitivity of similarity searches and improve the consistency of annotation within clusters.Results: Intra-cluster molecular function consistency was examined by analysis of Gene Ontology terms. Results show that UniRef clusters bring together proteins of identical molecular function in more than 97\% of the clusters, implying that clusters are useful for annotation and can also be used to detect annotation inconsistencies. To examine coverage in similarity results, BLASTP searches against UniRef50 followed by expansion of the hit lists with cluster members demonstrated advantages compared with searches against UniProtKB sequences; the searches are concise (∼7 times shorter hit list before expansion), faster (∼6 times) and more sensitive in detection of remote similarities (\&gt;96\% recall at e-value \&lt;0.0001). Our results support the use of UniRef clusters as a comprehensive and scalable alternative to native sequence databases for similarity searches and reinforces its reliability for use in functional annotation.Availability and implementation: Web access and file download from UniProt website at http://www.uniprot.org/uniref and ftp://ftp.uniprot.org/pub/databases/uniprot/uniref. BLAST searches against UniRef are available at http://www.uniprot.org/blast/Contact:huang@dbi.udel.edu},
  author       = {Suzek, Baris E. and Wang, Yuqi and Huang, Hongzhan and McGarvey, Peter B. and Wu, Cathy H. and {the UniProt Consortium}},
  date         = {2015-03-15},
  doi          = {10.1093/bioinformatics/btu739},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {6},
  pages        = {926--932},
  shortjournal = {Bioinformatics},
  shorttitle   = {{{UniRef}} Clusters},
  title        = {{{UniRef}} Clusters: A Comprehensive and Scalable Alternative for Improving Sequence Similarity Searches},
  url          = {https://doi.org/10.1093/bioinformatics/btu739},
  urldate      = {2022-09-28},
  volume       = {31}
}

@article{voznicaDeepLearningPhylogenies2022,
  abstract     = {Widely applicable, accurate and fast inference methods in phylodynamics are needed to fully profit from the richness of genetic data in uncovering the dynamics of epidemics. Standard methods, including maximum-likelihood and Bayesian approaches, generally rely on complex mathematical formulae and approximations, and do not scale with dataset size. We develop a likelihood-free, simulation-based approach, which combines deep learning with (1) a large set of summary statistics measured on phylogenies or (2) a complete and compact representation of trees, which avoids potential limitations of summary statistics and applies to any phylodynamics model. Our method enables both model selection and estimation of epidemiological parameters from very large phylogenies. We demonstrate its speed and accuracy on simulated data, where it performs better than the state-of-the-art methods. To illustrate its applicability, we assess the dynamics induced by superspreading individuals in an HIV dataset of men-having-sex-with-men in Zurich. Our tool PhyloDeep is available on github.com/evolbioinfo/phylodeep.},
  author       = {Voznica, J. and Zhukova, A. and Boskova, V. and Saulnier, E. and Lemoine, F. and Moslonka-Lefebvre, M. and Gascuel, O.},
  date         = {2022-07-06},
  doi          = {10.1038/s41467-022-31511-0},
  issn         = {2041-1723},
  issue        = {1},
  journaltitle = {Nature Communications},
  keywords     = {Machine learning,Phylogeny,Statistical methods,Viral infection},
  langid       = {english},
  number       = {1},
  pages        = {3896},
  publisher    = {{Nature Publishing Group}},
  shortjournal = {Nat Commun},
  title        = {Deep Learning from Phylogenies to Uncover the Epidemiological Dynamics of Outbreaks},
  url          = {https://www.nature.com/articles/s41467-022-31511-0},
  urldate      = {2022-09-23},
  volume       = {13}
}

@misc{wangLinformerSelfAttentionLinear2020,
  abstract      = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n\^2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n\^2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the \textbackslash textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  archiveprefix = {arXiv},
  author        = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  date          = {2020-06-14},
  doi           = {10.48550/arXiv.2006.04768},
  eprint        = {2006.04768},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Machine Learning,Statistics - Machine Learning},
  number        = {arXiv:2006.04768},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  shorttitle    = {Linformer},
  title         = {Linformer: {{Self-Attention}} with {{Linear Complexity}}},
  url           = {http://arxiv.org/abs/2006.04768},
  urldate       = {2022-09-28}
}

@article{wangProgressMachineTranslation2021,
  abstract     = {After more than 70 years of evolution, great achievements have been made in machine translation. Especially in recent years, translation quality has been greatly improved with the emergence of neural machine translation (NMT). In this article, we first review the history of machine translation from rule-based machine translation to example-based machine translation and statistical machine translation. We then introduce NMT in more detail, including the basic framework and the current dominant framework, Transformer, as well as multilingual translation models to deal with the data sparseness problem. In addition, we introduce cutting-edge simultaneous translation methods that achieve a balance between translation quality and latency. We then describe various products and applications of machine translation. At the end of this article, we briefly discuss challenges and future research directions in this field.},
  author       = {Wang, Haifeng and Wu, Hua and He, Zhongjun and Huang, Liang and Ward Church, Kenneth},
  date         = {2021-07-14},
  doi          = {10.1016/j.eng.2021.03.023},
  issn         = {2095-8099},
  journaltitle = {Engineering},
  keywords     = {Machine translation,Neural machine translation,Simultaneous translation},
  langid       = {english},
  shortjournal = {Engineering},
  title        = {Progress in {{Machine Translation}}},
  url          = {https://www.sciencedirect.com/science/article/pii/S2095809921002745},
  urldate      = {2022-09-27}
}

@misc{wuLiteTransformerLongShort2020,
  abstract      = {Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.},
  archiveprefix = {arXiv},
  author        = {Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  date          = {2020-04-24},
  doi           = {10.48550/arXiv.2004.11886},
  eprint        = {2004.11886},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Computation and Language},
  number        = {arXiv:2004.11886},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  title         = {Lite {{Transformer}} with {{Long-Short Range Attention}}},
  url           = {http://arxiv.org/abs/2004.11886},
  urldate       = {2022-09-26}
}

@article{xiongNystromformerNystrombasedAlgorithm2021,
  abstract     = {Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nyströmformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention with O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.},
  author       = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  date         = {2021-05-18},
  doi          = {10.1609/aaai.v35i16.17664},
  issn         = {2374-3468},
  issue        = {16},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  keywords     = {Language Models},
  langid       = {english},
  number       = {16},
  pages        = {14138--14148},
  shorttitle   = {Nyströmformer},
  title        = {Nyströmformer: {{A Nyström-based Algorithm}} for {{Approximating Self-Attention}}},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17664},
  urldate      = {2022-09-28},
  volume       = {35}
}

@inproceedings{yanOptimizingAccuracyRandomized2022,
  abstract   = {Gapped alignment of sequenced data to a reference genome has traditionally been a computationally-intensive task due to the use of edit distance for dealing with indels and mismatches introduced by sequencing. In prior work, we developed Accel-Align [1], a Seed-Embed-Extend (SEE) sequence aligner that uses randomized embedding algorithms to quickly identify optimal candidate locations using Hamming distance rather than edit distance. While Accel-Align provides up to an order of magnitude improvement over state-of-the-art aligners, the randomized nature of embedding can lead to alignment errors resulting in lower precision and recall with downstream variant callers. In this work, we propose several techniques for improving the accuracy of randomized embedding-based sequence alignment. We provide an efficient implementation of these techniques in Accel-Align, and use it to present a comparative evaluation that demonstrates that the accuracy improvements can be achieved without sacrificing performance. Code is accessible in github.com/raja-appuswamy/accel-ali2n-release.},
  author     = {Yan, Yiqing and Chaturvedi, Nimisha and Appuswamy, Raja},
  booktitle  = {2022 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  date       = {2022-05},
  doi        = {10.1109/IPDPSW55747.2022.00036},
  eventtitle = {2022 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  keywords   = {alignment,Codes,Conferences,Distributed processing,embedding,Error analysis,Genomics,Hamming distance,mapping,Sequential analysis},
  pages      = {144--151},
  title      = {Optimizing the {{Accuracy}} of {{Randomized Embedding}} for {{Sequence Alignment}}}
}



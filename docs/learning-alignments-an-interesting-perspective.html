<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments</title>
  <meta name="description" content="Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="lucblassel/phd-manuscript" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments" />
  
  
  

<meta name="author" content="Luc Blassel" />


<meta name="date" content="2022-09-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="HIV-paper.html"/>
<link rel="next" href="global-conclusion.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<link href="libs/tabwid-1.0.0/scrool.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">From sequences to knowledge,</br> improving and learning from sequence alignments.</a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#résumé"><i class="fa fa-check"></i>Résumé</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="list-of-acronyms-and-abbreviations.html"><a href="list-of-acronyms-and-abbreviations.html"><i class="fa fa-check"></i>List of Acronyms and abbreviations</a></li>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i>General Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#research-output"><i class="fa fa-check"></i>Research output</a>
<ul>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#journal-publications"><i class="fa fa-check"></i>Journal publications</a></li>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#presentations-and-posters"><i class="fa fa-check"></i>Presentations and posters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html"><i class="fa fa-check"></i><b>1</b> What is Sequence data ?</a>
<ul>
<li class="chapter" data-level="1.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#biological-sequences-a-primer"><i class="fa fa-check"></i><b>1.1</b> Biological sequences, a primer</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#what-is-dna"><i class="fa fa-check"></i><b>1.1.1</b> What is DNA ?</a></li>
<li class="chapter" data-level="1.1.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#from-information-to-action"><i class="fa fa-check"></i><b>1.1.2</b> From Information to action</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#obtaining-sequence-data"><i class="fa fa-check"></i><b>1.2</b> Obtaining sequence data</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#sanger-sequencing-a-breakthrough"><i class="fa fa-check"></i><b>1.2.1</b> Sanger sequencing, a breakthrough</a></li>
<li class="chapter" data-level="1.2.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#next-generation-sequencing"><i class="fa fa-check"></i><b>1.2.2</b> Next-generation sequencing</a></li>
<li class="chapter" data-level="1.2.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#long-read-sequencing"><i class="fa fa-check"></i><b>1.2.3</b> Long read sequencing</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#sequencing-errors-how-to-account-for-them"><i class="fa fa-check"></i><b>1.3</b> Sequencing errors, how to account for them ?</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#error-correction-methods"><i class="fa fa-check"></i><b>1.3.1</b> Error correction methods</a></li>
<li class="chapter" data-level="1.3.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#more-accurate-sequencing-methods"><i class="fa fa-check"></i><b>1.3.2</b> More accurate sequencing methods</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#the-special-case-of-homopolymers"><i class="fa fa-check"></i><b>1.4</b> The special case of homopolymers</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#homopolymers-and-the-human-genome"><i class="fa fa-check"></i><b>1.4.1</b> Homopolymers and the human genome</a></li>
<li class="chapter" data-level="1.4.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#homopolymers-and-long-reads"><i class="fa fa-check"></i><b>1.4.2</b> Homopolymers and long reads</a></li>
<li class="chapter" data-level="1.4.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#accounting-for-homopolymers"><i class="fa fa-check"></i><b>1.4.3</b> Accounting for homopolymers</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html"><i class="fa fa-check"></i><b>2</b> Aligning sequence data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#what-is-an-alignment"><i class="fa fa-check"></i><b>2.1</b> What is an alignment ?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#why-align"><i class="fa fa-check"></i><b>2.1.1</b> Why align ?</a></li>
<li class="chapter" data-level="2.1.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#how-to-align-two-sequences"><i class="fa fa-check"></i><b>2.1.2</b> How to align two sequences ?</a></li>
<li class="chapter" data-level="2.1.3" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#scoring-and-substitution-models"><i class="fa fa-check"></i><b>2.1.3</b> Scoring and substitution models</a></li>
<li class="chapter" data-level="2.1.4" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#dealing-with-gaps"><i class="fa fa-check"></i><b>2.1.4</b> Dealing with gaps</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#how-do-we-speed-up-pairwise-alignment"><i class="fa fa-check"></i><b>2.2</b> How do we speed up pairwise alignment ?</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#changing-the-method"><i class="fa fa-check"></i><b>2.2.1</b> Changing the method</a></li>
<li class="chapter" data-level="2.2.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#seed-and-extend-with-data-structures"><i class="fa fa-check"></i><b>2.2.2</b> Seed and extend with data structures</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#the-specificities-of-read-mapping"><i class="fa fa-check"></i><b>2.3</b> The specificities of read-mapping</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#what-is-read-mapping"><i class="fa fa-check"></i><b>2.3.1</b> What is read-mapping ?</a></li>
<li class="chapter" data-level="2.3.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#challenges-of-read-mapping"><i class="fa fa-check"></i><b>2.3.2</b> Challenges of read-mapping</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#multiple-sequence-alignment"><i class="fa fa-check"></i><b>2.4</b> Multiple sequence alignment</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#progressive-alignment"><i class="fa fa-check"></i><b>2.4.1</b> Progressive alignment</a></li>
<li class="chapter" data-level="2.4.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#other-methods"><i class="fa fa-check"></i><b>2.4.2</b> Other methods</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#conclusion-1"><i class="fa fa-check"></i><b>2.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="HPC-paper.html"><a href="HPC-paper.html"><i class="fa fa-check"></i><b>3</b> Contribution 1: Improving read alignment by exploring a sequence transformation space</a>
<ul>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#highlights"><i class="fa fa-check"></i>Highlights</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#graphical-abstract"><i class="fa fa-check"></i>Graphical Abstract</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="3.1" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="HPC-paper.html"><a href="HPC-paper.html#methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="HPC-paper.html"><a href="HPC-paper.html#sec:msr-def"><i class="fa fa-check"></i><b>3.2.1</b> Streaming sequence reductions</a></li>
<li class="chapter" data-level="3.2.2" data-path="HPC-paper.html"><a href="HPC-paper.html#sec:enum"><i class="fa fa-check"></i><b>3.2.2</b> Restricting the space of streaming sequence reductions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="HPC-paper.html"><a href="HPC-paper.html#datasets-and-pipelines"><i class="fa fa-check"></i><b>3.3</b> Datasets and Pipelines</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="HPC-paper.html"><a href="HPC-paper.html#datasets"><i class="fa fa-check"></i><b>3.3.1</b> Datasets</a></li>
<li class="chapter" data-level="3.3.2" data-path="HPC-paper.html"><a href="HPC-paper.html#simulation-pipeline"><i class="fa fa-check"></i><b>3.3.2</b> Simulation pipeline</a></li>
<li class="chapter" data-level="3.3.3" data-path="HPC-paper.html"><a href="HPC-paper.html#evaluation-pipeline"><i class="fa fa-check"></i><b>3.3.3</b> Evaluation pipeline</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-results"><i class="fa fa-check"></i><b>3.4</b> Results</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="HPC-paper.html"><a href="HPC-paper.html#selection-of-mapping-friendly-sequence-reductions"><i class="fa fa-check"></i><b>3.4.1</b> Selection of mapping-friendly sequence reductions</a></li>
<li class="chapter" data-level="3.4.2" data-path="HPC-paper.html"><a href="HPC-paper.html#mapping-friendly-sequence-reductions-lead-to-lower-mapping-errors-on-whole-genomes"><i class="fa fa-check"></i><b>3.4.2</b> Mapping-friendly sequence reductions lead to lower mapping errors on whole genomes</a></li>
<li class="chapter" data-level="3.4.3" data-path="HPC-paper.html"><a href="HPC-paper.html#mapping-friendly-sequence-reductions-increase-mapping-quality-on-repeated-regions-of-the-human-genome"><i class="fa fa-check"></i><b>3.4.3</b> Mapping-friendly sequence reductions increase mapping quality on repeated regions of the human genome</a></li>
<li class="chapter" data-level="3.4.4" data-path="HPC-paper.html"><a href="HPC-paper.html#raw-mapping-improves-upon-hpc-on-centromeric-regions"><i class="fa fa-check"></i><b>3.4.4</b> Raw mapping improves upon HPC on centromeric regions</a></li>
<li class="chapter" data-level="3.4.5" data-path="HPC-paper.html"><a href="HPC-paper.html#positions-of-incorrectly-mapped-reads-across-the-entire-human-genome"><i class="fa fa-check"></i><b>3.4.5</b> Positions of incorrectly mapped reads across the entire human genome</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="HPC-paper.html"><a href="HPC-paper.html#discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
<li class="chapter" data-level="3.6" data-path="HPC-paper.html"><a href="HPC-paper.html#limitations-of-this-study"><i class="fa fa-check"></i><b>3.6</b> Limitations of this study</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#author-contributions"><i class="fa fa-check"></i>Author contributions</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#declaration-of-interests"><i class="fa fa-check"></i>Declaration of interests</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#star-methods"><i class="fa fa-check"></i>STAR Methods</a>
<ul>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#lead-contact"><i class="fa fa-check"></i>Lead contact</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#materials-availability"><i class="fa fa-check"></i>Materials availability</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#data-and-code-availability"><i class="fa fa-check"></i>Data and code availability</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#method-details"><i class="fa fa-check"></i>Method details</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#supplementary-information"><i class="fa fa-check"></i>Supplementary information</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html"><i class="fa fa-check"></i><b>4</b> Learning from sequences and alignments</a>
<ul>
<li class="chapter" data-level="4.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#why-learn-from-alignments"><i class="fa fa-check"></i><b>4.1</b> Why learn from alignments ?</a></li>
<li class="chapter" data-level="4.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#what-to-learn"><i class="fa fa-check"></i><b>4.2</b> What to learn ?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#supervised-learning"><i class="fa fa-check"></i><b>4.2.1</b> Supervised learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#unsupervised-learning"><i class="fa fa-check"></i><b>4.2.2</b> Unsupervised learning</a></li>
<li class="chapter" data-level="4.2.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#others-paradigms"><i class="fa fa-check"></i><b>4.2.3</b> Others paradigms</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#how-to-learn"><i class="fa fa-check"></i><b>4.3</b> How to learn ?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#general-setting"><i class="fa fa-check"></i><b>4.3.1</b> General setting</a></li>
<li class="chapter" data-level="4.3.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#tests-and-statistical-learning"><i class="fa fa-check"></i><b>4.3.2</b> Tests and statistical learning</a></li>
<li class="chapter" data-level="4.3.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#more-complex-methods"><i class="fa fa-check"></i><b>4.3.3</b> More complex methods</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#preprocessing-the-alignment-for-machine-learning"><i class="fa fa-check"></i><b>4.4</b> Preprocessing the alignment for machine learning</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#general-purpose-encodings"><i class="fa fa-check"></i><b>4.4.1</b> General purpose encodings</a></li>
<li class="chapter" data-level="4.4.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#biological-sequence-specific-encodings"><i class="fa fa-check"></i><b>4.4.2</b> Biological sequence-specific encodings</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#conclusion-2"><i class="fa fa-check"></i><b>4.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html"><i class="fa fa-check"></i><b>5</b> Viruses, HIV and drug resistance</a>
<ul>
<li class="chapter" data-level="5.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#what-are-viruses"><i class="fa fa-check"></i><b>5.1</b> What are viruses ?</a></li>
<li class="chapter" data-level="5.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#getting-to-know-hiv"><i class="fa fa-check"></i><b>5.2</b> Getting to know HIV</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#quick-presentation-of-hiv"><i class="fa fa-check"></i><b>5.2.1</b> Quick Presentation of HIV</a></li>
<li class="chapter" data-level="5.2.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#the-replication-cycle-of-hiv"><i class="fa fa-check"></i><b>5.2.2</b> The replication cycle of HIV</a></li>
<li class="chapter" data-level="5.2.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#genetics-of-hiv"><i class="fa fa-check"></i><b>5.2.3</b> Genetics of HIV</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#drug-resistance-in-hiv"><i class="fa fa-check"></i><b>5.3</b> Drug resistance in HIV</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#a-quick-history-of-art"><i class="fa fa-check"></i><b>5.3.1</b> A quick history of ART</a></li>
<li class="chapter" data-level="5.3.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#drug-mechanisms"><i class="fa fa-check"></i><b>5.3.2</b> Main mechanisms of viral proteins, antiretroviral drugs and associated resistance.</a></li>
<li class="chapter" data-level="5.3.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#consequences-of-resistance-on-global-health"><i class="fa fa-check"></i><b>5.3.3</b> Consequences of resistance on global health</a></li>
<li class="chapter" data-level="5.3.4" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#finding-drms"><i class="fa fa-check"></i><b>5.3.4</b> Finding DRMs </a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#conclusion-3"><i class="fa fa-check"></i><b>5.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="HIV-paper.html"><a href="HIV-paper.html"><i class="fa fa-check"></i><b>6</b> Contribution 2: Inferring mutation roles from sequence alignments using machine learning</a>
<ul>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#abstract-paper"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#author-summary"><i class="fa fa-check"></i>Author summary</a></li>
<li class="chapter" data-level="6.1" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="HIV-paper.html"><a href="HIV-paper.html#materials-and-methods"><i class="fa fa-check"></i><b>6.2</b> Materials and methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="HIV-paper.html"><a href="HIV-paper.html#data"><i class="fa fa-check"></i><b>6.2.1</b> Data</a></li>
<li class="chapter" data-level="6.2.2" data-path="HIV-paper.html"><a href="HIV-paper.html#classifier-training"><i class="fa fa-check"></i><b>6.2.2</b> Classifier training</a></li>
<li class="chapter" data-level="6.2.3" data-path="HIV-paper.html"><a href="HIV-paper.html#measuring-classifier-performance"><i class="fa fa-check"></i><b>6.2.3</b> Measuring classifier performance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-results"><i class="fa fa-check"></i><b>6.3</b> Results</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="HIV-paper.html"><a href="HIV-paper.html#classifier-performance-interpretation"><i class="fa fa-check"></i><b>6.3.1</b> Classifier performance &amp; interpretation</a></li>
<li class="chapter" data-level="6.3.2" data-path="HIV-paper.html"><a href="HIV-paper.html#additional-classification-results"><i class="fa fa-check"></i><b>6.3.2</b> Additional classification results</a></li>
<li class="chapter" data-level="6.3.3" data-path="HIV-paper.html"><a href="HIV-paper.html#identifying-new-mutations-from-classifiers"><i class="fa fa-check"></i><b>6.3.3</b> Identifying new mutations from classifiers</a></li>
<li class="chapter" data-level="6.3.4" data-path="HIV-paper.html"><a href="HIV-paper.html#detailed-analysis-of-potentially-resistance-associated-mutations"><i class="fa fa-check"></i><b>6.3.4</b> Detailed analysis of potentially resistance-associated mutations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="HIV-paper.html"><a href="HIV-paper.html#discussion-and-perspectives"><i class="fa fa-check"></i><b>6.4</b> Discussion and perspectives</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#supporting-information"><i class="fa fa-check"></i>Supporting Information</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html"><i class="fa fa-check"></i><b>7</b> Learning alignments, an interesting perspective</a>
<ul>
<li class="chapter" data-level="7.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#deep-learning-and-sequences"><i class="fa fa-check"></i><b>7.1</b> Deep learning and sequences</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#intro-to-deep-learning"><i class="fa fa-check"></i><b>7.1.1</b> Intro to deep learning</a></li>
<li class="chapter" data-level="7.1.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learned-sequence-embeddings"><i class="fa fa-check"></i><b>7.1.2</b> Learned sequence embeddings</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learning-pairwise-alignment"><i class="fa fa-check"></i><b>7.2</b> Learning pairwise alignment</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#predicting-a-substitution-matrix"><i class="fa fa-check"></i><b>7.2.1</b> Predicting a substitution matrix</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#predicting-an-alignment"><i class="fa fa-check"></i><b>7.2.2</b> predicting an alignment</a></li>
<li class="chapter" data-level="7.2.3" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learning-seeds"><i class="fa fa-check"></i><b>7.2.3</b> Learning seeds</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html"><i class="fa fa-check"></i>Global conclusion</a>
<ul>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#hpc-part"><i class="fa fa-check"></i>HPC part</a></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#hiv-part"><i class="fa fa-check"></i>HIV part</a></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#final-words"><i class="fa fa-check"></i>Final words</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="global-references.html"><a href="global-references.html"><i class="fa fa-check"></i>Global References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="HPC-appendix.html"><a href="HPC-appendix.html"><i class="fa fa-check"></i><b>A</b> Supporting Information for “Mapping-friendly sequence reductions: going beyond homopolymer compression”</a>
<ul>
<li class="chapter" data-level="A.1" data-path="HPC-appendix.html"><a href="HPC-appendix.html#appendix:tandemtools"><i class="fa fa-check"></i><b>A.1</b> “TandemTools” dataset generation</a></li>
<li class="chapter" data-level="A.2" data-path="HPC-appendix.html"><a href="HPC-appendix.html#msr-performance-comparison"><i class="fa fa-check"></i><b>A.2</b> MSR performance comparison</a></li>
<li class="chapter" data-level="A.3" data-path="HPC-appendix.html"><a href="HPC-appendix.html#analyzing-read-origin-on-whole-human-genome"><i class="fa fa-check"></i><b>A.3</b> Analyzing read origin on whole human genome</a></li>
<li class="chapter" data-level="A.4" data-path="HPC-appendix.html"><a href="HPC-appendix.html#performance-of-msrs-on-the-drosophila-genome"><i class="fa fa-check"></i><b>A.4</b> Performance of MSRs on the Drosophila genome</a></li>
<li class="chapter" data-level="A.5" data-path="HPC-appendix.html"><a href="HPC-appendix.html#key-resource-table"><i class="fa fa-check"></i><b>A.5</b> Key Resource Table</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html"><i class="fa fa-check"></i><b>B</b> Supporting Information for “HIV and DRMs”</a>
<ul>
<li class="chapter" data-level="B.1" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html#detailed-list-of-hiv-1-protein-structures-used-for-figure-generation."><i class="fa fa-check"></i><b>B.1</b> Detailed list of HIV-1 protein structures used for figure generation.</a></li>
<li class="chapter" data-level="B.2" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html#list-of-all-antiretroviral-drugs"><i class="fa fa-check"></i><b>B.2</b> List of all antiretroviral drugs</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="HIV-appendix.html"><a href="HIV-appendix.html"><i class="fa fa-check"></i><b>C</b> Supporting Information for “Using Machine Learning and Big Data to Explore the Drug Resistance Landscape in HIV”</a>
<ul>
<li class="chapter" data-level="C.1" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S1-Appendix"><i class="fa fa-check"></i><b>C.1</b> S1 Appendix (Technical appendix).</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="HIV-appendix.html"><a href="HIV-appendix.html#data-appendix"><i class="fa fa-check"></i><b>C.1.1</b> Data</a></li>
<li class="chapter" data-level="C.1.2" data-path="HIV-appendix.html"><a href="HIV-appendix.html#classifiers"><i class="fa fa-check"></i><b>C.1.2</b> Classifiers</a></li>
<li class="chapter" data-level="C.1.3" data-path="HIV-appendix.html"><a href="HIV-appendix.html#scoring"><i class="fa fa-check"></i><b>C.1.3</b> Scoring</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s1-fig."><i class="fa fa-check"></i><b>C.2</b> S1 Fig.</a></li>
<li class="chapter" data-level="C.3" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s2-fig."><i class="fa fa-check"></i><b>C.3</b> S2 Fig.</a></li>
<li class="chapter" data-level="C.4" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s3-fig."><i class="fa fa-check"></i><b>C.4</b> S3 Fig.</a></li>
<li class="chapter" data-level="C.5" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S1-Table"><i class="fa fa-check"></i><b>C.5</b> S1 Table.</a></li>
<li class="chapter" data-level="C.6" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S2-Appendix"><i class="fa fa-check"></i><b>C.6</b> S2 Appendix. (Fisher exact tests)</a></li>
<li class="chapter" data-level="C.7" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s1-data."><i class="fa fa-check"></i><b>C.7</b> S1 Data.</a></li>
<li class="chapter" data-level="C.8" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s2-data."><i class="fa fa-check"></i><b>C.8</b> S2 Data.</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://lucblassel.com" target="blank">Back to main website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">From sequences to knowledge, improving and learning from sequence alignments</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="learning-alignments-an-interesting-perspective" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Learning alignments, an interesting perspective<a href="learning-alignments-an-interesting-perspective.html#learning-alignments-an-interesting-perspective" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="deep-learning-and-sequences" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Deep learning and sequences<a href="learning-alignments-an-interesting-perspective.html#deep-learning-and-sequences" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="intro-to-deep-learning" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Intro to deep learning<a href="learning-alignments-an-interesting-perspective.html#intro-to-deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The perceptron<span class="citation"><sup><a href="#ref-rosenblattPerceptronProbabilisticModel1958" role="doc-biblioref">671</a></sup></span> &amp; neuron structure (c.f. Figure <a href="learning-alignments-an-interesting-perspective.html#fig:perceptron">7.1</a>:</p>
<ul>
<li><p>Inputs -&gt; weighted sum -&gt; step activation function -&gt; output</p></li>
<li><p>By tweaking the weights you can solve linear separation problems</p></li>
</ul></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:perceptron"></span>
<img src="figures/Learn-alignments/perceptron.png" alt="**Computational graph of a perceptron.**  
Here, $n$ inputs are passed into the perceptron where they are summed, weighted by $w_1,\ldots,w_n$. This sum is then fed through the perceptron's activation function (here a binary step function) which is the output of the perceptron. Often the sum is implicitely considered part of the activation function." width="60%" />
<p class="caption">
Figure 7.1: <strong>Computational graph of a perceptron.</strong><br />
Here, <span class="math inline">\(n\)</span> inputs are passed into the perceptron where they are summed, weighted by <span class="math inline">\(w_1,\ldots,w_n\)</span>. This sum is then fed through the perceptron’s activation function (here a binary step function) which is the output of the perceptron. Often the sum is implicitely considered part of the activation function.
</p>
</div>
<ul>
<li><p>Resurgence in 90s due to Back Prop<span class="citation"><sup><a href="#ref-rumelhartLearningRepresentationsBackpropagating1986" role="doc-biblioref">672</a></sup></span> managed by continuous, differentiable activations -&gt; gradient based training procedures!:</p>
<ul>
<li><p>Sigmoid</p></li>
<li><p>Tanh</p></li>
<li><p>Relu (has some nice convergence properties<span class="citation"><sup><a href="#ref-liConvergenceAnalysisTwolayer2017" role="doc-biblioref">673</a></sup></span></p></li>
</ul></li>
<li><p>and the MLP (c.f. Figure <a href="learning-alignments-an-interesting-perspective.html#fig:mlp">7.2</a>) which is versatile (non linear problems!) and could now be trained efficiently<span class="citation"><sup><a href="#ref-murtaghMultilayerPerceptronsClassification1991" role="doc-biblioref">674</a></sup></span></p></li>
<li><p>Turns out they’re universal function approximators<span class="citation"><sup><a href="#ref-cybenkoApproximationSuperpositionsSigmoidal1989" role="doc-biblioref">675</a>–<a href="#ref-hornikApproximationCapabilitiesMultilayer1991a" role="doc-biblioref">677</a></sup></span> !</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlp"></span>
<img src="figures/Learn-alignments/mlp.png" alt="**Computational graph of a multilayer perceptron.**  
This MLP, also called feedforward neural network, has $n$ inputs represented as the input layer, 2 hidden layers of 3 neurons each and an output layer of 2 neurons (e.g. suitable to binary classification). It is fully connected meaning that each node of a given layer is used as input for every neuron of the following layer. Each edge in this graph is paired to a weight, these are the tunable parameters during the training process." width="60%" />
<p class="caption">
Figure 7.2: <strong>Computational graph of a multilayer perceptron.</strong><br />
This MLP, also called feedforward neural network, has <span class="math inline">\(n\)</span> inputs represented as the input layer, 2 hidden layers of 3 neurons each and an output layer of 2 neurons (e.g. suitable to binary classification). It is fully connected meaning that each node of a given layer is used as input for every neuron of the following layer. Each edge in this graph is paired to a weight, these are the tunable parameters during the training process.
</p>
</div>
<ul>
<li><p>Then came convolution neural networks, explosion in computer vision (Le cun<span class="citation"><sup><a href="#ref-lecunBackpropagationAppliedHandwritten1989" role="doc-biblioref">678</a>,<a href="#ref-lecunGradientbasedLearningApplied1998" role="doc-biblioref">679</a></sup></span>) but also other tasks (steiner<span class="citation"><sup><a href="#ref-steinerDrugResistancePrediction2020a" role="doc-biblioref">329</a></sup></span>, subcellular localization<span class="citation"><sup><a href="#ref-weiPredictionHumanProtein2018" role="doc-biblioref">348</a></sup></span>, epidemiological parameters<span class="citation"><sup><a href="#ref-voznicaDeepLearningPhylogenies2022" role="doc-biblioref">680</a></sup></span>, etc…). an input feature is represented as a weighted linear combination of its neighbors, very interesting you start to inculde “context” in networks.</p></li>
<li><p>Recently super deep network with billions of params, reaching state of the art in many tasks:</p>
<ul>
<li><p>image recognition with deep CNNs like alexnet<span class="citation"><sup><a href="#ref-krizhevskyImageNetClassificationDeep2017" role="doc-biblioref">681</a></sup></span> or resnet<span class="citation"><sup><a href="#ref-heDeepResidualLearning2016" role="doc-biblioref">682</a></sup></span></p></li>
<li><p>translation with RNNs<span class="citation"><sup><a href="#ref-bahdanauNeuralMachineTranslation2016" role="doc-biblioref">683</a></sup></span> and later Transformers<span class="citation"><sup><a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">684</a></sup></span> (more on that below)</p></li>
<li><p>protein folding with alphafold-2<span class="citation"><sup><a href="#ref-jumperHighlyAccurateProtein2021" role="doc-biblioref">136</a></sup></span></p></li>
</ul></li>
</ul>
</div>
<div id="learned-sequence-embeddings" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Learned sequence embeddings<a href="learning-alignments-an-interesting-perspective.html#learned-sequence-embeddings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An area that in which deep learning has proved very useful is creating relevant learned embeddings, idea to capture some context with powerful non-linearity.</p>
<ul>
<li><p>(Variational) Auto-encoders:</p>
<ul>
<li>Bottlneck in deep neural neck, task is to predict input. Add noise in the hidden layers -&gt; remove noise or regularize to have smooth latent space and get embeddings</li>
<li>Used for ancestral sequence reconstruction<span class="citation"><sup><a href="#ref-moretaAncestralProteinSequence2022" role="doc-biblioref">685</a></sup></span> and estimating evolutionary distances<span class="citation"><sup><a href="#ref-corsoNeuralDistanceEmbeddings2021" role="doc-biblioref">358</a></sup></span></li>
<li>VAEs used for sequence design as well<span class="citation"><sup><a href="#ref-wuProteinSequenceDesign2021" role="doc-biblioref">686</a>,<a href="#ref-stantonAcceleratingBayesianOptimization2022" role="doc-biblioref">687</a></sup></span></li>
</ul></li>
</ul>
<p>NLP:</p>
<ul>
<li><p>From the field of natural language processing where very high dimensionality (470,000 words in the Merriam-Webster English dictionary<span class="citation"><sup><a href="#ref-HowManyWords" role="doc-biblioref">688</a></sup></span>, so naive one hot is out of the question), we need other ways to transform words into sequences.</p></li>
<li><p>Method of pre-training embedding methods</p></li>
<li><p>Word2Vec derivatives:</p>
<ul>
<li><p>word2Vec<span class="citation"><sup><a href="#ref-mikolovEfficientEstimationWord2013" role="doc-biblioref">689</a>,<a href="#ref-mikolovDistributedRepresentationsWords2013" role="doc-biblioref">690</a></sup></span>, take in a large corpus of text and learns a vector space from it. Then each word in the corpus can be assigned a vector, constraints mean that similar words have similar vectors (i.e. low distance in the vector space). And that the embeddings make sense grammatically (e.g. of the Paper <span class="math inline">\(vec(Madrid) - vec(Spain)\)</span> should be close to <span class="math inline">\(vec(Paris)\)</span> in the learned space.</p>
<ul>
<li><p>Context of a word = window of <span class="math inline">\(k\)</span> words centered around it</p></li>
<li><p>The model is a neural network and the hidden layer corresponds to the embedding (similar to auto-encoders)</p></li>
<li><p>2 ways to train it<span class="citation"><sup><a href="#ref-goldbergWord2vecExplainedDeriving2014" role="doc-biblioref">691</a></sup></span>:</p>
<ul>
<li><p>CBOW (continuous bag of words) = predict word from context</p></li>
<li><p>skip-gram = predict context from word</p></li>
</ul></li>
</ul></li>
<li><p>dna2vec<span class="citation"><sup><a href="#ref-ngDna2vecConsistentVector2017" role="doc-biblioref">692</a></sup></span></p>
<ul>
<li>Used to predict methylation sites<span class="citation"><sup><a href="#ref-liangHyb4mCHybridDNA2vecbased2022" role="doc-biblioref">693</a></sup></span></li>
</ul></li>
<li><p>seq2vec<span class="citation"><sup><a href="#ref-kimothiDistributedRepresentationsBiological2016" role="doc-biblioref">694</a></sup></span></p></li>
<li><p>BioVec/ProtVec/GeneVec<span class="citation"><sup><a href="#ref-asgariContinuousDistributedRepresentation2015" role="doc-biblioref">695</a></sup></span></p>
<ul>
<li>Seq2vec and ProtVec both used in classification<span class="citation"><sup><a href="#ref-kimothiMetricLearningBiological2017" role="doc-biblioref">696</a></sup></span></li>
</ul></li>
</ul></li>
<li><p>Transformers / NN-based language models:</p>
<ul>
<li><p>Also from NLP, more recent development,</p>
<ul>
<li>Some have seen a lot of success like BERT<span class="citation"><sup><a href="#ref-devlinBERTPretrainingDeep2019" role="doc-biblioref">697</a></sup></span> and GPT-3<span class="citation"><sup><a href="#ref-brownLanguageModelsAre2020" role="doc-biblioref">698</a></sup></span></li>
<li>Based on the very popular Transformer architecture<span class="citation"><sup><a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">684</a></sup></span>, with attention maps. Embed features as a linear weighted sum of other features (learn weights).</li>
<li>Allows for long range dependencies to be captured efficiently</li>
<li>LLMs trained with MLM</li>
<li>Replaced methods based on RNNs / LSTMs which have trouble capturing long range dependencies<span class="citation"><sup><a href="#ref-songPretrainingModelBiological2021" role="doc-biblioref">699</a></sup></span>.</li>
</ul></li>
<li><p>protein language models have been developed from this with the same idea.</p>
<ul>
<li><p>ProGen<span class="citation"><sup><a href="#ref-madaniProGenLanguageModeling2020" role="doc-biblioref">700</a></sup></span> and ProGen2<span class="citation"><sup><a href="#ref-eriknijkampProGen2ExploringBoundaries2022" role="doc-biblioref">701</a></sup></span></p></li>
<li><p>ProtBERT<span class="citation"><sup><a href="#ref-elnaggarProtTransCrackingLanguage2021" role="doc-biblioref">369</a></sup></span></p></li>
<li><p>DNABert<span class="citation"><sup><a href="#ref-jiDNABERTPretrainedBidirectional2021" role="doc-biblioref">702</a></sup></span></p></li>
<li><p>They have interesting properties<span class="citation"><sup><a href="#ref-beplerLearningProteinLanguage2021" role="doc-biblioref">703</a></sup></span>:</p>
<ul>
<li><p>Intuitively learn structure of proteins<span class="citation"><sup><a href="#ref-raoTransformerProteinLanguage2020" role="doc-biblioref">704</a>–<a href="#ref-bhattacharyaSingleLayersAttention2020" role="doc-biblioref">706</a></sup></span></p></li>
<li><p>Protein function<span class="citation"><sup><a href="#ref-huExploringEvolutionbasedFree2022" role="doc-biblioref">707</a></sup></span></p></li>
<li><p>Learn mutational effects<span class="citation"><sup><a href="#ref-meierLanguageModelsEnable2021" role="doc-biblioref">708</a></sup></span></p></li>
<li><p>Evolutionary characteristics<span class="citation"><sup><a href="#ref-hieEvolutionaryVelocityProtein2022" role="doc-biblioref">709</a></sup></span></p></li>
<li><p>Even used to generate relevant protein sequences<span class="citation"><sup><a href="#ref-madaniProGenLanguageModeling2020" role="doc-biblioref">700</a></sup></span></p></li>
<li><p>Even when trained on DNA, they learn variant effects<span class="citation"><sup><a href="#ref-benegasDNALanguageModels2022" role="doc-biblioref">710</a></sup></span></p></li>
</ul></li>
</ul></li>
<li><p>Include information from MSA directly in embedding<span class="citation"><sup><a href="#ref-caiGenomewidePredictionSmall2020" role="doc-biblioref">711</a></sup></span>: transform aligned sequence in to tokens -&gt; use ALBERT to embed tokens</p></li>
<li><p>MSA Transformer<span class="citation"><sup><a href="#ref-raoMSATransformer2021" role="doc-biblioref">712</a></sup></span> that extends attention to include aligned residues from an input MSA as well.</p>
<ul>
<li><p>Similarly: learn on profiles derived from MSAs<span class="citation"><sup><a href="#ref-sturmfelsProfilePredictionAlignmentBased2020" role="doc-biblioref">713</a></sup></span> as a pre-training task for protein language models</p></li>
<li><p>Learn a protein structure model (potts model) directly on the MSA with a mechanism similar to attention<span class="citation"><sup><a href="#ref-sercuNeuralPottsModel2021" role="doc-biblioref">714</a></sup></span></p></li>
<li><p>Transformer used subreads aligned to CCS + sequencer info to polish HiFi reads<span class="citation"><sup><a href="#ref-baidDeepConsensusImprovesAccuracy2022" role="doc-biblioref">715</a></sup></span></p></li>
</ul></li>
<li><p>EvoFormer from Alphafold2<span class="citation"><sup><a href="#ref-jumperHighlyAccurateProtein2021" role="doc-biblioref">136</a></sup></span> also takes MSAs as inputs to predict protein contacts (and is trained end-to-end)</p></li>
</ul></li>
<li><p>Powerful but hard to interpret what the model actually learns. i.e. “black box” but some work is being done to interpret attention maps<span class="citation"><sup><a href="#ref-vigBERTologyMeetsBiology2021" role="doc-biblioref">716</a></sup></span></p></li>
</ul>
</div>
</div>
<div id="learning-pairwise-alignment" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Learning pairwise alignment<a href="learning-alignments-an-interesting-perspective.html#learning-pairwise-alignment" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="predicting-a-substitution-matrix" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Predicting a substitution matrix<a href="learning-alignments-an-interesting-perspective.html#predicting-a-substitution-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One approach is to learn a substitution matrix (specific position/position scoring matrix) and plug it in a differentiable SW or NW algorithm (for end-to-end learning):</p>
<ul>
<li><p>SAdLSA<span class="citation"><sup><a href="#ref-gaoNovelSequenceAlignment2021" role="doc-biblioref">717</a></sup></span></p>
<ul>
<li><p>sequences are encoded as PSI blast profiles</p></li>
<li><p>Fed through a deep CNN</p></li>
<li><p>predict a scoring matrix</p></li>
<li><p>No differentiable alignment algorithm -&gt; cross entropy between alignment and structural alignment.</p></li>
</ul></li>
<li><p>DeepBLAST<span class="citation"><sup><a href="#ref-mortonProteinStructuralAlignments2020" role="doc-biblioref">718</a></sup></span></p>
<ul>
<li><p>Embed sequences with LSTM-based language model (trained on PFAM)</p></li>
<li><p>predict substitution/gap score</p></li>
<li><p>differentiable NW (not to learn parameters but only to backpropagate the error)</p></li>
</ul></li>
<li><p>DEDAL<span class="citation"><sup><a href="#ref-llinares-lopezDeepEmbeddingAlignment2022" role="doc-biblioref">719</a></sup></span></p>
<ul>
<li><p>2 seqs are embedded with encoder-only transformer</p></li>
<li><p>Trained on TPUs with a fast differentiable algo (SW ?)</p></li>
<li><p>Training set: parwise alignments extracted from PFAM</p></li>
<li><p>predicts substitution, gap open and extend scoring matrices (position per position)</p></li>
<li><p>improves alignment for remote homologies</p></li>
</ul></li>
<li><p>The Learned Alignement module<span class="citation"><sup><a href="#ref-pettiEndtoendLearningMultiple2022" role="doc-biblioref">720</a></sup></span></p>
<ul>
<li><p>Learns a “context specific scoring matrix”, i.e. a 20x20 matrix for a window around a given position.</p></li>
<li><p>Differentiable SW</p></li>
<li><p>Uses convolutional NN</p></li>
<li><p>Learns an “MSA”, actually outputs all to one pairwise alignments.</p></li>
<li><p>Used to as plugin to alphafold2 and improved some metrics</p></li>
</ul></li>
<li><p>Prediction of PSSM with RNN + LSTM<span class="citation"><sup><a href="#ref-guoComprehensiveStudyEnhancing2021" role="doc-biblioref">721</a></sup></span> , not directly used on alignment but structure prediction.</p></li>
</ul>
</div>
<div id="predicting-an-alignment" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> predicting an alignment<a href="learning-alignments-an-interesting-perspective.html#predicting-an-alignment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>BetaAlign<span class="citation"><sup><a href="#ref-dotanHarnessingMachineTranslation2022" role="doc-biblioref">722</a></sup></span></p>
<ul>
<li><p>Unaligned sequences = a “language”</p></li>
<li><p>Aligned sequences = another “language”</p></li>
<li><p>So use transformers to translate one into the other (tried several ways to represent these “languages”)</p></li>
<li><p>Tested with both DNA and protein</p></li>
<li><p>limitations:</p>
<ul>
<li><p>length of sequences</p></li>
<li><p>Training / testing set</p></li>
</ul></li>
</ul></li>
<li><p>Another direction could be to predict the state from 2 residues (like a PSSM but directly match/indel).</p></li>
</ul>
<!-- -->
<ul>
<li><p>To counter the space limitations (i.e. sequence length limitations) induced by attention, other types of transformers used:</p>
<ul>
<li><p>with linear scale attention maps not quadratic<span class="citation"><sup><a href="#ref-choromanskiMaskedLanguageModeling2020" role="doc-biblioref">723</a></sup></span></p></li>
<li><p>single layer attention or factored attention<span class="citation"><sup><a href="#ref-bhattacharyaInterpretingPottsTransformer2021" role="doc-biblioref">724</a></sup></span> which lowers the number of parameters to estimate but keeps relevant information.</p></li>
</ul></li>
<li><p>This is not a problem limited to bioinformatics, other fields have tried to come up with solutions:</p>
<ul>
<li><p>adaptive attention span<span class="citation"><sup><a href="#ref-sukhbaatarAdaptiveAttentionSpan2019" role="doc-biblioref">725</a></sup></span></p></li>
<li><p>Long-Short range attention<span class="citation"><sup><a href="#ref-wuLiteTransformerLongShort2020" role="doc-biblioref">726</a></sup></span></p></li>
<li><p>sparse transformers<span class="citation"><sup><a href="#ref-childGeneratingLongSequences2019" role="doc-biblioref">727</a>,<a href="#ref-correiaAdaptivelySparseTransformers2019" role="doc-biblioref">728</a></sup></span></p></li>
<li><p>Reformer replace dot product to reduce memory from quadratic to linear<span class="citation"><sup><a href="#ref-kitaevReformerEfficientTransformer2020" role="doc-biblioref">729</a></sup></span></p></li>
</ul></li>
</ul>
<p>For learning DNA alignment several challenges:</p>
<ul>
<li><p>longer sequences</p></li>
<li><p>less information in a single residue than a single nucleotide</p></li>
<li><p>In mapping size discrepancy between sequences.</p></li>
</ul>
</div>
<div id="learning-seeds" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Learning seeds<a href="learning-alignments-an-interesting-perspective.html#learning-seeds" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Some work has been done in learning seeding procedures</p>
<ul>
<li><p>Learn index structures on a specific reference (not necessarily DL), although fairly recent developments already thought of in 2018<span class="citation"><sup><a href="#ref-kraskaCaseLearnedIndex2018" role="doc-biblioref">730</a></sup></span>:</p>
<ul>
<li><p>BWA-MEME<span class="citation"><sup><a href="#ref-jungBWAMEMEBWAMEMEmulated2022" role="doc-biblioref">731</a></sup></span> predicts the position in a suffix array, lowering query time and no need to compute the whole suffix array</p></li>
<li><p>Sapling<span class="citation"><sup><a href="#ref-kirscheSaplingAcceleratingSuffix2021" role="doc-biblioref">732</a></sup></span> same thing</p></li>
<li><p>LISA<span class="citation"><sup><a href="#ref-hoLISALearnedIndexes2021" role="doc-biblioref">733</a></sup></span> predict position in FM-index</p></li>
</ul></li>
</ul>
<!-- -->
<ul>
<li><p>DeepMinimizer<span class="citation"><sup><a href="#ref-hoangDifferentiableLearningSequenceSpecific2022" role="doc-biblioref">734</a></sup></span>:</p>
<ul>
<li><p>Train a neural network to select minimizers</p></li>
<li><p>Results in a better density, seeds are spread out evenly accross sequences</p></li>
</ul></li>
<li><p>Select candidate alignment sites in mRNA-miRNA pairs with DL: TargetNet<span class="citation"><sup><a href="#ref-minTargetNetFunctionalMicroRNA2022" role="doc-biblioref">735</a></sup></span></p></li>
</ul>
<p>Final note, we could also learn a pre-processing function as in Chapter <a href="HPC-paper.html#HPC-paper">3</a> in an end to end fashion: either by learning the connections in MSRs or by learning transformations with sequence to sequence models (like transformers). This is still a little abstract and would need a differentiable read mapping algo for end-to-end learning (SW is possible but seeding -&gt; deepminimizer ?).</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" line-spacing="2">
<div id="ref-jumperHighlyAccurateProtein2021" class="csl-entry">
<div class="csl-left-margin">136. </div><div class="csl-right-inline">Jumper, J. <em>et al.</em> <a href="https://doi.org/10.1038/s41586-021-03819-2">Highly accurate protein structure prediction with AlphaFold</a>. <em>Nature</em> <strong>596</strong>, 583–589 (2021).</div>
</div>
<div id="ref-steinerDrugResistancePrediction2020a" class="csl-entry">
<div class="csl-left-margin">329. </div><div class="csl-right-inline">Steiner, M. C., Gibson, K. M. &amp; Crandall, K. A. <a href="https://doi.org/10.3390/v12050560">Drug <span>Resistance Prediction Using Deep Learning Techniques</span> on <span>HIV</span>-1 <span>Sequence Data</span></a>. <em>Viruses</em> <strong>12</strong>, 560 (2020).</div>
</div>
<div id="ref-weiPredictionHumanProtein2018" class="csl-entry">
<div class="csl-left-margin">348. </div><div class="csl-right-inline">Wei, L., Ding, Y., Su, R., Tang, J. &amp; Zou, Q. <a href="https://doi.org/10.1016/j.jpdc.2017.08.009">Prediction of human protein subcellular localization using deep learning</a>. <em>Journal of Parallel and Distributed Computing</em> <strong>117</strong>, 212–217 (2018).</div>
</div>
<div id="ref-corsoNeuralDistanceEmbeddings2021" class="csl-entry">
<div class="csl-left-margin">358. </div><div class="csl-right-inline">Corso, G. <em>et al.</em> Neural distance embeddings for biological sequences. in vol. 34 1853918551 (Curran Associates, Inc., 2021).</div>
</div>
<div id="ref-elnaggarProtTransCrackingLanguage2021" class="csl-entry">
<div class="csl-left-margin">369. </div><div class="csl-right-inline">Elnaggar, A. <em>et al.</em> ProtTrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing. doi:<a href="https://doi.org/10.48550/arXiv.2007.06225">10.48550/arXiv.2007.06225</a>.</div>
</div>
<div id="ref-rosenblattPerceptronProbabilisticModel1958" class="csl-entry">
<div class="csl-left-margin">671. </div><div class="csl-right-inline">Rosenblatt, F. <a href="https://doi.org/10.1037/h0042519">The perceptron: A probabilistic model for information storage and organization in the brain</a>. <em>Psychological Review</em> <strong>65</strong>, 386–408 (1958).</div>
</div>
<div id="ref-rumelhartLearningRepresentationsBackpropagating1986" class="csl-entry">
<div class="csl-left-margin">672. </div><div class="csl-right-inline">Rumelhart, D. E., Hinton, G. E. &amp; Williams, R. J. <a href="https://doi.org/10.1038/323533a0">Learning representations by back-propagating errors</a>. <em>Nature</em> <strong>323</strong>, 533–536 (1986).</div>
</div>
<div id="ref-liConvergenceAnalysisTwolayer2017" class="csl-entry">
<div class="csl-left-margin">673. </div><div class="csl-right-inline">Li, Y. &amp; Yuan, Y. <a href="https://proceedings.neurips.cc/paper/2017/hash/a96b65a721e561e1e3de768ac819ffbb-Abstract.html">Convergence analysis of two-layer neural networks with ReLU activation</a>. in vol. 30 (Curran Associates, Inc., 2017).</div>
</div>
<div id="ref-murtaghMultilayerPerceptronsClassification1991" class="csl-entry">
<div class="csl-left-margin">674. </div><div class="csl-right-inline">Murtagh, F. <a href="https://doi.org/10.1016/0925-2312(91)90023-5">Multilayer perceptrons for classification and regression</a>. <em>Neurocomputing</em> <strong>2</strong>, 183–197 (1991).</div>
</div>
<div id="ref-cybenkoApproximationSuperpositionsSigmoidal1989" class="csl-entry">
<div class="csl-left-margin">675. </div><div class="csl-right-inline">Cybenko, G. <a href="https://doi.org/10.1007/BF02551274">Approximation by superpositions of a sigmoidal function</a>. <em>Mathematics of Control, Signals and Systems</em> <strong>2</strong>, 303–314 (1989).</div>
</div>
<div id="ref-hornikApproximationCapabilitiesMultilayer1991a" class="csl-entry">
<div class="csl-left-margin">677. </div><div class="csl-right-inline">Hornik, K. <a href="https://doi.org/10.1016/0893-6080(91)90009-T">Approximation capabilities of multilayer feedforward networks</a>. <em>Neural Networks</em> <strong>4</strong>, 251–257 (1991).</div>
</div>
<div id="ref-lecunBackpropagationAppliedHandwritten1989" class="csl-entry">
<div class="csl-left-margin">678. </div><div class="csl-right-inline">LeCun, Y. <em>et al.</em> <a href="https://doi.org/10.1162/neco.1989.1.4.541">Backpropagation applied to handwritten zip code recognition</a>. <em>Neural Computation</em> <strong>1</strong>, 541–551 (1989).</div>
</div>
<div id="ref-lecunGradientbasedLearningApplied1998" class="csl-entry">
<div class="csl-left-margin">679. </div><div class="csl-right-inline">Lecun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. <a href="https://doi.org/10.1109/5.726791">Gradient-based learning applied to document recognition</a>. <em>Proceedings of the IEEE</em> <strong>86</strong>, 2278–2324 (1998).</div>
</div>
<div id="ref-voznicaDeepLearningPhylogenies2022" class="csl-entry">
<div class="csl-left-margin">680. </div><div class="csl-right-inline">Voznica, J. <em>et al.</em> <a href="https://doi.org/10.1038/s41467-022-31511-0">Deep learning from phylogenies to uncover the epidemiological dynamics of outbreaks</a>. <em>Nature Communications</em> <strong>13</strong>, 3896 (2022).</div>
</div>
<div id="ref-krizhevskyImageNetClassificationDeep2017" class="csl-entry">
<div class="csl-left-margin">681. </div><div class="csl-right-inline">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. <a href="https://doi.org/10.1145/3065386">ImageNet classification with deep convolutional neural networks</a>. <em>Communications of the ACM</em> <strong>60</strong>, 8490 (2017).</div>
</div>
<div id="ref-heDeepResidualLearning2016" class="csl-entry">
<div class="csl-left-margin">682. </div><div class="csl-right-inline">He, K., Zhang, X., Ren, S. &amp; Sun, J. <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Proceedings of the IEEE conference on computer vision and pattern recognition</a>. in 770–778 (2016).</div>
</div>
<div id="ref-bahdanauNeuralMachineTranslation2016" class="csl-entry">
<div class="csl-left-margin">683. </div><div class="csl-right-inline">Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. doi:<a href="https://doi.org/10.48550/arXiv.1409.0473">10.48550/arXiv.1409.0473</a>.</div>
</div>
<div id="ref-vaswaniAttentionAllYou2017" class="csl-entry">
<div class="csl-left-margin">684. </div><div class="csl-right-inline">Vaswani, A. <em>et al.</em> Attention is all you need. in vol. 30 (Curran Associates, Inc., 2017).</div>
</div>
<div id="ref-moretaAncestralProteinSequence2022" class="csl-entry">
<div class="csl-left-margin">685. </div><div class="csl-right-inline">Moreta, L. S. <em>et al.</em> International Conference on Learning Representations. in (2022).</div>
</div>
<div id="ref-wuProteinSequenceDesign2021" class="csl-entry">
<div class="csl-left-margin">686. </div><div class="csl-right-inline">Wu, Z., Johnston, K. E., Arnold, F. H. &amp; Yang, K. K. <a href="https://doi.org/10.1016/j.cbpa.2021.04.004">Protein sequence design with deep generative models</a>. <em>Current Opinion in Chemical Biology</em> <strong>65</strong>, 18–27 (2021).</div>
</div>
<div id="ref-stantonAcceleratingBayesianOptimization2022" class="csl-entry">
<div class="csl-left-margin">687. </div><div class="csl-right-inline">Stanton, S. <em>et al.</em> Accelerating bayesian optimization for biological sequence design with denoising autoencoders. doi:<a href="https://doi.org/10.48550/arXiv.2203.12742">10.48550/arXiv.2203.12742</a>.</div>
</div>
<div id="ref-HowManyWords" class="csl-entry">
<div class="csl-left-margin">688. </div><div class="csl-right-inline"><a href="https://www.merriam-webster.com/help/faq-how-many-english-words">How many words are there in english? | merriam-webster</a>.</div>
</div>
<div id="ref-mikolovEfficientEstimationWord2013" class="csl-entry">
<div class="csl-left-margin">689. </div><div class="csl-right-inline">Mikolov, T., Chen, K., Corrado, G. &amp; Dean, J. Efficient estimation of word representations in vector space. doi:<a href="https://doi.org/10.48550/arXiv.1301.3781">10.48550/arXiv.1301.3781</a>.</div>
</div>
<div id="ref-mikolovDistributedRepresentationsWords2013" class="csl-entry">
<div class="csl-left-margin">690. </div><div class="csl-right-inline">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. &amp; Dean, J. Distributed representations of words and phrases and their compositionality. in vol. 26 (Curran Associates, Inc., 2013).</div>
</div>
<div id="ref-goldbergWord2vecExplainedDeriving2014" class="csl-entry">
<div class="csl-left-margin">691. </div><div class="csl-right-inline">Goldberg, Y. &amp; Levy, O. word2vec explained: Deriving mikolov et al.’s negative-sampling word-embedding method. doi:<a href="https://doi.org/10.48550/arXiv.1402.3722">10.48550/arXiv.1402.3722</a>.</div>
</div>
<div id="ref-ngDna2vecConsistentVector2017" class="csl-entry">
<div class="csl-left-margin">692. </div><div class="csl-right-inline">Ng, P. dna2vec: Consistent vector representations of variable-length k-mers. doi:<a href="https://doi.org/10.48550/arXiv.1701.06279">10.48550/arXiv.1701.06279</a>.</div>
</div>
<div id="ref-liangHyb4mCHybridDNA2vecbased2022" class="csl-entry">
<div class="csl-left-margin">693. </div><div class="csl-right-inline">Liang, Y. <em>et al.</em> <a href="https://doi.org/10.1186/s12859-022-04789-6">Hyb4mC: a hybrid DNA2vec-based model for DNA N4-methylcytosine sites prediction</a>. <em>BMC Bioinformatics</em> <strong>23</strong>, 258 (2022).</div>
</div>
<div id="ref-kimothiDistributedRepresentationsBiological2016" class="csl-entry">
<div class="csl-left-margin">694. </div><div class="csl-right-inline">Kimothi, D., Soni, A., Biyani, P. &amp; Hogan, J. M. Distributed representations for biological sequence analysis. doi:<a href="https://doi.org/10.48550/arXiv.1608.05949">10.48550/arXiv.1608.05949</a>.</div>
</div>
<div id="ref-asgariContinuousDistributedRepresentation2015" class="csl-entry">
<div class="csl-left-margin">695. </div><div class="csl-right-inline">Asgari, E. &amp; Mofrad, M. R. K. <a href="https://doi.org/10.1371/journal.pone.0141287">Continuous distributed representation of biological sequences for deep proteomics and genomics</a>. <em>PLoS ONE</em> <strong>10</strong>, e0141287 (2015).</div>
</div>
<div id="ref-kimothiMetricLearningBiological2017" class="csl-entry">
<div class="csl-left-margin">696. </div><div class="csl-right-inline">Kimothi, D., Shukla, A., Biyani, P., Anand, S. &amp; Hogan, J. M. 2017 IEEE 18th international workshop on signal processing advances in wireless communications (SPAWC). in 1–5 (2017). doi:<a href="https://doi.org/10.1109/SPAWC.2017.8227769">10.1109/SPAWC.2017.8227769</a>.</div>
</div>
<div id="ref-devlinBERTPretrainingDeep2019" class="csl-entry">
<div class="csl-left-margin">697. </div><div class="csl-right-inline">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. doi:<a href="https://doi.org/10.48550/arXiv.1810.04805">10.48550/arXiv.1810.04805</a>.</div>
</div>
<div id="ref-brownLanguageModelsAre2020" class="csl-entry">
<div class="csl-left-margin">698. </div><div class="csl-right-inline">Brown, T. <em>et al.</em> Language models are few-shot learners. in vol. 33 18771901 (Curran Associates, Inc., 2020).</div>
</div>
<div id="ref-songPretrainingModelBiological2021" class="csl-entry">
<div class="csl-left-margin">699. </div><div class="csl-right-inline">Song, B. <em>et al.</em> <a href="https://doi.org/10.1093/bfgp/elab025">Pretraining model for biological sequence data.</a> <em>Briefings in Functional Genomics</em> <strong>20</strong>, 181–195 (2021).</div>
</div>
<div id="ref-madaniProGenLanguageModeling2020" class="csl-entry">
<div class="csl-left-margin">700. </div><div class="csl-right-inline">Madani, A. <em>et al.</em> ProGen: Language modeling for protein generation. <em>bioRxiv</em> (2020) doi:<a href="https://doi.org/10.1101/2020.03.07.982272">10.1101/2020.03.07.982272</a>.</div>
</div>
<div id="ref-eriknijkampProGen2ExploringBoundaries2022" class="csl-entry">
<div class="csl-left-margin">701. </div><div class="csl-right-inline">Erik Nijkamp, Jeffrey A. Ruffolo, Eli N. Weinstein, Nikhil Naik &amp; Ali Madani. ProGen2: Exploring the boundaries of protein language models. <em>ArXiv</em> (2022) doi:<a href="https://doi.org/10.48550/arxiv.2206.13517">10.48550/arxiv.2206.13517</a>.</div>
</div>
<div id="ref-jiDNABERTPretrainedBidirectional2021" class="csl-entry">
<div class="csl-left-margin">702. </div><div class="csl-right-inline">Ji, Y., Zhou, Z., Liu, H. &amp; Davuluri, R. V. <a href="https://doi.org/10.1093/bioinformatics/btab083">DNABERT: Pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</a>. <em>Bioinformatics</em> <strong>37</strong>, 2112–2120 (2021).</div>
</div>
<div id="ref-beplerLearningProteinLanguage2021" class="csl-entry">
<div class="csl-left-margin">703. </div><div class="csl-right-inline">Bepler, T. &amp; Berger, B. <a href="https://doi.org/10.1016/j.cels.2021.05.017">Learning the protein language: Evolution, structure, and function.</a> <em>Cell systems</em> <strong>12</strong>, (2021).</div>
</div>
<div id="ref-raoTransformerProteinLanguage2020" class="csl-entry">
<div class="csl-left-margin">704. </div><div class="csl-right-inline">Rao, R., Meier, J., Sercu, T., Ovchinnikov, S. &amp; Rives, A. Transformer protein language models are unsupervised structure learners. doi:<a href="https://doi.org/10.1101/2020.12.15.422761">10.1101/2020.12.15.422761</a>.</div>
</div>
<div id="ref-bhattacharyaSingleLayersAttention2020" class="csl-entry">
<div class="csl-left-margin">706. </div><div class="csl-right-inline">Bhattacharya, N. <em>et al.</em> Single Layers of Attention Suffice to Predict Protein Contacts. doi:<a href="https://doi.org/10.1101/2020.12.21.423882">10.1101/2020.12.21.423882</a>.</div>
</div>
<div id="ref-huExploringEvolutionbasedFree2022" class="csl-entry">
<div class="csl-left-margin">707. </div><div class="csl-right-inline">Hu, M. <em>et al.</em> Exploring evolution-based &amp; -free protein language models as protein function predictors. doi:<a href="https://doi.org/10.48550/arXiv.2206.06583">10.48550/arXiv.2206.06583</a>.</div>
</div>
<div id="ref-meierLanguageModelsEnable2021" class="csl-entry">
<div class="csl-left-margin">708. </div><div class="csl-right-inline">Meier, J. <em>et al.</em> <a href="https://doi.org/10.1101/2021.07.09.450648">Language models enable zero-shot prediction of the effects of mutations on protein function</a>. <em>bioRxiv</em> <strong>34</strong>, (2021).</div>
</div>
<div id="ref-hieEvolutionaryVelocityProtein2022" class="csl-entry">
<div class="csl-left-margin">709. </div><div class="csl-right-inline">Hie, B., Kevin K Yang &amp; Kim, S. K. Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins. <em>Cell systems</em> (2022) doi:<a href="https://doi.org/10.1016/j.cels.2022.01.003">10.1016/j.cels.2022.01.003</a>.</div>
</div>
<div id="ref-benegasDNALanguageModels2022" class="csl-entry">
<div class="csl-left-margin">710. </div><div class="csl-right-inline">Benegas, G., Batra, S. S. &amp; Song, Y. S. DNA language models are powerful zero-shot predictors of non-coding variant effects. doi:<a href="https://doi.org/10.1101/2022.08.22.504706">10.1101/2022.08.22.504706</a>.</div>
</div>
<div id="ref-caiGenomewidePredictionSmall2020" class="csl-entry">
<div class="csl-left-margin">711. </div><div class="csl-right-inline">Cai, T. <em>et al.</em> Genome-wide Prediction of Small Molecule Binding to Remote Orphan Proteins Using Distilled Sequence Alignment Embedding. doi:<a href="https://doi.org/10.1101/2020.08.04.236729">10.1101/2020.08.04.236729</a>.</div>
</div>
<div id="ref-raoMSATransformer2021" class="csl-entry">
<div class="csl-left-margin">712. </div><div class="csl-right-inline">Rao, R. <em>et al.</em> MSA transformer. <em>bioRxiv</em> (2021) doi:<a href="https://doi.org/10.1101/2021.02.12.430858">10.1101/2021.02.12.430858</a>.</div>
</div>
<div id="ref-sturmfelsProfilePredictionAlignmentBased2020" class="csl-entry">
<div class="csl-left-margin">713. </div><div class="csl-right-inline">Sturmfels, P., Vig, J., Madani, A. &amp; Rajani, N. F. Profile prediction: An alignment-based pre-training task for protein sequence models. doi:<a href="https://doi.org/10.48550/arXiv.2012.00195">10.48550/arXiv.2012.00195</a>.</div>
</div>
<div id="ref-sercuNeuralPottsModel2021" class="csl-entry">
<div class="csl-left-margin">714. </div><div class="csl-right-inline">Sercu, T. <em>et al.</em> Neural Potts Model. doi:<a href="https://doi.org/10.1101/2021.04.08.439084">10.1101/2021.04.08.439084</a>.</div>
</div>
<div id="ref-baidDeepConsensusImprovesAccuracy2022" class="csl-entry">
<div class="csl-left-margin">715. </div><div class="csl-right-inline">Baid, G. <em>et al.</em> DeepConsensus improves the accuracy of sequences with a gap-aware sequence transformer. <em>Nature Biotechnology</em> 1–7 (2022) doi:<a href="https://doi.org/10.1038/s41587-022-01435-7">10.1038/s41587-022-01435-7</a>.</div>
</div>
<div id="ref-vigBERTologyMeetsBiology2021" class="csl-entry">
<div class="csl-left-margin">716. </div><div class="csl-right-inline">Vig, J. <em>et al.</em> BERTology meets biology: Interpreting attention in protein language models. doi:<a href="https://doi.org/10.48550/arXiv.2006.15222">10.48550/arXiv.2006.15222</a>.</div>
</div>
<div id="ref-gaoNovelSequenceAlignment2021" class="csl-entry">
<div class="csl-left-margin">717. </div><div class="csl-right-inline">Gao, M. &amp; Skolnick, J. <a href="https://doi.org/10.1093/bioinformatics/btaa810">A novel sequence alignment algorithm based on deep learning of the protein folding code</a>. <em>Bioinformatics</em> <strong>37</strong>, 490–496 (2021).</div>
</div>
<div id="ref-mortonProteinStructuralAlignments2020" class="csl-entry">
<div class="csl-left-margin">718. </div><div class="csl-right-inline">Morton, J. T. <em>et al.</em> Protein Structural Alignments From Sequence. doi:<a href="https://doi.org/10.1101/2020.11.03.365932">10.1101/2020.11.03.365932</a>.</div>
</div>
<div id="ref-llinares-lopezDeepEmbeddingAlignment2022" class="csl-entry">
<div class="csl-left-margin">719. </div><div class="csl-right-inline">Llinares-López, F., Berthet, Q., Blondel, M., Teboul, O. &amp; Vert, J.-P. Deep embedding and alignment of protein sequences. doi:<a href="https://doi.org/10.1101/2021.11.15.468653">10.1101/2021.11.15.468653</a>.</div>
</div>
<div id="ref-pettiEndtoendLearningMultiple2022" class="csl-entry">
<div class="csl-left-margin">720. </div><div class="csl-right-inline">Petti, S. <em>et al.</em> End-to-end learning of multiple sequence alignments with differentiable Smith-Waterman. doi:<a href="https://doi.org/10.1101/2021.10.23.465204">10.1101/2021.10.23.465204</a>.</div>
</div>
<div id="ref-guoComprehensiveStudyEnhancing2021" class="csl-entry">
<div class="csl-left-margin">721. </div><div class="csl-right-inline">Guo, Y., Wu, J., Ma, H., Wang, S. &amp; Huang, J. <a href="https://doi.org/10.1089/cmb.2020.0416">Comprehensive study on enhancing low-quality position-specific scoring matrix with deep learning for accurate protein structure property prediction: Using bagging multiple sequence alignment learning</a>. <em>Journal of Computational Biology</em> <strong>28</strong>, 346–361 (2021).</div>
</div>
<div id="ref-dotanHarnessingMachineTranslation2022" class="csl-entry">
<div class="csl-left-margin">722. </div><div class="csl-right-inline">Dotan, E. <em>et al.</em> Harnessing machine translation methods for sequence alignment. doi:<a href="https://doi.org/10.1101/2022.07.22.501063">10.1101/2022.07.22.501063</a>.</div>
</div>
<div id="ref-choromanskiMaskedLanguageModeling2020" class="csl-entry">
<div class="csl-left-margin">723. </div><div class="csl-right-inline">Choromanski, K. <em>et al.</em> Masked language modeling for proteins via linearly scalable long-context transformers. doi:<a href="https://doi.org/10.48550/arXiv.2006.03555">10.48550/arXiv.2006.03555</a>.</div>
</div>
<div id="ref-bhattacharyaInterpretingPottsTransformer2021" class="csl-entry">
<div class="csl-left-margin">724. </div><div class="csl-right-inline">Bhattacharya, N. <em>et al.</em> Interpreting potts and transformer protein models through the lens of simplified attention. in 34–45 (WORLD SCIENTIFIC, 2021). doi:<a href="https://doi.org/10.1142/9789811250477_0004">10.1142/9789811250477_0004</a>.</div>
</div>
<div id="ref-sukhbaatarAdaptiveAttentionSpan2019" class="csl-entry">
<div class="csl-left-margin">725. </div><div class="csl-right-inline">Sukhbaatar, S., Grave, E., Bojanowski, P. &amp; Joulin, A. Adaptive attention span in transformers. doi:<a href="https://doi.org/10.48550/arXiv.1905.07799">10.48550/arXiv.1905.07799</a>.</div>
</div>
<div id="ref-wuLiteTransformerLongShort2020" class="csl-entry">
<div class="csl-left-margin">726. </div><div class="csl-right-inline">Wu, Z., Liu, Z., Lin, J., Lin, Y. &amp; Han, S. Lite transformer with long-short range attention. doi:<a href="https://doi.org/10.48550/arXiv.2004.11886">10.48550/arXiv.2004.11886</a>.</div>
</div>
<div id="ref-childGeneratingLongSequences2019" class="csl-entry">
<div class="csl-left-margin">727. </div><div class="csl-right-inline">Child, R., Gray, S., Radford, A. &amp; Sutskever, I. Generating long sequences with sparse transformers. doi:<a href="https://doi.org/10.48550/arXiv.1904.10509">10.48550/arXiv.1904.10509</a>.</div>
</div>
<div id="ref-correiaAdaptivelySparseTransformers2019" class="csl-entry">
<div class="csl-left-margin">728. </div><div class="csl-right-inline">Correia, G. M., Niculae, V. &amp; Martins, A. F. T. EMNLP-IJCNLP 2019. in 21742184 (Association for Computational Linguistics, 2019). doi:<a href="https://doi.org/10.18653/v1/D19-1223">10.18653/v1/D19-1223</a>.</div>
</div>
<div id="ref-kitaevReformerEfficientTransformer2020" class="csl-entry">
<div class="csl-left-margin">729. </div><div class="csl-right-inline">Kitaev, N., Kaiser, Ł. &amp; Levskaya, A. Reformer: The efficient transformer. doi:<a href="https://doi.org/10.48550/arXiv.2001.04451">10.48550/arXiv.2001.04451</a>.</div>
</div>
<div id="ref-kraskaCaseLearnedIndex2018" class="csl-entry">
<div class="csl-left-margin">730. </div><div class="csl-right-inline">Kraska, T., Beutel, A., Chi, E. H., Dean, J. &amp; Polyzotis, N. The case for learned index structures. in 489504 (Association for Computing Machinery, 2018). doi:<a href="https://doi.org/10.1145/3183713.3196909">10.1145/3183713.3196909</a>.</div>
</div>
<div id="ref-jungBWAMEMEBWAMEMEmulated2022" class="csl-entry">
<div class="csl-left-margin">731. </div><div class="csl-right-inline">Jung, Y. &amp; Han, D. <a href="https://doi.org/10.1093/bioinformatics/btac137">BWA-MEME: BWA-MEM emulated with a machine learning approach</a>. <em>Bioinformatics</em> <strong>38</strong>, 2404–2413 (2022).</div>
</div>
<div id="ref-kirscheSaplingAcceleratingSuffix2021" class="csl-entry">
<div class="csl-left-margin">732. </div><div class="csl-right-inline">Kirsche, M., Das, A. &amp; Schatz, M. C. <a href="https://doi.org/10.1093/bioinformatics/btaa911">Sapling: Accelerating suffix array queries with learned data models</a>. <em>Bioinformatics</em> <strong>37</strong>, 744–749 (2021).</div>
</div>
<div id="ref-hoLISALearnedIndexes2021" class="csl-entry">
<div class="csl-left-margin">733. </div><div class="csl-right-inline">Ho, D. <em>et al.</em> LISA: Learned Indexes for Sequence Analysis. doi:<a href="https://doi.org/10.1101/2020.12.22.423964">10.1101/2020.12.22.423964</a>.</div>
</div>
<div id="ref-hoangDifferentiableLearningSequenceSpecific2022" class="csl-entry">
<div class="csl-left-margin">734. </div><div class="csl-right-inline">Hoang, M., Zheng, H. &amp; Kingsford, C. Differentiable learning of sequence-specific minimizer schemes with DeepMinimizer. <em>Journal of Computational Biology</em> (2022) doi:<a href="https://doi.org/10.1089/cmb.2022.0275">10.1089/cmb.2022.0275</a>.</div>
</div>
<div id="ref-minTargetNetFunctionalMicroRNA2022" class="csl-entry">
<div class="csl-left-margin">735. </div><div class="csl-right-inline">Min, S., Lee, B. &amp; Yoon, S. <a href="https://doi.org/10.1093/bioinformatics/btab733">TargetNet: Functional microRNA target prediction with deep neural networks</a>. <em>Bioinformatics</em> <strong>38</strong>, 671–677 (2022).</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="HIV-paper.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="global-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lucblassel/phd-manuscript/edit/main/07-learning-alignments.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

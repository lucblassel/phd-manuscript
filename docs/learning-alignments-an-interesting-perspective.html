<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments</title>
  <meta name="description" content="Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="lucblassel/phd-manuscript" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments" />
  
  
  

<meta name="author" content="Luc Blassel" />


<meta name="date" content="2022-09-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="HIV-paper.html"/>
<link rel="next" href="global-conclusion.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<link href="libs/tabwid-1.0.0/scrool.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">From sequences to knowledge,</br> improving and learning from sequence alignments.</a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#résumé"><i class="fa fa-check"></i>Résumé</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="list-of-acronyms-and-abbreviations.html"><a href="list-of-acronyms-and-abbreviations.html"><i class="fa fa-check"></i>List of Acronyms and Abbreviations</a></li>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i>General Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#research-output"><i class="fa fa-check"></i>Research output</a>
<ul>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#journal-publications"><i class="fa fa-check"></i>Journal publications</a></li>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#presentations-and-posters"><i class="fa fa-check"></i>Presentations and posters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html"><i class="fa fa-check"></i><b>1</b> What is Sequence data ?</a>
<ul>
<li class="chapter" data-level="1.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#biological-sequences-a-primer"><i class="fa fa-check"></i><b>1.1</b> Biological sequences, a primer</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#what-is-dna"><i class="fa fa-check"></i><b>1.1.1</b> What is DNA ?</a></li>
<li class="chapter" data-level="1.1.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#from-information-to-action"><i class="fa fa-check"></i><b>1.1.2</b> From Information to action</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#obtaining-sequence-data"><i class="fa fa-check"></i><b>1.2</b> Obtaining sequence data</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#sanger-sequencing-a-breakthrough"><i class="fa fa-check"></i><b>1.2.1</b> Sanger sequencing, a breakthrough</a></li>
<li class="chapter" data-level="1.2.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#next-generation-sequencing"><i class="fa fa-check"></i><b>1.2.2</b> Next-generation sequencing</a></li>
<li class="chapter" data-level="1.2.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#long-read-sequencing"><i class="fa fa-check"></i><b>1.2.3</b> Long read sequencing</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#sequencing-errors-how-to-account-for-them"><i class="fa fa-check"></i><b>1.3</b> Sequencing errors, how to account for them ?</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#error-correction-methods"><i class="fa fa-check"></i><b>1.3.1</b> Error correction methods</a></li>
<li class="chapter" data-level="1.3.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#more-accurate-sequencing-methods"><i class="fa fa-check"></i><b>1.3.2</b> More accurate sequencing methods</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#the-special-case-of-homopolymers"><i class="fa fa-check"></i><b>1.4</b> The special case of homopolymers</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#homopolymers-and-the-human-genome"><i class="fa fa-check"></i><b>1.4.1</b> Homopolymers and the human genome</a></li>
<li class="chapter" data-level="1.4.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#homopolymers-and-long-reads"><i class="fa fa-check"></i><b>1.4.2</b> Homopolymers and long reads</a></li>
<li class="chapter" data-level="1.4.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#accounting-for-homopolymers"><i class="fa fa-check"></i><b>1.4.3</b> Accounting for homopolymers</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html"><i class="fa fa-check"></i><b>2</b> Aligning sequence data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#what-is-an-alignment"><i class="fa fa-check"></i><b>2.1</b> What is an alignment ?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#why-align"><i class="fa fa-check"></i><b>2.1.1</b> Why align ?</a></li>
<li class="chapter" data-level="2.1.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#how-to-align-two-sequences"><i class="fa fa-check"></i><b>2.1.2</b> How to align two sequences ?</a></li>
<li class="chapter" data-level="2.1.3" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#scoring-and-substitution-models"><i class="fa fa-check"></i><b>2.1.3</b> Scoring and substitution models</a></li>
<li class="chapter" data-level="2.1.4" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#dealing-with-gaps"><i class="fa fa-check"></i><b>2.1.4</b> Dealing with gaps</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#how-to-speed-up-pairwise-alignment"><i class="fa fa-check"></i><b>2.2</b> How to speed up pairwise alignment ?</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#changing-the-method"><i class="fa fa-check"></i><b>2.2.1</b> Changing the method</a></li>
<li class="chapter" data-level="2.2.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#seed-and-extend-with-data-structures"><i class="fa fa-check"></i><b>2.2.2</b> Seed and extend with data structures</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#the-specificities-of-read-mapping"><i class="fa fa-check"></i><b>2.3</b> The specificities of read-mapping</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#what-is-read-mapping"><i class="fa fa-check"></i><b>2.3.1</b> What is read-mapping ?</a></li>
<li class="chapter" data-level="2.3.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#challenges-of-read-mapping"><i class="fa fa-check"></i><b>2.3.2</b> Challenges of read-mapping</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#multiple-sequence-alignment"><i class="fa fa-check"></i><b>2.4</b> Multiple sequence alignment</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#progressive-alignment"><i class="fa fa-check"></i><b>2.4.1</b> Progressive alignment</a></li>
<li class="chapter" data-level="2.4.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#other-methods"><i class="fa fa-check"></i><b>2.4.2</b> Other methods</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#conclusion-1"><i class="fa fa-check"></i><b>2.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="HPC-paper.html"><a href="HPC-paper.html"><i class="fa fa-check"></i><b>3</b> Contribution 1: Improving read alignment by exploring a sequence transformation space</a>
<ul>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#highlights"><i class="fa fa-check"></i>Highlights</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#graphical-abstract"><i class="fa fa-check"></i>Graphical Abstract</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="3.1" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-results"><i class="fa fa-check"></i><b>3.2</b> Results</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="HPC-paper.html"><a href="HPC-paper.html#sec:msr-def"><i class="fa fa-check"></i><b>3.2.1</b> Streaming sequence reductions</a></li>
<li class="chapter" data-level="3.2.2" data-path="HPC-paper.html"><a href="HPC-paper.html#sec:enum"><i class="fa fa-check"></i><b>3.2.2</b> Restricting the space of streaming sequence reductions</a></li>
<li class="chapter" data-level="3.2.3" data-path="HPC-paper.html"><a href="HPC-paper.html#selection-of-mapping-friendly-sequence-reductions"><i class="fa fa-check"></i><b>3.2.3</b> Selection of mapping-friendly sequence reductions</a></li>
<li class="chapter" data-level="3.2.4" data-path="HPC-paper.html"><a href="HPC-paper.html#mapping-friendly-sequence-reductions-lead-to-lower-mapping-errors-on-whole-genomes"><i class="fa fa-check"></i><b>3.2.4</b> Mapping-friendly sequence reductions lead to lower mapping errors on whole genomes</a></li>
<li class="chapter" data-level="3.2.5" data-path="HPC-paper.html"><a href="HPC-paper.html#mapping-friendly-sequence-reductions-increase-mapping-quality-on-repeated-regions-of-the-human-genome"><i class="fa fa-check"></i><b>3.2.5</b> Mapping-friendly sequence reductions increase mapping quality on repeated regions of the human genome</a></li>
<li class="chapter" data-level="3.2.6" data-path="HPC-paper.html"><a href="HPC-paper.html#raw-mapping-improves-upon-hpc-on-centromeric-regions"><i class="fa fa-check"></i><b>3.2.6</b> Raw mapping improves upon HPC on centromeric regions</a></li>
<li class="chapter" data-level="3.2.7" data-path="HPC-paper.html"><a href="HPC-paper.html#positions-of-incorrectly-mapped-reads-across-the-entire-human-genome"><i class="fa fa-check"></i><b>3.2.7</b> Positions of incorrectly mapped reads across the entire human genome</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="HPC-paper.html"><a href="HPC-paper.html#discussion"><i class="fa fa-check"></i><b>3.3</b> Discussion</a></li>
<li class="chapter" data-level="3.4" data-path="HPC-paper.html"><a href="HPC-paper.html#limitations-of-this-study"><i class="fa fa-check"></i><b>3.4</b> Limitations of this study</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#author-contributions"><i class="fa fa-check"></i>Author contributions</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#declaration-of-interests"><i class="fa fa-check"></i>Declaration of interests</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#sec:methods"><i class="fa fa-check"></i>STAR Methods</a>
<ul>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#lead-contact"><i class="fa fa-check"></i>Lead contact</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#materials-availability"><i class="fa fa-check"></i>Materials availability</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#data-and-code-availability"><i class="fa fa-check"></i>Data and code availability</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#method-details"><i class="fa fa-check"></i>Method details</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#supplementary-information"><i class="fa fa-check"></i>Supplementary information</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html"><i class="fa fa-check"></i><b>4</b> Learning from sequences and alignments</a>
<ul>
<li class="chapter" data-level="4.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#why-learn-from-sequences-and-alignments"><i class="fa fa-check"></i><b>4.1</b> Why learn from sequences and alignments ?</a></li>
<li class="chapter" data-level="4.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#what-to-learn"><i class="fa fa-check"></i><b>4.2</b> What to learn ?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#supervised-learning-from-biological-sequences"><i class="fa fa-check"></i><b>4.2.1</b> Supervised learning from biological sequences</a></li>
<li class="chapter" data-level="4.2.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#unsupervised-learning-from-biological-sequences"><i class="fa fa-check"></i><b>4.2.2</b> Unsupervised learning from biological sequences</a></li>
<li class="chapter" data-level="4.2.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#others-paradigms"><i class="fa fa-check"></i><b>4.2.3</b> Others paradigms</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#how-to-learn"><i class="fa fa-check"></i><b>4.3</b> How to learn ?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#general-setting"><i class="fa fa-check"></i><b>4.3.1</b> General setting</a></li>
<li class="chapter" data-level="4.3.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#tests-and-statistical-learning"><i class="fa fa-check"></i><b>4.3.2</b> Tests and statistical learning</a></li>
<li class="chapter" data-level="4.3.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#more-complex-methods"><i class="fa fa-check"></i><b>4.3.3</b> More complex methods</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#pre-processing-the-alignment-for-machine-learning"><i class="fa fa-check"></i><b>4.4</b> Pre-processing the alignment for machine learning</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#general-purpose-encodings"><i class="fa fa-check"></i><b>4.4.1</b> General purpose encodings</a></li>
<li class="chapter" data-level="4.4.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#biological-sequence-specific-encodings"><i class="fa fa-check"></i><b>4.4.2</b> Biological sequence-specific encodings</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#conclusion-2"><i class="fa fa-check"></i><b>4.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html"><i class="fa fa-check"></i><b>5</b> Viruses, HIV and drug resistance</a>
<ul>
<li class="chapter" data-level="5.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#what-are-viruses"><i class="fa fa-check"></i><b>5.1</b> What are viruses ?</a></li>
<li class="chapter" data-level="5.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#getting-to-know-hiv"><i class="fa fa-check"></i><b>5.2</b> Getting to know HIV</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#quick-presentation-of-hiv"><i class="fa fa-check"></i><b>5.2.1</b> Quick Presentation of HIV</a></li>
<li class="chapter" data-level="5.2.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#the-replication-cycle-of-hiv"><i class="fa fa-check"></i><b>5.2.2</b> The replication cycle of HIV</a></li>
<li class="chapter" data-level="5.2.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#genetics-of-hiv"><i class="fa fa-check"></i><b>5.2.3</b> Genetics of HIV</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#drug-resistance-in-hiv"><i class="fa fa-check"></i><b>5.3</b> Drug resistance in HIV</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#a-quick-history-of-art"><i class="fa fa-check"></i><b>5.3.1</b> A quick history of ART</a></li>
<li class="chapter" data-level="5.3.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#drug-mechanisms"><i class="fa fa-check"></i><b>5.3.2</b> Main mechanisms of viral proteins, antiretroviral drugs and associated resistance.</a></li>
<li class="chapter" data-level="5.3.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#consequences-of-resistance-on-global-health"><i class="fa fa-check"></i><b>5.3.3</b> Consequences of resistance on global health</a></li>
<li class="chapter" data-level="5.3.4" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#finding-drms"><i class="fa fa-check"></i><b>5.3.4</b> Finding DRMs </a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#conclusion-3"><i class="fa fa-check"></i><b>5.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="HIV-paper.html"><a href="HIV-paper.html"><i class="fa fa-check"></i><b>6</b> Contribution 2: Inferring mutation roles from sequence alignments using machine learning</a>
<ul>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#abstract-paper"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#author-summary"><i class="fa fa-check"></i>Author summary</a></li>
<li class="chapter" data-level="6.1" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="HIV-paper.html"><a href="HIV-paper.html#materials-and-methods"><i class="fa fa-check"></i><b>6.2</b> Materials and methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="HIV-paper.html"><a href="HIV-paper.html#data"><i class="fa fa-check"></i><b>6.2.1</b> Data</a></li>
<li class="chapter" data-level="6.2.2" data-path="HIV-paper.html"><a href="HIV-paper.html#classifier-training"><i class="fa fa-check"></i><b>6.2.2</b> Classifier training</a></li>
<li class="chapter" data-level="6.2.3" data-path="HIV-paper.html"><a href="HIV-paper.html#measuring-classifier-performance"><i class="fa fa-check"></i><b>6.2.3</b> Measuring classifier performance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-results"><i class="fa fa-check"></i><b>6.3</b> Results</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="HIV-paper.html"><a href="HIV-paper.html#classifier-performance-interpretation"><i class="fa fa-check"></i><b>6.3.1</b> Classifier performance &amp; interpretation</a></li>
<li class="chapter" data-level="6.3.2" data-path="HIV-paper.html"><a href="HIV-paper.html#additional-classification-results"><i class="fa fa-check"></i><b>6.3.2</b> Additional classification results</a></li>
<li class="chapter" data-level="6.3.3" data-path="HIV-paper.html"><a href="HIV-paper.html#identifying-new-mutations-from-classifiers"><i class="fa fa-check"></i><b>6.3.3</b> Identifying new mutations from classifiers</a></li>
<li class="chapter" data-level="6.3.4" data-path="HIV-paper.html"><a href="HIV-paper.html#detailed-analysis-of-potentially-resistance-associated-mutations"><i class="fa fa-check"></i><b>6.3.4</b> Detailed analysis of potentially resistance-associated mutations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="HIV-paper.html"><a href="HIV-paper.html#discussion-and-perspectives"><i class="fa fa-check"></i><b>6.4</b> Discussion and perspectives</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#supporting-information"><i class="fa fa-check"></i>Supporting Information</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html"><i class="fa fa-check"></i><b>7</b> Learning alignments, an interesting perspective</a>
<ul>
<li class="chapter" data-level="7.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#deep-learning-and-sequences"><i class="fa fa-check"></i><b>7.1</b> Deep learning and sequences</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#intro-to-deep-learning"><i class="fa fa-check"></i><b>7.1.1</b> Intro to deep learning</a></li>
<li class="chapter" data-level="7.1.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learned-sequence-embeddings"><i class="fa fa-check"></i><b>7.1.2</b> Learned sequence embeddings</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learning-sequence-alignment"><i class="fa fa-check"></i><b>7.2</b> Learning sequence alignment</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#predicting-a-substitution-matrix"><i class="fa fa-check"></i><b>7.2.1</b> Predicting a substitution matrix</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#predicting-an-alignment"><i class="fa fa-check"></i><b>7.2.2</b> predicting an alignment</a></li>
<li class="chapter" data-level="7.2.3" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#the-attention-limitation"><i class="fa fa-check"></i><b>7.2.3</b> The attention limitation</a></li>
<li class="chapter" data-level="7.2.4" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#predicting-read-mappings"><i class="fa fa-check"></i><b>7.2.4</b> Predicting read-mappings</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#conclusion-4"><i class="fa fa-check"></i><b>7.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html"><i class="fa fa-check"></i>Global conclusion</a>
<ul>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#hpc-part"><i class="fa fa-check"></i>HPC part</a></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#hiv-part"><i class="fa fa-check"></i>HIV part</a></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#final-words"><i class="fa fa-check"></i>Final words</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="global-references.html"><a href="global-references.html"><i class="fa fa-check"></i>Global References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="HPC-appendix.html"><a href="HPC-appendix.html"><i class="fa fa-check"></i><b>A</b> Supporting Information for “Mapping-friendly sequence reductions: going beyond homopolymer compression”</a>
<ul>
<li class="chapter" data-level="A.1" data-path="HPC-appendix.html"><a href="HPC-appendix.html#appendix:tandemtools"><i class="fa fa-check"></i><b>A.1</b> “TandemTools” dataset generation</a></li>
<li class="chapter" data-level="A.2" data-path="HPC-appendix.html"><a href="HPC-appendix.html#msr-performance-comparison"><i class="fa fa-check"></i><b>A.2</b> MSR performance comparison</a></li>
<li class="chapter" data-level="A.3" data-path="HPC-appendix.html"><a href="HPC-appendix.html#analyzing-read-origin-on-whole-human-genome"><i class="fa fa-check"></i><b>A.3</b> Analyzing read origin on whole human genome</a></li>
<li class="chapter" data-level="A.4" data-path="HPC-appendix.html"><a href="HPC-appendix.html#performance-of-msrs-on-the-drosophila-genome"><i class="fa fa-check"></i><b>A.4</b> Performance of MSRs on the Drosophila genome</a></li>
<li class="chapter" data-level="A.5" data-path="HPC-appendix.html"><a href="HPC-appendix.html#key-resource-table"><i class="fa fa-check"></i><b>A.5</b> Key Resource Table</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html"><i class="fa fa-check"></i><b>B</b> Supporting Information for “HIV and DRMs”</a>
<ul>
<li class="chapter" data-level="B.1" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html#detailed-list-of-hiv-1-protein-structures-used-for-figure-generation."><i class="fa fa-check"></i><b>B.1</b> Detailed list of HIV-1 protein structures used for figure generation.</a></li>
<li class="chapter" data-level="B.2" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html#list-of-all-antiretroviral-drugs"><i class="fa fa-check"></i><b>B.2</b> List of all antiretroviral drugs</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="HIV-appendix.html"><a href="HIV-appendix.html"><i class="fa fa-check"></i><b>C</b> Supporting Information for “Using Machine Learning and Big Data to Explore the Drug Resistance Landscape in HIV”</a>
<ul>
<li class="chapter" data-level="C.1" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S1-Appendix"><i class="fa fa-check"></i><b>C.1</b> S1 Appendix (Technical appendix).</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="HIV-appendix.html"><a href="HIV-appendix.html#data-appendix"><i class="fa fa-check"></i><b>C.1.1</b> Data</a></li>
<li class="chapter" data-level="C.1.2" data-path="HIV-appendix.html"><a href="HIV-appendix.html#classifiers"><i class="fa fa-check"></i><b>C.1.2</b> Classifiers</a></li>
<li class="chapter" data-level="C.1.3" data-path="HIV-appendix.html"><a href="HIV-appendix.html#scoring"><i class="fa fa-check"></i><b>C.1.3</b> Scoring</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s1-fig."><i class="fa fa-check"></i><b>C.2</b> S1 Fig.</a></li>
<li class="chapter" data-level="C.3" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s2-fig."><i class="fa fa-check"></i><b>C.3</b> S2 Fig.</a></li>
<li class="chapter" data-level="C.4" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s3-fig."><i class="fa fa-check"></i><b>C.4</b> S3 Fig.</a></li>
<li class="chapter" data-level="C.5" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S1-Table"><i class="fa fa-check"></i><b>C.5</b> S1 Table.</a></li>
<li class="chapter" data-level="C.6" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S2-Appendix"><i class="fa fa-check"></i><b>C.6</b> S2 Appendix. (Fisher exact tests)</a></li>
<li class="chapter" data-level="C.7" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s1-data."><i class="fa fa-check"></i><b>C.7</b> S1 Data.</a></li>
<li class="chapter" data-level="C.8" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s2-data."><i class="fa fa-check"></i><b>C.8</b> S2 Data.</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://lucblassel.com" target="blank">Back to main website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">From sequences to knowledge, improving and learning from sequence alignments</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="learning-alignments-an-interesting-perspective" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Learning alignments, an interesting perspective<a href="learning-alignments-an-interesting-perspective.html#learning-alignments-an-interesting-perspective" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Recently, machine learning methods have been increasingly applied to the process of alignment. Learning an alignment method through machine learning could result in methods with less design biases and with data-driven insights.</p>
<div id="deep-learning-and-sequences" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Deep learning and sequences<a href="learning-alignments-an-interesting-perspective.html#deep-learning-and-sequences" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As many of these techniques are based on deep learning, which I did not introduce in Chapter <a href="learning-from-sequences-and-alignments.html#learning-from-sequences-and-alignments">4</a>, I will first introduce deep learning very shortly. I will then introduce the concept of learned sequence embeddings which have become very useful for machine sequence alignment.</p>
<div id="intro-to-deep-learning" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Intro to deep learning<a href="learning-alignments-an-interesting-perspective.html#intro-to-deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Deep learning is the process of learning using neural networks. Neural all started in 1958 when Rosenblatt proposed the <em>perceptron</em><span class="citation"><sup><a href="#ref-rosenblattPerceptronProbabilisticModel1958" role="doc-biblioref">671</a></sup></span>. This learning algorithm was loosely inspired by biological neurons, which led to the name: <em>neural networks.</em> The perceptron takes as input <span class="math inline">\(n\)</span> values, these are used in a weighted sum that is then fed through an <em>activation function.</em> The output of this function is the output of the perceptron. Originally, to replicate biological neurons, the activation function was a step function where, the perceptron has an output only if the weighted sum crosses a given threshold. This structure is often represented through a computational graph like in Figure <a href="learning-alignments-an-interesting-perspective.html#fig:perceptron">7.1</a>. By tweaking the weights of the inputs, the perceptron can be used to solve linear separation problems.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:perceptron"></span>
<img src="figures/Learn-alignments/perceptron.png" alt="**Computational graph of a perceptron.**  
Here, $n$ inputs are passed into the perceptron where they are summed, weighted by $w_1,\ldots,w_n$. This sum is then fed through the perceptron's activation function (here a binary step function) which is the output of the perceptron. Often the sum is implicitely considered part of the activation function." width="60%" />
<p class="caption">
Figure 7.1: <strong>Computational graph of a perceptron.</strong><br />
Here, <span class="math inline">\(n\)</span> inputs are passed into the perceptron where they are summed, weighted by <span class="math inline">\(w_1,\ldots,w_n\)</span>. This sum is then fed through the perceptron’s activation function (here a binary step function) which is the output of the perceptron. Often the sum is implicitely considered part of the activation function.
</p>
</div>
<p>While the perceptron could be useful, some other methods could solve more complex problems. However it was discovered that by linking several perceptrons together, as in a biological brain, some complex problems could also be solved. These structure, called <em>multilayer perceptrons</em> (MLP), are organized in layers, where the outputs of perceptrons on a layer are used as inputs by perceptrons is the next layer (c.f. Figure <a href="learning-alignments-an-interesting-perspective.html#fig:mlp">7.2</a>). The perceptrons, when in this form, are often called <em>neurons</em>, and the MLP a <em>neural network</em> (NN). These neural networks are organized in layers, with an input and output layer on either end, and hidden layers in the middle. With the large number of weights to tune, these models were very difficult to train and therefore not practically useful.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlp"></span>
<img src="figures/Learn-alignments/mlp.png" alt="**Computational graph of a multilayer perceptron.**  
This MLP, also called feedforward neural network, has $n$ inputs represented as the input layer, 2 hidden layers of 3 neurons each and an output layer of 2 neurons (e.g. suitable to binary classification). It is fully connected meaning that each node of a given layer is used as input for every neuron of the following layer. Each edge in this graph is paired to a weight, these are the tunable parameters during the training process." width="60%" />
<p class="caption">
Figure 7.2: <strong>Computational graph of a multilayer perceptron.</strong><br />
This MLP, also called feedforward neural network, has <span class="math inline">\(n\)</span> inputs represented as the input layer, 2 hidden layers of 3 neurons each and an output layer of 2 neurons (e.g. suitable to binary classification). It is fully connected meaning that each node of a given layer is used as input for every neuron of the following layer. Each edge in this graph is paired to a weight, these are the tunable parameters during the training process.
</p>
</div>
<p>There was a great resurgence of these models in the nineties due to the invention of <em>backpropagation</em><span class="citation"><sup><a href="#ref-rumelhartLearningRepresentationsBackpropagating1986" role="doc-biblioref">672</a></sup></span>. By replacing the step functions of neurons with continuous, differentiable activation functions like sigmoid or hyperbolic tangent functions. With backpropagation, a gradient of the output could be computed w.r.t to each weight, enabling gradient descent procedures for automatically learning the optimal weights from data as described in Section <a href="#supervised-learning"><strong>??</strong></a>. With this, method, neural networks could be efficiently trained on complex classification and regression problems<span class="citation"><sup><a href="#ref-murtaghMultilayerPerceptronsClassification1991" role="doc-biblioref">673</a></sup></span>. It was also proven that with hidden layers, neural networks are universal function approximators<span class="citation"><sup><a href="#ref-cybenkoApproximationSuperpositionsSigmoidal1989" role="doc-biblioref">674</a>–<a href="#ref-hornikApproximationCapabilitiesMultilayer1991a" role="doc-biblioref">676</a></sup></span>, suitable for all types of tasks. One notable caveat for neural networks is that, due to the large amount of weights to tune, they require large amounts of training data, which also explained their low usage before the internet and resulting data collection.</p>
<p>In the following years, neural networks saw an explosion in usage, with more complex architectures like convolutional neural networks (CNN) achieving state of the art result in computer vision tasks<span class="citation"><sup><a href="#ref-lecunBackpropagationAppliedHandwritten1989" role="doc-biblioref">677</a>,<a href="#ref-lecunGradientbasedLearningApplied1998" role="doc-biblioref">678</a></sup></span>. By representing an input variable as a linear combination of its neighbors some form of contextual information can be passed to the NN and improve performance. CNNs can also have good results in non computer-vision tasks like: drug resistance prediction<span class="citation"><sup><a href="#ref-steinerDrugResistancePrediction2020a" role="doc-biblioref">329</a></sup></span>, protein subcellular localization<span class="citation"><sup><a href="#ref-weiPredictionHumanProtein2018" role="doc-biblioref">348</a></sup></span>, or epidemiological model parameter estimation<span class="citation"><sup><a href="#ref-voznicaDeepLearningPhylogenies2022" role="doc-biblioref">679</a></sup></span>.</p>
<p>More recently, as computational power and the amount of training data grew, larger and deeper (i.e. more hidden layers) architecture were able to be trained and achieved state of the art performance in many fields: image recognition with deep CNNs like <code>Alexnet</code><span class="citation"><sup><a href="#ref-krizhevskyImageNetClassificationDeep2017" role="doc-biblioref">680</a></sup></span> or <code>Resnet</code><span class="citation"><sup><a href="#ref-heDeepResidualLearning2016" role="doc-biblioref">681</a></sup></span>, translation with Recurrent NNs<span class="citation"><sup><a href="#ref-bahdanauNeuralMachineTranslation2016" role="doc-biblioref">682</a></sup></span> and Transformers<span class="citation"><sup><a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">683</a></sup></span> (more on that in Section <a href="learning-alignments-an-interesting-perspective.html#learned-sequence-embeddings">7.1.2</a>) or protein structure prediction with <code>Alphafold2</code><span class="citation"><sup><a href="#ref-jumperHighlyAccurateProtein2021" role="doc-biblioref">136</a></sup></span></p>
</div>
<div id="learned-sequence-embeddings" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Learned sequence embeddings<a href="learning-alignments-an-interesting-perspective.html#learned-sequence-embeddings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An area that in which deep learning has recently proved particularly useful is the creation relevant learned embeddings. These embeddings, similarly to the encodings discussed in Section <a href="#preprocessing-the-alignment-for-machine-learning"><strong>??</strong></a>, transform a sequence of tokens in a numerical vector which can then be used in other machine learning tasks. By learning these embeddings, the hope is that the resulting vector will retain the most important information in the sequence and keep some context.</p>
<div id="x-2vec" class="section level4 hasAnchor" number="7.1.2.1">
<h4><span class="header-section-number">7.1.2.1</span> <span class="math inline">\(x\)</span>-<code>2vec</code><a href="learning-alignments-an-interesting-perspective.html#x-2vec" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Learned embeddings were principally developed in the field of natural language processing (NLP), where machine learning algorithms use text in languages such as English or French as input. In these contexts, simple encodings like OHE are not very practical because of the very high dimensionality of a language. For example, Merriam-Webster English dictionary contains 470,000 words<span class="citation"><sup><a href="#ref-HowManyWords" role="doc-biblioref">684</a></sup></span> so to encode a single word as a One-Hot encoded vector would result in a 470,000-dimensional sparse vector. Encoding a whole text or even a single sentence is wildly unpractical. Therefore, as a field, NLP needed to come up with ways of efficiently representing words in lower-dimensional vectors than naive encoding methods, while retaining semantic meaning.</p>
<p>One of the early methods for sequence embedding was called <code>word2vec</code><span class="citation"><sup><a href="#ref-mikolovEfficientEstimationWord2013" role="doc-biblioref">685</a>,<a href="#ref-mikolovDistributedRepresentationsWords2013" role="doc-biblioref">686</a></sup></span>, proposed by researchers at Google, that learns an word-embedding method on a particular text corpus. This method is designed to make embeddings that contain semantically relevant information, and example of the article is that the vector corresponding to <span class="math inline">\(vec(Madrid) - vec(Spain)\)</span> should be very similar to the vector <span class="math inline">\(vec(Paris)\)</span>, and that similar words result in similar vectors.</p>
<p>The way this method works is by considering a word within its context, i.e. a window of length <span class="math inline">\(k\)</span> centered around the word. In a corpus of words (i.e. our training data), each word is encoded as a One Hot Vector, which is possible since the corpus contains only a subset of the words in the English language. A neural is then trained on one one of two tasks<span class="citation"><sup><a href="#ref-goldbergWord2vecExplainedDeriving2014" role="doc-biblioref">687</a></sup></span>:</p>
<ul>
<li><p>Continuous Bag of words: where the word is predicted given the context of the word as input</p></li>
<li><p>Skip-gram: where the context is predicted given the encoded word vector</p></li>
</ul>
<p>After having sufficiently trained the neural network on the corpus on one of these tasks, one of the hidden layers of the network can be extracted and used as a vector representation of the input word, this results in an embedding method that is specific to a given corpus and the embedded vectors can be used in downstream learning tasks.</p>
<p><code>word2vec</code> was very successful and widely used in the field of NLP, it is perhaps no surprise that the ideas behind it were adapted and reused in the field of bioinformatics. <code>dna2vec</code><span class="citation"><sup><a href="#ref-ngDna2vecConsistentVector2017" role="doc-biblioref">688</a></sup></span> uses similar ideas and was used to embed <span class="math inline">\(k\)</span>-mers, and predict methylation sites on DNA sequences<span class="citation"><sup><a href="#ref-liangHyb4mCHybridDNA2vecbased2022" role="doc-biblioref">689</a></sup></span>. Similar embedding methods like <code>seq2vec</code><span class="citation"><sup><a href="#ref-kimothiDistributedRepresentationsBiological2016" role="doc-biblioref">690</a></sup></span> as well as <code>bioVec</code> (including the protein specific <code>protVec</code>)<span class="citation"><sup><a href="#ref-asgariContinuousDistributedRepresentation2015" role="doc-biblioref">691</a></sup></span> were also developed to embed whole biological sequences. They were successfully used in biological sequence classification problems<span class="citation"><sup><a href="#ref-kimothiMetricLearningBiological2017" role="doc-biblioref">692</a></sup></span>.</p>
</div>
<div id="the-attention-revolution" class="section level4 hasAnchor" number="7.1.2.2">
<h4><span class="header-section-number">7.1.2.2</span> The attention revolution<a href="learning-alignments-an-interesting-perspective.html#the-attention-revolution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>While <code>word2vec</code> was widely used for many NLP tasks where word embeddings were needed, a lot of interesting developments on word embeddings were made in the field of automated machine translation. In this application, the desired embedding characteristics are slightly different. While semantic relevance is useful, in machine translation the embedding method needs to be able to capture dependencies, e.g. within a sentence where the link between the subject and the verb must be captured even though they are not necessarily next to each other. This was initially done by using recurrent neural networks, called RNNs or LSTMs<span class="citation"><sup><a href="#ref-beplerLearningProteinSequence2019" role="doc-biblioref">693</a></sup></span>, but they were hard to train and had trouble properly capturing long-range dependencies<span class="citation"><sup><a href="#ref-songPretrainingModelBiological2021" role="doc-biblioref">694</a></sup></span>.</p>
<p>One of the most successful methods developed for this task it the transformer<span class="citation"><sup><a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">683</a></sup></span>, also created by Google researchers. The main mechanisms of the transformer is the <em>self-attention</em> mechanisms: each input token, usually encoded as a One-Hot vector, is represented as a weighted sum of all the other tokens in a sequence <em>(here a token is a word and the sequence is a sentence)</em>. The weights of this sum are trained along with the rest of this network. by stacking several of these self-attention blocks, transformers can learn to represent and leverage long-range dependencies. This mechanism, and the transformer in general have had very successful application in machine translation, while being easier to train than recurrent networks<span class="citation"><sup><a href="#ref-wangProgressMachineTranslation2021" role="doc-biblioref">695</a></sup></span>.</p>
<p>This architecture was used to create very large pre-trained language models, that is to say models that perform word embedding. These models like <code>GPT-3</code><span class="citation"><sup><a href="#ref-brownLanguageModelsAre2020" role="doc-biblioref">696</a></sup></span> or <code>BERT</code><span class="citation"><sup><a href="#ref-devlinBERTPretrainingDeep2019" role="doc-biblioref">697</a></sup></span> are huge, with millions or even billions of learned weights, and have been trained on huge quantities of data in order to produce word embeddings useful in a wide range of contexts. <code>BERT</code> was trained using Masked Language Modelling (MLM), where some percentage of the tokens <em>(words)</em> in an input sequence <em>(sentence)</em> are replaced by a special <code>[MASK]</code> token, and the model is trained to predict the whole sentence, effectively guessing what tokens are missing based on the context of the whole sequence. This process allows the model to learn relevant dependencies between tokens in the training data.</p>
<p>As was the case with <code>word2vec</code>, these methods have been adapted to bioinformatics tasks with state of the art results, proving the versatility of the transformer model. Several <em>protein language models</em> similar to <code>BERT</code> were trained on various training sets of protein data. Some of these are <code>ProGen</code><span class="citation"><sup><a href="#ref-madaniProGenLanguageModeling2020" role="doc-biblioref">698</a></sup></span>, <code>ProGen2</code><span class="citation"><sup><a href="#ref-eriknijkampProGen2ExploringBoundaries2022" role="doc-biblioref">699</a></sup></span> and <code>ProtBERT</code><span class="citation"><sup><a href="#ref-elnaggarProtTransCrackingLanguage2021" role="doc-biblioref">369</a></sup></span>. These large protein language models have been studied and interesting properties have been observed<span class="citation"><sup><a href="#ref-beplerLearningProteinLanguage2021" role="doc-biblioref">700</a></sup></span>, and some specific characteristics of proteins can be inferred from these models without specifying them in the training step. For example, protein language models seem to learn some information about the protein structure and attention maps can be used to infer residue contact maps<span class="citation"><sup><a href="#ref-raoTransformerProteinLanguage2020" role="doc-biblioref">701</a>–<a href="#ref-bhattacharyaSingleLayersAttention2020" role="doc-biblioref">703</a></sup></span>. Similarly these models capture some information about protein function<span class="citation"><sup><a href="#ref-huExploringEvolutionbasedFree2022" role="doc-biblioref">704</a></sup></span>, mutational effects<span class="citation"><sup><a href="#ref-meierLanguageModelsEnable2021" role="doc-biblioref">705</a></sup></span>, evolutionary characteristics<span class="citation"><sup><a href="#ref-hieEvolutionaryVelocityProtein2022" role="doc-biblioref">706</a></sup></span> and can even be used to generate new protein with desired properties<span class="citation"><sup><a href="#ref-madaniProGenLanguageModeling2020" role="doc-biblioref">698</a></sup></span>. Some large language models have also been trained on DNA sequences like <code>DNABert</code><span class="citation"><sup><a href="#ref-jiDNABERTPretrainedBidirectional2021" role="doc-biblioref">707</a></sup></span> and also seem to capture some information without explicit specification during training, like variant effects<span class="citation"><sup><a href="#ref-benegasDNALanguageModels2022" role="doc-biblioref">708</a></sup></span>.</p>
<p>While, these protein language models have shown very useful for embedding sequences, some developments have been made to embed multiple sequence alignments as learning inputs. In some cases this is done by including information on the alignment in the tokens and then using a regular language model to embed them<span class="citation"><sup><a href="#ref-caiGenomewidePredictionSmall2020" role="doc-biblioref">709</a></sup></span>. In the case of the MSA transformer<span class="citation"><sup><a href="#ref-raoMSATransformer2021" role="doc-biblioref">710</a></sup></span>, the attention mechanism was extended to include a weighted sum between aligned sequences effectively taking the alignment into account when embedding sequences. An attention-like mechanism was also used to train a protein structural model directly on MSAs<span class="citation"><sup><a href="#ref-sercuNeuralPottsModel2021" role="doc-biblioref">711</a></sup></span>. Similarly, by pre-training language models on profiles derived from MSAs, some information about the alignment can also be included in the resulting embeddings<span class="citation"><sup><a href="#ref-sturmfelsProfilePredictionAlignmentBased2020" role="doc-biblioref">712</a></sup></span>. Finally aligned sequences can be used as inputs in a regular transformer as was done <code>DeepConsensus</code><span class="citation"><sup><a href="#ref-baidDeepConsensusImprovesAccuracy2022" role="doc-biblioref">713</a></sup></span>, a transformer-based polisher to improve PacBio HiFi reads even further. Finally the <em>EvoFormer</em> model included in <code>AlphaFold2</code><span class="citation"><sup><a href="#ref-jumperHighlyAccurateProtein2021" role="doc-biblioref">136</a></sup></span>, which embeds MSAs to predict protein structure, is partly responsible for the leap in performance between the two generations of the <code>AlphaFold</code> model.</p>
<p>It is important to note that while these transformer models are very powerful and useful in practice, their complexity and size makes it very hard to study and understand what the model actually learns. There is work to peer inside this “black box”, notably by interpreting the learn attention maps<span class="citation"><sup><a href="#ref-vigBERTologyMeetsBiology2021" role="doc-biblioref">714</a></sup></span> to decipher biologically relevant information contained within.</p>
</div>
</div>
</div>
<div id="learning-sequence-alignment" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Learning sequence alignment<a href="learning-alignments-an-interesting-perspective.html#learning-sequence-alignment" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>With the success of deep learning methods learning informative and effective embeddings from sequences, it is maybe natural to try and see if similar methods can learn to align sequences to each other.</p>
<div id="predicting-a-substitution-matrix" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Predicting a substitution matrix<a href="learning-alignments-an-interesting-perspective.html#predicting-a-substitution-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One approach is to learn a <em>position-specific scoring matrix</em> (PSSM), which assigns an alignment cost not between two amino-acids but between two specific residues of the sequences (i.e. an amino acid/position pair). Therefore, when aligning a sequence of length <span class="math inline">\(m\)</span> and another of length <span class="math inline">\(n\)</span>, we can use a standard alignment method such as NW or SW with an <span class="math inline">\(m\times n\)</span> PSSM.</p>
<p>One approach, used in the <code>SAdLSA</code> model<span class="citation"><sup><a href="#ref-gaoNovelSequenceAlignment2021" role="doc-biblioref">715</a></sup></span> used CNNs to refine a PSSM. The model is trained on experimentally validated structural alignments. A starting PSSM is created from both sequences with PSI-BLAST<span class="citation"><sup><a href="#ref-altschulGappedBLASTPSIBLAST1997" role="doc-biblioref">199</a></sup></span>, and fed through a deep CNN, which outputs a refined PSSM. This learned matrix is used with a SW algorithm to locally align the two sequences. This alignment is then compared to the structural alignment to compute the loss and train the model.</p>
<p>Some methods rely on protein language model embeddings coupled with differentiable alignment algorithms to learn a PSSM in an end-to-end fashion. <code>DeepBLAST</code> is one such model<span class="citation"><sup><a href="#ref-mortonProteinStructuralAlignments2020" role="doc-biblioref">716</a></sup></span>, it was trained on 1.5 million structural alignments obtained from the PDB database<span class="citation"><sup><a href="#ref-bermanWorldwideProteinData2007" role="doc-biblioref">717</a></sup></span>. The sequences are embedded using a pre-trained LSTM-based protein language model, not transformer-based. These embeddings are fed through LSTM networks to predict a match scoring and gap scoring PSSMs. These matrices are then used in a differentiable variant of the NW algorithm, that can be used to backpropagate the alignment error through the network and learn relevant parameters. RNNs and LSTMs were also used to predict PSSMs by Guo <em>et al.</em> albeit with the goal of protein structure prediction rather than alignment<span class="citation"><sup><a href="#ref-guoComprehensiveStudyEnhancing2021" role="doc-biblioref">718</a></sup></span>.</p>
<p>The <code>DEDAL</code> model<span class="citation"><sup><a href="#ref-llinares-lopezDeepEmbeddingAlignment2022" role="doc-biblioref">719</a></sup></span> implements similar ideas. It predicts matching, gap-open and gap-extend PSSMs form a pair of sequences, that can then be used in a classical alignment method, in this case a SW algorithm. In this model, and transformer-based embedding network, is used to embed each residue of both sequence. Then each possible pair of embedded residues from both sequences are used to predict specific gap-open, gap-extend and match scores used to build the PSSMs. The DEDAL model is trained on three tasks at once:</p>
<ol style="list-style-type: decimal">
<li>Masked language modelling (c.f. Section <a href="learning-alignments-an-interesting-perspective.html#the-attention-revolution">7.1.2.2</a>) to train the transformer-based embedding model on 30 million sequences from the UniRef50 database<span class="citation"><sup><a href="#ref-suzekUniRefClustersComprehensive2015" role="doc-biblioref">720</a></sup></span>.</li>
<li>A homology detection task where the whole model is trained to predict if a pair of sequences are evolutionarily related or not. This was done on pairs of sequences extracted from the 1.2 million sequences of the Pfam-A seed database<span class="citation"><sup><a href="#ref-mistryPfamProteinFamilies2021" role="doc-biblioref">721</a></sup></span>.</li>
<li>An alignment task, where the whole model is trained to align two sequences using the author’s differentiable variant of the SW algorithm to backpropagate the alignment error through the network and tune the parameters. This training task was also done using aligned sequence pairs from the Pfam-A seed database.</li>
</ol>
<p>Trained on the three tasks at once, the <code>DEDAL</code> model predicts PSSMs leading to good alignments overall, however where it really shines and outperforms other methods is on alignments of remote homologs. Classical alignment algorithms can struggle when the similarity between two sequences dips below a certain threshold, however <code>DEDAL</code> is able to pick up on this remote homology and produce a sensible and accurate alignment.</p>
<p>The Learn Alignment module<span class="citation"><sup><a href="#ref-pettiEndtoendLearningMultiple2022" role="doc-biblioref">722</a></sup></span> also uses a differentiable variant of the SW algorithm to learn a scoring matrix. Sequences are encoded as OHE vectors and fed embedded with simple convolutions, to predict a “context-specific” scoring matrix. This module is used to build MSAs where, similarly to the center star alignment, all <em>target</em> sequences are aligned to a single <em>query</em> sequence. This model was validated by including it in the <code>Alphafold2</code> model and seeing the improvement in performance for certain protein structure prediction tasks.</p>
</div>
<div id="predicting-an-alignment" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> predicting an alignment<a href="learning-alignments-an-interesting-perspective.html#predicting-an-alignment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Predicting a PSSM is one way of learning an alignment, however an alignment algorithm still need to be used in order to obtain aligned sequences. However, it might be possible to directly output an alignment between input sequences. As stated above, transformers have been particularly useful in automated translation, and one could construe the alignment problem as translating from an unaligned sequence “language” to an aligned sequence “language”. This is exactly the idea behind <code>BetaAlign</code>, a recently developed transformer model used for pairwise and multiple sequence alignment<span class="citation"><sup><a href="#ref-dotanHarnessingMachineTranslation2022" role="doc-biblioref">723</a></sup></span>. For example, the two sequences <code>AAG</code> and <code>ACGG</code> can be represented as a single “sentence”: <code>AAG|ACGG</code> with the <code>|</code> special token denoting a separation between sequences. Aligned sequences output by the transformer can then be represented as a succession of aligned pairs: <code>AAAC-GGG</code> corresponding to the following alignment:</p>
<center>
<div class="line-block"><strong><code>AA-G</code></strong><br />
<strong><code>ACGG</code></strong></div>
</center>
<p>The authors trained this model on millions of simulated alignments, of two to ten sequences, generated with different underlying evolutionary models, in the same fashion that regular transformers are trained for machine translation. The authors trained models for protein and DNA sequence alignment on these simulated datasets, containing sequences around 40 residues long. According to some measures, <code>BetaAlign</code> outperforms widely used multiple sequence aligners such as MUSCLE, CLUSTALW or T-Coffee, especially on nucleotide sequence alignment. This model was also trained to deal with longer sequences, generating MSAs of 5 sequences between 500 and 1000 residues long. In this setting <code>BetaAlign</code> performs on par with most widely used aligners.</p>
<p>While <code>BetaAlign</code> is an interesting step in the direction of learned alignment methods, and a good proof of concept, it seems to be efficient only on short sequences and a low number of sequences. This is mostly due to the attention mechanism at the heart of transformers.</p>
</div>
<div id="the-attention-limitation" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> The attention limitation<a href="learning-alignments-an-interesting-perspective.html#the-attention-limitation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While the transformer architecture has revolutionized the field of machine translation, and proved to be useful in sequence-related bioinformatics tasks, the attention mechanism at the heart of it presents some problems. The main problem is that by including a weighted sum of all input tokens in the embedding of a specific one, the time and space complexity of the attention mechanism is quadratic in sequence length. This is particularly problematic in biological tasks where DNA and protein sequences can be much longer than a typical sentence, in any spoken language. This limitation is mentioned in the article for both the <code>DEDAL</code> and <code>BetaAlign</code> models described above.</p>
<p>This problem is not inherent to biology and many different approaches have been proposed in other fields where transformer-usage is prevalent. The <em>Linformer</em><span class="citation"><sup><a href="#ref-wangLinformerSelfAttentionLinear2020" role="doc-biblioref">724</a></sup></span> and <em>Nyströmformer</em><span class="citation"><sup><a href="#ref-xiongNystromformerNystrombasedAlgorithm2021" role="doc-biblioref">725</a></sup></span> architectures both present different approximations of the attention mechanism that scale linearly w.r.t. to sequence length both in time and memory. Others have tried to make the attention process produce sparse matrices, reducing the memory requirements<span class="citation"><sup><a href="#ref-childGeneratingLongSequences2019" role="doc-biblioref">726</a>,<a href="#ref-correiaAdaptivelySparseTransformers2019" role="doc-biblioref">727</a></sup></span>. Others have tried adjusting the attention span, i.e. the number of tokens taken into account in the attention mechanism, with an adaptive attention span<span class="citation"><sup><a href="#ref-sukhbaatarAdaptiveAttentionSpan2019" role="doc-biblioref">728</a></sup></span> or long-short range attention<span class="citation"><sup><a href="#ref-wuLiteTransformerLongShort2020" role="doc-biblioref">729</a></sup></span>. Finally some change the operations in the attention mechanism, the <em>Reformer</em> model reduces the memory requirements to a linear complexity by replacing a dot product operation<span class="citation"><sup><a href="#ref-kitaevReformerEfficientTransformer2020" role="doc-biblioref">730</a></sup></span>.</p>
<p>Some improvements to the attention mechanism have also been tried in a biological context. Choromanski <em>et al.</em> propose the <em>Performer</em> model that uses a fast attention mechanism<span class="citation"><sup><a href="#ref-choromanskiMaskedLanguageModeling2020" role="doc-biblioref">731</a></sup></span>, based on orthogonal random features and trained on an protein MLM task. With this approach, the attention mechanism scales linearly with the sequence length rather than quadratically. Another team used factored attention in their model trained on predicting protein structure prediction<span class="citation"><sup><a href="#ref-bhattacharyaInterpretingPottsTransformer2021" role="doc-biblioref">732</a></sup></span>. They show that with this mechanisms, fewer parameters need to be tuned, lowering the memory and time requirements, while retaining structurally relevant information.</p>
</div>
<div id="predicting-read-mappings" class="section level3 hasAnchor" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Predicting read-mappings<a href="learning-alignments-an-interesting-perspective.html#predicting-read-mappings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the situation of read-mapping, the methods described above are of limited use. This is due to some intrinsic characteristics of read-mapping: mainly the size discrepancy between reads and the reference sequence, as well as the length of the reference sequence. Some work has been done however on including machine learning methods into the read-mapping process.</p>
<p>One first approach, is to learn the data structures, called <em>learned indices</em>, used to store potential seeds in the seed and extend framework. These learned indices are trained to replicate the output of a particular data structure. This approach was first proposed in 2018<span class="citation"><sup><a href="#ref-kraskaCaseLearnedIndex2018" role="doc-biblioref">733</a></sup></span>, although it was not implemented then. The <code>BWA-MEME</code><span class="citation"><sup><a href="#ref-jungBWAMEMEBWAMEMEmulated2022" role="doc-biblioref">734</a></sup></span> read-mapper uses a learned index that predicts positions in a suffix array. This approach is also the one used by the <code>Sapling</code> algorithm<span class="citation"><sup><a href="#ref-kirscheSaplingAcceleratingSuffix2021" role="doc-biblioref">735</a></sup></span>. Learned indices have also been used to predict a position in an FM-index<span class="citation"><sup><a href="#ref-hoLISALearnedIndexes2021" role="doc-biblioref">736</a></sup></span>. These learned indices lower the memory cost and execution time costs by not having to build the whole data structure and not having to store it in memory or on disk. Furthermore it is well adapted to read-mapping since it only needs to be trained once on a specific reference sequence that can be used anytime reads need to be mapped to this reference.</p>
<p>Another approach where machine learning has proven useful is in learning a seed selection scheme. <code>DeepMinimizer</code><span class="citation"><sup><a href="#ref-hoangDifferentiableLearningSequenceSpecific2022" role="doc-biblioref">737</a></sup></span> is one such method, where neural networks are trained to select appropriate minimizers from a DNA sequence. This approach results in minimizers with optimal density, that is to say they are spread out evenly over the whole sequence lowering the memory and time costs of building a seed index. Similarly, although not a direct application of read mapping, deep learning has been used to predict candidate alignment sites in mRNA-miRNA pairs, a similar task to seed selection<span class="citation"><sup><a href="#ref-minTargetNetFunctionalMicroRNA2022" role="doc-biblioref">738</a></sup></span>.</p>
<p>Finally, the pre-processing function framework of MSRs presented in Chapter <a href="HPC-paper.html#HPC-paper">3</a> can also be extended with machine learning methods. Learning connections in the graphs representing MSRs could allow the exploration of the large function spaces of higher-order MSRs. Alternatively some sequence-to-sequence models like transformers could also be used to learn a pre-processing function. To learn an appropriate pre-processing function in an end-to-end fashion, a differentiable read-mapping algorithm is needed. Differentiable versions of the NW and SW could be used in read-mappers, but differentiable seeding and seed-selection processes are also needed.</p>
</div>
</div>
<div id="conclusion-4" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Conclusion<a href="learning-alignments-an-interesting-perspective.html#conclusion-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Deep learning is a powerful framework in sequence-based tasks. The recent transformer architecture has shown an unprecedented ability to capture within-sequence dependencies and learn relevant information. This ability has made them dominant in the NLP field, particularly machine translation. Transformers and large language models have also shown some power in biological sequence processing and sequence alignment. However, the attention mechanism that makes these models so successful has limitations, especially w.r.t. to input sequence length. Some approaches and approximations, have allowed researchers to lower the time and memory complexity of the attention mechanism and make it more manageable, but these improvements have yet to be used in a sequence alignment task. In the special case of read-mapping, even with improved attention mechanisms, the size discrepancy between reference and reads, as well as the often very large scale fo the reference sequence, make transformer based embedding approaches impractical. Learned data structures and seeding schemes might be one of the approaches to improve read alignment.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" line-spacing="2">
<div id="ref-jumperHighlyAccurateProtein2021" class="csl-entry">
<div class="csl-left-margin">136. </div><div class="csl-right-inline">Jumper, J. <em>et al.</em> <a href="https://doi.org/10.1038/s41586-021-03819-2">Highly accurate protein structure prediction with AlphaFold</a>. <em>Nature</em> <strong>596</strong>, 583–589 (2021).</div>
</div>
<div id="ref-altschulGappedBLASTPSIBLAST1997" class="csl-entry">
<div class="csl-left-margin">199. </div><div class="csl-right-inline">Altschul, S. F. <em>et al.</em> <a href="https://doi.org/10.1093/nar/25.17.3389">Gapped BLAST and PSI-BLAST: A new generation of protein database search programs</a>. <em>Nucleic Acids Research</em> <strong>25</strong>, 3389–3402 (1997).</div>
</div>
<div id="ref-steinerDrugResistancePrediction2020a" class="csl-entry">
<div class="csl-left-margin">329. </div><div class="csl-right-inline">Steiner, M. C., Gibson, K. M. &amp; Crandall, K. A. <a href="https://doi.org/10.3390/v12050560">Drug <span>Resistance Prediction Using Deep Learning Techniques</span> on <span>HIV</span>-1 <span>Sequence Data</span></a>. <em>Viruses</em> <strong>12</strong>, 560 (2020).</div>
</div>
<div id="ref-weiPredictionHumanProtein2018" class="csl-entry">
<div class="csl-left-margin">348. </div><div class="csl-right-inline">Wei, L., Ding, Y., Su, R., Tang, J. &amp; Zou, Q. <a href="https://doi.org/10.1016/j.jpdc.2017.08.009">Prediction of human protein subcellular localization using deep learning</a>. <em>Journal of Parallel and Distributed Computing</em> <strong>117</strong>, 212–217 (2018).</div>
</div>
<div id="ref-elnaggarProtTransCrackingLanguage2021" class="csl-entry">
<div class="csl-left-margin">369. </div><div class="csl-right-inline">Elnaggar, A. <em>et al.</em> ProtTrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing. doi:<a href="https://doi.org/10.48550/arXiv.2007.06225">10.48550/arXiv.2007.06225</a>.</div>
</div>
<div id="ref-rosenblattPerceptronProbabilisticModel1958" class="csl-entry">
<div class="csl-left-margin">671. </div><div class="csl-right-inline">Rosenblatt, F. <a href="https://doi.org/10.1037/h0042519">The perceptron: A probabilistic model for information storage and organization in the brain</a>. <em>Psychological Review</em> <strong>65</strong>, 386–408 (1958).</div>
</div>
<div id="ref-rumelhartLearningRepresentationsBackpropagating1986" class="csl-entry">
<div class="csl-left-margin">672. </div><div class="csl-right-inline">Rumelhart, D. E., Hinton, G. E. &amp; Williams, R. J. <a href="https://doi.org/10.1038/323533a0">Learning representations by back-propagating errors</a>. <em>Nature</em> <strong>323</strong>, 533–536 (1986).</div>
</div>
<div id="ref-murtaghMultilayerPerceptronsClassification1991" class="csl-entry">
<div class="csl-left-margin">673. </div><div class="csl-right-inline">Murtagh, F. <a href="https://doi.org/10.1016/0925-2312(91)90023-5">Multilayer perceptrons for classification and regression</a>. <em>Neurocomputing</em> <strong>2</strong>, 183–197 (1991).</div>
</div>
<div id="ref-cybenkoApproximationSuperpositionsSigmoidal1989" class="csl-entry">
<div class="csl-left-margin">674. </div><div class="csl-right-inline">Cybenko, G. <a href="https://doi.org/10.1007/BF02551274">Approximation by superpositions of a sigmoidal function</a>. <em>Mathematics of Control, Signals and Systems</em> <strong>2</strong>, 303–314 (1989).</div>
</div>
<div id="ref-hornikApproximationCapabilitiesMultilayer1991a" class="csl-entry">
<div class="csl-left-margin">676. </div><div class="csl-right-inline">Hornik, K. <a href="https://doi.org/10.1016/0893-6080(91)90009-T">Approximation capabilities of multilayer feedforward networks</a>. <em>Neural Networks</em> <strong>4</strong>, 251–257 (1991).</div>
</div>
<div id="ref-lecunBackpropagationAppliedHandwritten1989" class="csl-entry">
<div class="csl-left-margin">677. </div><div class="csl-right-inline">LeCun, Y. <em>et al.</em> <a href="https://doi.org/10.1162/neco.1989.1.4.541">Backpropagation applied to handwritten zip code recognition</a>. <em>Neural Computation</em> <strong>1</strong>, 541–551 (1989).</div>
</div>
<div id="ref-lecunGradientbasedLearningApplied1998" class="csl-entry">
<div class="csl-left-margin">678. </div><div class="csl-right-inline">Lecun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. <a href="https://doi.org/10.1109/5.726791">Gradient-based learning applied to document recognition</a>. <em>Proceedings of the IEEE</em> <strong>86</strong>, 2278–2324 (1998).</div>
</div>
<div id="ref-voznicaDeepLearningPhylogenies2022" class="csl-entry">
<div class="csl-left-margin">679. </div><div class="csl-right-inline">Voznica, J. <em>et al.</em> <a href="https://doi.org/10.1038/s41467-022-31511-0">Deep learning from phylogenies to uncover the epidemiological dynamics of outbreaks</a>. <em>Nature Communications</em> <strong>13</strong>, 3896 (2022).</div>
</div>
<div id="ref-krizhevskyImageNetClassificationDeep2017" class="csl-entry">
<div class="csl-left-margin">680. </div><div class="csl-right-inline">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. <a href="https://doi.org/10.1145/3065386">ImageNet classification with deep convolutional neural networks</a>. <em>Communications of the ACM</em> <strong>60</strong>, 8490 (2017).</div>
</div>
<div id="ref-heDeepResidualLearning2016" class="csl-entry">
<div class="csl-left-margin">681. </div><div class="csl-right-inline">He, K., Zhang, X., Ren, S. &amp; Sun, J. <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Proceedings of the IEEE conference on computer vision and pattern recognition</a>. in 770–778 (2016).</div>
</div>
<div id="ref-bahdanauNeuralMachineTranslation2016" class="csl-entry">
<div class="csl-left-margin">682. </div><div class="csl-right-inline">Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. doi:<a href="https://doi.org/10.48550/arXiv.1409.0473">10.48550/arXiv.1409.0473</a>.</div>
</div>
<div id="ref-vaswaniAttentionAllYou2017" class="csl-entry">
<div class="csl-left-margin">683. </div><div class="csl-right-inline">Vaswani, A. <em>et al.</em> Attention is all you need. in vol. 30 (Curran Associates, Inc., 2017).</div>
</div>
<div id="ref-HowManyWords" class="csl-entry">
<div class="csl-left-margin">684. </div><div class="csl-right-inline"><a href="https://www.merriam-webster.com/help/faq-how-many-english-words">How many words are there in english? | merriam-webster</a>.</div>
</div>
<div id="ref-mikolovEfficientEstimationWord2013" class="csl-entry">
<div class="csl-left-margin">685. </div><div class="csl-right-inline">Mikolov, T., Chen, K., Corrado, G. &amp; Dean, J. Efficient estimation of word representations in vector space. doi:<a href="https://doi.org/10.48550/arXiv.1301.3781">10.48550/arXiv.1301.3781</a>.</div>
</div>
<div id="ref-mikolovDistributedRepresentationsWords2013" class="csl-entry">
<div class="csl-left-margin">686. </div><div class="csl-right-inline">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. &amp; Dean, J. Distributed representations of words and phrases and their compositionality. in vol. 26 (Curran Associates, Inc., 2013).</div>
</div>
<div id="ref-goldbergWord2vecExplainedDeriving2014" class="csl-entry">
<div class="csl-left-margin">687. </div><div class="csl-right-inline">Goldberg, Y. &amp; Levy, O. word2vec explained: Deriving mikolov et al.’s negative-sampling word-embedding method. doi:<a href="https://doi.org/10.48550/arXiv.1402.3722">10.48550/arXiv.1402.3722</a>.</div>
</div>
<div id="ref-ngDna2vecConsistentVector2017" class="csl-entry">
<div class="csl-left-margin">688. </div><div class="csl-right-inline">Ng, P. dna2vec: Consistent vector representations of variable-length k-mers. doi:<a href="https://doi.org/10.48550/arXiv.1701.06279">10.48550/arXiv.1701.06279</a>.</div>
</div>
<div id="ref-liangHyb4mCHybridDNA2vecbased2022" class="csl-entry">
<div class="csl-left-margin">689. </div><div class="csl-right-inline">Liang, Y. <em>et al.</em> <a href="https://doi.org/10.1186/s12859-022-04789-6">Hyb4mC: a hybrid DNA2vec-based model for DNA N4-methylcytosine sites prediction</a>. <em>BMC Bioinformatics</em> <strong>23</strong>, 258 (2022).</div>
</div>
<div id="ref-kimothiDistributedRepresentationsBiological2016" class="csl-entry">
<div class="csl-left-margin">690. </div><div class="csl-right-inline">Kimothi, D., Soni, A., Biyani, P. &amp; Hogan, J. M. Distributed representations for biological sequence analysis. doi:<a href="https://doi.org/10.48550/arXiv.1608.05949">10.48550/arXiv.1608.05949</a>.</div>
</div>
<div id="ref-asgariContinuousDistributedRepresentation2015" class="csl-entry">
<div class="csl-left-margin">691. </div><div class="csl-right-inline">Asgari, E. &amp; Mofrad, M. R. K. <a href="https://doi.org/10.1371/journal.pone.0141287">Continuous distributed representation of biological sequences for deep proteomics and genomics</a>. <em>PLoS ONE</em> <strong>10</strong>, e0141287 (2015).</div>
</div>
<div id="ref-kimothiMetricLearningBiological2017" class="csl-entry">
<div class="csl-left-margin">692. </div><div class="csl-right-inline">Kimothi, D., Shukla, A., Biyani, P., Anand, S. &amp; Hogan, J. M. 2017 IEEE 18th international workshop on signal processing advances in wireless communications (SPAWC). in 1–5 (2017). doi:<a href="https://doi.org/10.1109/SPAWC.2017.8227769">10.1109/SPAWC.2017.8227769</a>.</div>
</div>
<div id="ref-beplerLearningProteinSequence2019" class="csl-entry">
<div class="csl-left-margin">693. </div><div class="csl-right-inline">Bepler, T. &amp; Berger, B. Learning protein sequence embeddings using information from structure. doi:<a href="https://doi.org/10.48550/arXiv.1902.08661">10.48550/arXiv.1902.08661</a>.</div>
</div>
<div id="ref-songPretrainingModelBiological2021" class="csl-entry">
<div class="csl-left-margin">694. </div><div class="csl-right-inline">Song, B. <em>et al.</em> <a href="https://doi.org/10.1093/bfgp/elab025">Pretraining model for biological sequence data.</a> <em>Briefings in Functional Genomics</em> <strong>20</strong>, 181–195 (2021).</div>
</div>
<div id="ref-wangProgressMachineTranslation2021" class="csl-entry">
<div class="csl-left-margin">695. </div><div class="csl-right-inline">Wang, H., Wu, H., He, Z., Huang, L. &amp; Ward Church, K. Progress in Machine Translation. <em>Engineering</em> (2021) doi:<a href="https://doi.org/10.1016/j.eng.2021.03.023">10.1016/j.eng.2021.03.023</a>.</div>
</div>
<div id="ref-brownLanguageModelsAre2020" class="csl-entry">
<div class="csl-left-margin">696. </div><div class="csl-right-inline">Brown, T. <em>et al.</em> Language models are few-shot learners. in vol. 33 18771901 (Curran Associates, Inc., 2020).</div>
</div>
<div id="ref-devlinBERTPretrainingDeep2019" class="csl-entry">
<div class="csl-left-margin">697. </div><div class="csl-right-inline">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. doi:<a href="https://doi.org/10.48550/arXiv.1810.04805">10.48550/arXiv.1810.04805</a>.</div>
</div>
<div id="ref-madaniProGenLanguageModeling2020" class="csl-entry">
<div class="csl-left-margin">698. </div><div class="csl-right-inline">Madani, A. <em>et al.</em> ProGen: Language modeling for protein generation. <em>bioRxiv</em> (2020) doi:<a href="https://doi.org/10.1101/2020.03.07.982272">10.1101/2020.03.07.982272</a>.</div>
</div>
<div id="ref-eriknijkampProGen2ExploringBoundaries2022" class="csl-entry">
<div class="csl-left-margin">699. </div><div class="csl-right-inline">Erik Nijkamp, Jeffrey A. Ruffolo, Eli N. Weinstein, Nikhil Naik &amp; Ali Madani. ProGen2: Exploring the boundaries of protein language models. <em>ArXiv</em> (2022) doi:<a href="https://doi.org/10.48550/arxiv.2206.13517">10.48550/arxiv.2206.13517</a>.</div>
</div>
<div id="ref-beplerLearningProteinLanguage2021" class="csl-entry">
<div class="csl-left-margin">700. </div><div class="csl-right-inline">Bepler, T. &amp; Berger, B. <a href="https://doi.org/10.1016/j.cels.2021.05.017">Learning the protein language: Evolution, structure, and function.</a> <em>Cell systems</em> <strong>12</strong>, (2021).</div>
</div>
<div id="ref-raoTransformerProteinLanguage2020" class="csl-entry">
<div class="csl-left-margin">701. </div><div class="csl-right-inline">Rao, R., Meier, J., Sercu, T., Ovchinnikov, S. &amp; Rives, A. Transformer protein language models are unsupervised structure learners. doi:<a href="https://doi.org/10.1101/2020.12.15.422761">10.1101/2020.12.15.422761</a>.</div>
</div>
<div id="ref-bhattacharyaSingleLayersAttention2020" class="csl-entry">
<div class="csl-left-margin">703. </div><div class="csl-right-inline">Bhattacharya, N. <em>et al.</em> Single Layers of Attention Suffice to Predict Protein Contacts. doi:<a href="https://doi.org/10.1101/2020.12.21.423882">10.1101/2020.12.21.423882</a>.</div>
</div>
<div id="ref-huExploringEvolutionbasedFree2022" class="csl-entry">
<div class="csl-left-margin">704. </div><div class="csl-right-inline">Hu, M. <em>et al.</em> Exploring evolution-based &amp; -free protein language models as protein function predictors. doi:<a href="https://doi.org/10.48550/arXiv.2206.06583">10.48550/arXiv.2206.06583</a>.</div>
</div>
<div id="ref-meierLanguageModelsEnable2021" class="csl-entry">
<div class="csl-left-margin">705. </div><div class="csl-right-inline">Meier, J. <em>et al.</em> <a href="https://doi.org/10.1101/2021.07.09.450648">Language models enable zero-shot prediction of the effects of mutations on protein function</a>. <em>bioRxiv</em> <strong>34</strong>, (2021).</div>
</div>
<div id="ref-hieEvolutionaryVelocityProtein2022" class="csl-entry">
<div class="csl-left-margin">706. </div><div class="csl-right-inline">Hie, B., Kevin K Yang &amp; Kim, S. K. Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins. <em>Cell systems</em> (2022) doi:<a href="https://doi.org/10.1016/j.cels.2022.01.003">10.1016/j.cels.2022.01.003</a>.</div>
</div>
<div id="ref-jiDNABERTPretrainedBidirectional2021" class="csl-entry">
<div class="csl-left-margin">707. </div><div class="csl-right-inline">Ji, Y., Zhou, Z., Liu, H. &amp; Davuluri, R. V. <a href="https://doi.org/10.1093/bioinformatics/btab083">DNABERT: Pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</a>. <em>Bioinformatics</em> <strong>37</strong>, 2112–2120 (2021).</div>
</div>
<div id="ref-benegasDNALanguageModels2022" class="csl-entry">
<div class="csl-left-margin">708. </div><div class="csl-right-inline">Benegas, G., Batra, S. S. &amp; Song, Y. S. DNA language models are powerful zero-shot predictors of non-coding variant effects. doi:<a href="https://doi.org/10.1101/2022.08.22.504706">10.1101/2022.08.22.504706</a>.</div>
</div>
<div id="ref-caiGenomewidePredictionSmall2020" class="csl-entry">
<div class="csl-left-margin">709. </div><div class="csl-right-inline">Cai, T. <em>et al.</em> Genome-wide Prediction of Small Molecule Binding to Remote Orphan Proteins Using Distilled Sequence Alignment Embedding. doi:<a href="https://doi.org/10.1101/2020.08.04.236729">10.1101/2020.08.04.236729</a>.</div>
</div>
<div id="ref-raoMSATransformer2021" class="csl-entry">
<div class="csl-left-margin">710. </div><div class="csl-right-inline">Rao, R. <em>et al.</em> MSA transformer. <em>bioRxiv</em> (2021) doi:<a href="https://doi.org/10.1101/2021.02.12.430858">10.1101/2021.02.12.430858</a>.</div>
</div>
<div id="ref-sercuNeuralPottsModel2021" class="csl-entry">
<div class="csl-left-margin">711. </div><div class="csl-right-inline">Sercu, T. <em>et al.</em> Neural Potts Model. doi:<a href="https://doi.org/10.1101/2021.04.08.439084">10.1101/2021.04.08.439084</a>.</div>
</div>
<div id="ref-sturmfelsProfilePredictionAlignmentBased2020" class="csl-entry">
<div class="csl-left-margin">712. </div><div class="csl-right-inline">Sturmfels, P., Vig, J., Madani, A. &amp; Rajani, N. F. Profile prediction: An alignment-based pre-training task for protein sequence models. doi:<a href="https://doi.org/10.48550/arXiv.2012.00195">10.48550/arXiv.2012.00195</a>.</div>
</div>
<div id="ref-baidDeepConsensusImprovesAccuracy2022" class="csl-entry">
<div class="csl-left-margin">713. </div><div class="csl-right-inline">Baid, G. <em>et al.</em> DeepConsensus improves the accuracy of sequences with a gap-aware sequence transformer. <em>Nature Biotechnology</em> 1–7 (2022) doi:<a href="https://doi.org/10.1038/s41587-022-01435-7">10.1038/s41587-022-01435-7</a>.</div>
</div>
<div id="ref-vigBERTologyMeetsBiology2021" class="csl-entry">
<div class="csl-left-margin">714. </div><div class="csl-right-inline">Vig, J. <em>et al.</em> BERTology meets biology: Interpreting attention in protein language models. doi:<a href="https://doi.org/10.48550/arXiv.2006.15222">10.48550/arXiv.2006.15222</a>.</div>
</div>
<div id="ref-gaoNovelSequenceAlignment2021" class="csl-entry">
<div class="csl-left-margin">715. </div><div class="csl-right-inline">Gao, M. &amp; Skolnick, J. <a href="https://doi.org/10.1093/bioinformatics/btaa810">A novel sequence alignment algorithm based on deep learning of the protein folding code</a>. <em>Bioinformatics</em> <strong>37</strong>, 490–496 (2021).</div>
</div>
<div id="ref-mortonProteinStructuralAlignments2020" class="csl-entry">
<div class="csl-left-margin">716. </div><div class="csl-right-inline">Morton, J. T. <em>et al.</em> Protein Structural Alignments From Sequence. doi:<a href="https://doi.org/10.1101/2020.11.03.365932">10.1101/2020.11.03.365932</a>.</div>
</div>
<div id="ref-bermanWorldwideProteinData2007" class="csl-entry">
<div class="csl-left-margin">717. </div><div class="csl-right-inline">Berman, H., Henrick, K., Nakamura, H. &amp; Markley, J. L. <a href="https://doi.org/10.1093/nar/gkl971">The worldwide protein data bank (wwPDB): Ensuring a single, uniform archive of PDB data</a>. <em>Nucleic Acids Research</em> <strong>35</strong>, D301–D303 (2007).</div>
</div>
<div id="ref-guoComprehensiveStudyEnhancing2021" class="csl-entry">
<div class="csl-left-margin">718. </div><div class="csl-right-inline">Guo, Y., Wu, J., Ma, H., Wang, S. &amp; Huang, J. <a href="https://doi.org/10.1089/cmb.2020.0416">Comprehensive study on enhancing low-quality position-specific scoring matrix with deep learning for accurate protein structure property prediction: Using bagging multiple sequence alignment learning</a>. <em>Journal of Computational Biology</em> <strong>28</strong>, 346–361 (2021).</div>
</div>
<div id="ref-llinares-lopezDeepEmbeddingAlignment2022" class="csl-entry">
<div class="csl-left-margin">719. </div><div class="csl-right-inline">Llinares-López, F., Berthet, Q., Blondel, M., Teboul, O. &amp; Vert, J.-P. Deep embedding and alignment of protein sequences. doi:<a href="https://doi.org/10.1101/2021.11.15.468653">10.1101/2021.11.15.468653</a>.</div>
</div>
<div id="ref-suzekUniRefClustersComprehensive2015" class="csl-entry">
<div class="csl-left-margin">720. </div><div class="csl-right-inline">Suzek, B. E. <em>et al.</em> <a href="https://doi.org/10.1093/bioinformatics/btu739">UniRef clusters: A comprehensive and scalable alternative for improving sequence similarity searches</a>. <em>Bioinformatics</em> <strong>31</strong>, 926–932 (2015).</div>
</div>
<div id="ref-mistryPfamProteinFamilies2021" class="csl-entry">
<div class="csl-left-margin">721. </div><div class="csl-right-inline">Mistry, J. <em>et al.</em> <a href="https://doi.org/10.1093/nar/gkaa913">Pfam: The protein families database in 2021</a>. <em>Nucleic Acids Research</em> <strong>49</strong>, D412–D419 (2021).</div>
</div>
<div id="ref-pettiEndtoendLearningMultiple2022" class="csl-entry">
<div class="csl-left-margin">722. </div><div class="csl-right-inline">Petti, S. <em>et al.</em> End-to-end learning of multiple sequence alignments with differentiable Smith-Waterman. doi:<a href="https://doi.org/10.1101/2021.10.23.465204">10.1101/2021.10.23.465204</a>.</div>
</div>
<div id="ref-dotanHarnessingMachineTranslation2022" class="csl-entry">
<div class="csl-left-margin">723. </div><div class="csl-right-inline">Dotan, E. <em>et al.</em> Harnessing machine translation methods for sequence alignment. doi:<a href="https://doi.org/10.1101/2022.07.22.501063">10.1101/2022.07.22.501063</a>.</div>
</div>
<div id="ref-wangLinformerSelfAttentionLinear2020" class="csl-entry">
<div class="csl-left-margin">724. </div><div class="csl-right-inline">Wang, S., Li, B. Z., Khabsa, M., Fang, H. &amp; Ma, H. Linformer: Self-attention with linear complexity. doi:<a href="https://doi.org/10.48550/arXiv.2006.04768">10.48550/arXiv.2006.04768</a>.</div>
</div>
<div id="ref-xiongNystromformerNystrombasedAlgorithm2021" class="csl-entry">
<div class="csl-left-margin">725. </div><div class="csl-right-inline">Xiong, Y. <em>et al.</em> <a href="https://doi.org/10.1609/aaai.v35i16.17664">Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention</a>. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> <strong>35</strong>, 14138–14148 (2021).</div>
</div>
<div id="ref-childGeneratingLongSequences2019" class="csl-entry">
<div class="csl-left-margin">726. </div><div class="csl-right-inline">Child, R., Gray, S., Radford, A. &amp; Sutskever, I. Generating long sequences with sparse transformers. doi:<a href="https://doi.org/10.48550/arXiv.1904.10509">10.48550/arXiv.1904.10509</a>.</div>
</div>
<div id="ref-correiaAdaptivelySparseTransformers2019" class="csl-entry">
<div class="csl-left-margin">727. </div><div class="csl-right-inline">Correia, G. M., Niculae, V. &amp; Martins, A. F. T. EMNLP-IJCNLP 2019. in 21742184 (Association for Computational Linguistics, 2019). doi:<a href="https://doi.org/10.18653/v1/D19-1223">10.18653/v1/D19-1223</a>.</div>
</div>
<div id="ref-sukhbaatarAdaptiveAttentionSpan2019" class="csl-entry">
<div class="csl-left-margin">728. </div><div class="csl-right-inline">Sukhbaatar, S., Grave, E., Bojanowski, P. &amp; Joulin, A. Adaptive attention span in transformers. doi:<a href="https://doi.org/10.48550/arXiv.1905.07799">10.48550/arXiv.1905.07799</a>.</div>
</div>
<div id="ref-wuLiteTransformerLongShort2020" class="csl-entry">
<div class="csl-left-margin">729. </div><div class="csl-right-inline">Wu, Z., Liu, Z., Lin, J., Lin, Y. &amp; Han, S. Lite transformer with long-short range attention. doi:<a href="https://doi.org/10.48550/arXiv.2004.11886">10.48550/arXiv.2004.11886</a>.</div>
</div>
<div id="ref-kitaevReformerEfficientTransformer2020" class="csl-entry">
<div class="csl-left-margin">730. </div><div class="csl-right-inline">Kitaev, N., Kaiser, Ł. &amp; Levskaya, A. Reformer: The efficient transformer. doi:<a href="https://doi.org/10.48550/arXiv.2001.04451">10.48550/arXiv.2001.04451</a>.</div>
</div>
<div id="ref-choromanskiMaskedLanguageModeling2020" class="csl-entry">
<div class="csl-left-margin">731. </div><div class="csl-right-inline">Choromanski, K. <em>et al.</em> Masked language modeling for proteins via linearly scalable long-context transformers. doi:<a href="https://doi.org/10.48550/arXiv.2006.03555">10.48550/arXiv.2006.03555</a>.</div>
</div>
<div id="ref-bhattacharyaInterpretingPottsTransformer2021" class="csl-entry">
<div class="csl-left-margin">732. </div><div class="csl-right-inline">Bhattacharya, N. <em>et al.</em> Interpreting potts and transformer protein models through the lens of simplified attention. in 34–45 (WORLD SCIENTIFIC, 2021). doi:<a href="https://doi.org/10.1142/9789811250477_0004">10.1142/9789811250477_0004</a>.</div>
</div>
<div id="ref-kraskaCaseLearnedIndex2018" class="csl-entry">
<div class="csl-left-margin">733. </div><div class="csl-right-inline">Kraska, T., Beutel, A., Chi, E. H., Dean, J. &amp; Polyzotis, N. The case for learned index structures. in 489504 (Association for Computing Machinery, 2018). doi:<a href="https://doi.org/10.1145/3183713.3196909">10.1145/3183713.3196909</a>.</div>
</div>
<div id="ref-jungBWAMEMEBWAMEMEmulated2022" class="csl-entry">
<div class="csl-left-margin">734. </div><div class="csl-right-inline">Jung, Y. &amp; Han, D. <a href="https://doi.org/10.1093/bioinformatics/btac137">BWA-MEME: BWA-MEM emulated with a machine learning approach</a>. <em>Bioinformatics</em> <strong>38</strong>, 2404–2413 (2022).</div>
</div>
<div id="ref-kirscheSaplingAcceleratingSuffix2021" class="csl-entry">
<div class="csl-left-margin">735. </div><div class="csl-right-inline">Kirsche, M., Das, A. &amp; Schatz, M. C. <a href="https://doi.org/10.1093/bioinformatics/btaa911">Sapling: Accelerating suffix array queries with learned data models</a>. <em>Bioinformatics</em> <strong>37</strong>, 744–749 (2021).</div>
</div>
<div id="ref-hoLISALearnedIndexes2021" class="csl-entry">
<div class="csl-left-margin">736. </div><div class="csl-right-inline">Ho, D. <em>et al.</em> LISA: Learned Indexes for Sequence Analysis. doi:<a href="https://doi.org/10.1101/2020.12.22.423964">10.1101/2020.12.22.423964</a>.</div>
</div>
<div id="ref-hoangDifferentiableLearningSequenceSpecific2022" class="csl-entry">
<div class="csl-left-margin">737. </div><div class="csl-right-inline">Hoang, M., Zheng, H. &amp; Kingsford, C. Differentiable learning of sequence-specific minimizer schemes with DeepMinimizer. <em>Journal of Computational Biology</em> (2022) doi:<a href="https://doi.org/10.1089/cmb.2022.0275">10.1089/cmb.2022.0275</a>.</div>
</div>
<div id="ref-minTargetNetFunctionalMicroRNA2022" class="csl-entry">
<div class="csl-left-margin">738. </div><div class="csl-right-inline">Min, S., Lee, B. &amp; Yoon, S. <a href="https://doi.org/10.1093/bioinformatics/btab733">TargetNet: Functional microRNA target prediction with deep neural networks</a>. <em>Bioinformatics</em> <strong>38</strong>, 671–677 (2022).</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="HIV-paper.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="global-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lucblassel/phd-manuscript/edit/main/07-learning-alignments.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments</title>
  <meta name="description" content="Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="lucblassel/phd-manuscript" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Learning alignments, an interesting perspective | From sequences to knowledge, improving and learning from sequence alignments" />
  
  
  

<meta name="author" content="Luc Blassel" />


<meta name="date" content="2022-09-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="HIV-paper.html"/>
<link rel="next" href="global-conclusion.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<link href="libs/tabwid-1.0.0/scrool.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">From sequences to knowledge,</br> improving and learning from sequence alignments.</a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#résumé"><i class="fa fa-check"></i>Résumé</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="list-of-acronyms-and-abbreviations.html"><a href="list-of-acronyms-and-abbreviations.html"><i class="fa fa-check"></i>List of Acronyms and Abbreviations</a></li>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i>General Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#research-output"><i class="fa fa-check"></i>Research output</a>
<ul>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#journal-publications"><i class="fa fa-check"></i>Journal publications</a></li>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#presentations-and-posters"><i class="fa fa-check"></i>Presentations and posters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html"><i class="fa fa-check"></i><b>1</b> What is Sequence data ?</a>
<ul>
<li class="chapter" data-level="1.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#biological-sequences-a-primer"><i class="fa fa-check"></i><b>1.1</b> Biological sequences, a primer</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#what-is-dna"><i class="fa fa-check"></i><b>1.1.1</b> What is DNA ?</a></li>
<li class="chapter" data-level="1.1.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#from-information-to-action"><i class="fa fa-check"></i><b>1.1.2</b> From Information to action</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#obtaining-sequence-data"><i class="fa fa-check"></i><b>1.2</b> Obtaining sequence data</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#sanger-sequencing-a-breakthrough"><i class="fa fa-check"></i><b>1.2.1</b> Sanger sequencing, a breakthrough</a></li>
<li class="chapter" data-level="1.2.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#next-generation-sequencing"><i class="fa fa-check"></i><b>1.2.2</b> Next-generation sequencing</a></li>
<li class="chapter" data-level="1.2.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#long-read-sequencing"><i class="fa fa-check"></i><b>1.2.3</b> Long read sequencing</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#sequencing-errors-how-to-account-for-them"><i class="fa fa-check"></i><b>1.3</b> Sequencing errors, how to account for them ?</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#error-correction-methods"><i class="fa fa-check"></i><b>1.3.1</b> Error correction methods</a></li>
<li class="chapter" data-level="1.3.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#more-accurate-sequencing-methods"><i class="fa fa-check"></i><b>1.3.2</b> More accurate sequencing methods</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#the-special-case-of-homopolymers"><i class="fa fa-check"></i><b>1.4</b> The special case of homopolymers</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#homopolymers-and-the-human-genome"><i class="fa fa-check"></i><b>1.4.1</b> Homopolymers and the human genome</a></li>
<li class="chapter" data-level="1.4.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#homopolymers-and-long-reads"><i class="fa fa-check"></i><b>1.4.2</b> Homopolymers and long reads</a></li>
<li class="chapter" data-level="1.4.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#accounting-for-homopolymers"><i class="fa fa-check"></i><b>1.4.3</b> Accounting for homopolymers</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html"><i class="fa fa-check"></i><b>2</b> Aligning sequence data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#what-is-an-alignment"><i class="fa fa-check"></i><b>2.1</b> What is an alignment ?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#why-align"><i class="fa fa-check"></i><b>2.1.1</b> Why align ?</a></li>
<li class="chapter" data-level="2.1.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#how-to-align-two-sequences"><i class="fa fa-check"></i><b>2.1.2</b> How to align two sequences ?</a></li>
<li class="chapter" data-level="2.1.3" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#scoring-and-substitution-models"><i class="fa fa-check"></i><b>2.1.3</b> Scoring and substitution models</a></li>
<li class="chapter" data-level="2.1.4" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#dealing-with-gaps"><i class="fa fa-check"></i><b>2.1.4</b> Dealing with gaps</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#how-do-we-speed-up-pairwise-alignment"><i class="fa fa-check"></i><b>2.2</b> How do we speed up pairwise alignment ?</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#changing-the-method"><i class="fa fa-check"></i><b>2.2.1</b> Changing the method</a></li>
<li class="chapter" data-level="2.2.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#seed-and-extend-with-data-structures"><i class="fa fa-check"></i><b>2.2.2</b> Seed and extend with data structures</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#the-specificities-of-read-mapping"><i class="fa fa-check"></i><b>2.3</b> The specificities of read-mapping</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#what-is-read-mapping"><i class="fa fa-check"></i><b>2.3.1</b> What is read-mapping ?</a></li>
<li class="chapter" data-level="2.3.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#challenges-of-read-mapping"><i class="fa fa-check"></i><b>2.3.2</b> Challenges of read-mapping</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#multiple-sequence-alignment"><i class="fa fa-check"></i><b>2.4</b> Multiple sequence alignment</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#progressive-alignment"><i class="fa fa-check"></i><b>2.4.1</b> Progressive alignment</a></li>
<li class="chapter" data-level="2.4.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#other-methods"><i class="fa fa-check"></i><b>2.4.2</b> Other methods</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#conclusion-1"><i class="fa fa-check"></i><b>2.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="HPC-paper.html"><a href="HPC-paper.html"><i class="fa fa-check"></i><b>3</b> Contribution 1: Improving read alignment by exploring a sequence transformation space</a>
<ul>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#highlights"><i class="fa fa-check"></i>Highlights</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#graphical-abstract"><i class="fa fa-check"></i>Graphical Abstract</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="3.1" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="HPC-paper.html"><a href="HPC-paper.html#methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="HPC-paper.html"><a href="HPC-paper.html#sec:msr-def"><i class="fa fa-check"></i><b>3.2.1</b> Streaming sequence reductions</a></li>
<li class="chapter" data-level="3.2.2" data-path="HPC-paper.html"><a href="HPC-paper.html#sec:enum"><i class="fa fa-check"></i><b>3.2.2</b> Restricting the space of streaming sequence reductions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="HPC-paper.html"><a href="HPC-paper.html#datasets-and-pipelines"><i class="fa fa-check"></i><b>3.3</b> Datasets and Pipelines</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="HPC-paper.html"><a href="HPC-paper.html#datasets"><i class="fa fa-check"></i><b>3.3.1</b> Datasets</a></li>
<li class="chapter" data-level="3.3.2" data-path="HPC-paper.html"><a href="HPC-paper.html#simulation-pipeline"><i class="fa fa-check"></i><b>3.3.2</b> Simulation pipeline</a></li>
<li class="chapter" data-level="3.3.3" data-path="HPC-paper.html"><a href="HPC-paper.html#evaluation-pipeline"><i class="fa fa-check"></i><b>3.3.3</b> Evaluation pipeline</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-results"><i class="fa fa-check"></i><b>3.4</b> Results</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="HPC-paper.html"><a href="HPC-paper.html#selection-of-mapping-friendly-sequence-reductions"><i class="fa fa-check"></i><b>3.4.1</b> Selection of mapping-friendly sequence reductions</a></li>
<li class="chapter" data-level="3.4.2" data-path="HPC-paper.html"><a href="HPC-paper.html#mapping-friendly-sequence-reductions-lead-to-lower-mapping-errors-on-whole-genomes"><i class="fa fa-check"></i><b>3.4.2</b> Mapping-friendly sequence reductions lead to lower mapping errors on whole genomes</a></li>
<li class="chapter" data-level="3.4.3" data-path="HPC-paper.html"><a href="HPC-paper.html#mapping-friendly-sequence-reductions-increase-mapping-quality-on-repeated-regions-of-the-human-genome"><i class="fa fa-check"></i><b>3.4.3</b> Mapping-friendly sequence reductions increase mapping quality on repeated regions of the human genome</a></li>
<li class="chapter" data-level="3.4.4" data-path="HPC-paper.html"><a href="HPC-paper.html#raw-mapping-improves-upon-hpc-on-centromeric-regions"><i class="fa fa-check"></i><b>3.4.4</b> Raw mapping improves upon HPC on centromeric regions</a></li>
<li class="chapter" data-level="3.4.5" data-path="HPC-paper.html"><a href="HPC-paper.html#positions-of-incorrectly-mapped-reads-across-the-entire-human-genome"><i class="fa fa-check"></i><b>3.4.5</b> Positions of incorrectly mapped reads across the entire human genome</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="HPC-paper.html"><a href="HPC-paper.html#discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
<li class="chapter" data-level="3.6" data-path="HPC-paper.html"><a href="HPC-paper.html#limitations-of-this-study"><i class="fa fa-check"></i><b>3.6</b> Limitations of this study</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#author-contributions"><i class="fa fa-check"></i>Author contributions</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#declaration-of-interests"><i class="fa fa-check"></i>Declaration of interests</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#star-methods"><i class="fa fa-check"></i>STAR Methods</a>
<ul>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#lead-contact"><i class="fa fa-check"></i>Lead contact</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#materials-availability"><i class="fa fa-check"></i>Materials availability</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#data-and-code-availability"><i class="fa fa-check"></i>Data and code availability</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#method-details"><i class="fa fa-check"></i>Method details</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#supplementary-information"><i class="fa fa-check"></i>Supplementary information</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html"><i class="fa fa-check"></i><b>4</b> Learning from sequences and alignments</a>
<ul>
<li class="chapter" data-level="4.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#why-learn-from-alignments"><i class="fa fa-check"></i><b>4.1</b> Why learn from alignments ?</a></li>
<li class="chapter" data-level="4.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#what-to-learn"><i class="fa fa-check"></i><b>4.2</b> What to learn ?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#supervised-learning"><i class="fa fa-check"></i><b>4.2.1</b> Supervised learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#unsupervised-learning"><i class="fa fa-check"></i><b>4.2.2</b> Unsupervised learning</a></li>
<li class="chapter" data-level="4.2.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#others-paradigms"><i class="fa fa-check"></i><b>4.2.3</b> Others paradigms</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#how-to-learn"><i class="fa fa-check"></i><b>4.3</b> How to learn ?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#general-setting"><i class="fa fa-check"></i><b>4.3.1</b> General setting</a></li>
<li class="chapter" data-level="4.3.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#tests-and-statistical-learning"><i class="fa fa-check"></i><b>4.3.2</b> Tests and statistical learning</a></li>
<li class="chapter" data-level="4.3.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#more-complex-methods"><i class="fa fa-check"></i><b>4.3.3</b> More complex methods</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#preprocessing-the-alignment-for-machine-learning"><i class="fa fa-check"></i><b>4.4</b> Preprocessing the alignment for machine learning</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#general-purpose-encodings"><i class="fa fa-check"></i><b>4.4.1</b> General purpose encodings</a></li>
<li class="chapter" data-level="4.4.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#biological-sequence-specific-encodings"><i class="fa fa-check"></i><b>4.4.2</b> Biological sequence-specific encodings</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#conclusion-2"><i class="fa fa-check"></i><b>4.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html"><i class="fa fa-check"></i><b>5</b> Viruses, HIV and drug resistance</a>
<ul>
<li class="chapter" data-level="5.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#what-are-viruses"><i class="fa fa-check"></i><b>5.1</b> What are viruses ?</a></li>
<li class="chapter" data-level="5.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#getting-to-know-hiv"><i class="fa fa-check"></i><b>5.2</b> Getting to know HIV</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#quick-presentation-of-hiv"><i class="fa fa-check"></i><b>5.2.1</b> Quick Presentation of HIV</a></li>
<li class="chapter" data-level="5.2.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#the-replication-cycle-of-hiv"><i class="fa fa-check"></i><b>5.2.2</b> The replication cycle of HIV</a></li>
<li class="chapter" data-level="5.2.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#genetics-of-hiv"><i class="fa fa-check"></i><b>5.2.3</b> Genetics of HIV</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#drug-resistance-in-hiv"><i class="fa fa-check"></i><b>5.3</b> Drug resistance in HIV</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#a-quick-history-of-art"><i class="fa fa-check"></i><b>5.3.1</b> A quick history of ART</a></li>
<li class="chapter" data-level="5.3.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#drug-mechanisms"><i class="fa fa-check"></i><b>5.3.2</b> Main mechanisms of viral proteins, antiretroviral drugs and associated resistance.</a></li>
<li class="chapter" data-level="5.3.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#consequences-of-resistance-on-global-health"><i class="fa fa-check"></i><b>5.3.3</b> Consequences of resistance on global health</a></li>
<li class="chapter" data-level="5.3.4" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#finding-drms"><i class="fa fa-check"></i><b>5.3.4</b> Finding DRMs </a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#conclusion-3"><i class="fa fa-check"></i><b>5.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="HIV-paper.html"><a href="HIV-paper.html"><i class="fa fa-check"></i><b>6</b> Contribution 2: Inferring mutation roles from sequence alignments using machine learning</a>
<ul>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#abstract-paper"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#author-summary"><i class="fa fa-check"></i>Author summary</a></li>
<li class="chapter" data-level="6.1" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="HIV-paper.html"><a href="HIV-paper.html#materials-and-methods"><i class="fa fa-check"></i><b>6.2</b> Materials and methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="HIV-paper.html"><a href="HIV-paper.html#data"><i class="fa fa-check"></i><b>6.2.1</b> Data</a></li>
<li class="chapter" data-level="6.2.2" data-path="HIV-paper.html"><a href="HIV-paper.html#classifier-training"><i class="fa fa-check"></i><b>6.2.2</b> Classifier training</a></li>
<li class="chapter" data-level="6.2.3" data-path="HIV-paper.html"><a href="HIV-paper.html#measuring-classifier-performance"><i class="fa fa-check"></i><b>6.2.3</b> Measuring classifier performance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-results"><i class="fa fa-check"></i><b>6.3</b> Results</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="HIV-paper.html"><a href="HIV-paper.html#classifier-performance-interpretation"><i class="fa fa-check"></i><b>6.3.1</b> Classifier performance &amp; interpretation</a></li>
<li class="chapter" data-level="6.3.2" data-path="HIV-paper.html"><a href="HIV-paper.html#additional-classification-results"><i class="fa fa-check"></i><b>6.3.2</b> Additional classification results</a></li>
<li class="chapter" data-level="6.3.3" data-path="HIV-paper.html"><a href="HIV-paper.html#identifying-new-mutations-from-classifiers"><i class="fa fa-check"></i><b>6.3.3</b> Identifying new mutations from classifiers</a></li>
<li class="chapter" data-level="6.3.4" data-path="HIV-paper.html"><a href="HIV-paper.html#detailed-analysis-of-potentially-resistance-associated-mutations"><i class="fa fa-check"></i><b>6.3.4</b> Detailed analysis of potentially resistance-associated mutations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="HIV-paper.html"><a href="HIV-paper.html#discussion-and-perspectives"><i class="fa fa-check"></i><b>6.4</b> Discussion and perspectives</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#supporting-information"><i class="fa fa-check"></i>Supporting Information</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html"><i class="fa fa-check"></i><b>7</b> Learning alignments, an interesting perspective</a>
<ul>
<li class="chapter" data-level="7.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#deep-learning-and-sequences"><i class="fa fa-check"></i><b>7.1</b> Deep learning and sequences</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#intro-to-deep-learning"><i class="fa fa-check"></i><b>7.1.1</b> Intro to deep learning</a></li>
<li class="chapter" data-level="7.1.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learned-sequence-embeddings"><i class="fa fa-check"></i><b>7.1.2</b> Learned sequence embeddings</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learning-sequence-alignment"><i class="fa fa-check"></i><b>7.2</b> Learning sequence alignment</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#predicting-a-substitution-matrix"><i class="fa fa-check"></i><b>7.2.1</b> Predicting a substitution matrix</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#predicting-an-alignment"><i class="fa fa-check"></i><b>7.2.2</b> predicting an alignment</a></li>
<li class="chapter" data-level="7.2.3" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learning-seeds"><i class="fa fa-check"></i><b>7.2.3</b> Learning seeds</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html"><i class="fa fa-check"></i>Global conclusion</a>
<ul>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#hpc-part"><i class="fa fa-check"></i>HPC part</a></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#hiv-part"><i class="fa fa-check"></i>HIV part</a></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#final-words"><i class="fa fa-check"></i>Final words</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="global-references.html"><a href="global-references.html"><i class="fa fa-check"></i>Global References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="HPC-appendix.html"><a href="HPC-appendix.html"><i class="fa fa-check"></i><b>A</b> Supporting Information for “Mapping-friendly sequence reductions: going beyond homopolymer compression”</a>
<ul>
<li class="chapter" data-level="A.1" data-path="HPC-appendix.html"><a href="HPC-appendix.html#appendix:tandemtools"><i class="fa fa-check"></i><b>A.1</b> “TandemTools” dataset generation</a></li>
<li class="chapter" data-level="A.2" data-path="HPC-appendix.html"><a href="HPC-appendix.html#msr-performance-comparison"><i class="fa fa-check"></i><b>A.2</b> MSR performance comparison</a></li>
<li class="chapter" data-level="A.3" data-path="HPC-appendix.html"><a href="HPC-appendix.html#analyzing-read-origin-on-whole-human-genome"><i class="fa fa-check"></i><b>A.3</b> Analyzing read origin on whole human genome</a></li>
<li class="chapter" data-level="A.4" data-path="HPC-appendix.html"><a href="HPC-appendix.html#performance-of-msrs-on-the-drosophila-genome"><i class="fa fa-check"></i><b>A.4</b> Performance of MSRs on the Drosophila genome</a></li>
<li class="chapter" data-level="A.5" data-path="HPC-appendix.html"><a href="HPC-appendix.html#key-resource-table"><i class="fa fa-check"></i><b>A.5</b> Key Resource Table</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html"><i class="fa fa-check"></i><b>B</b> Supporting Information for “HIV and DRMs”</a>
<ul>
<li class="chapter" data-level="B.1" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html#detailed-list-of-hiv-1-protein-structures-used-for-figure-generation."><i class="fa fa-check"></i><b>B.1</b> Detailed list of HIV-1 protein structures used for figure generation.</a></li>
<li class="chapter" data-level="B.2" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html#list-of-all-antiretroviral-drugs"><i class="fa fa-check"></i><b>B.2</b> List of all antiretroviral drugs</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="HIV-appendix.html"><a href="HIV-appendix.html"><i class="fa fa-check"></i><b>C</b> Supporting Information for “Using Machine Learning and Big Data to Explore the Drug Resistance Landscape in HIV”</a>
<ul>
<li class="chapter" data-level="C.1" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S1-Appendix"><i class="fa fa-check"></i><b>C.1</b> S1 Appendix (Technical appendix).</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="HIV-appendix.html"><a href="HIV-appendix.html#data-appendix"><i class="fa fa-check"></i><b>C.1.1</b> Data</a></li>
<li class="chapter" data-level="C.1.2" data-path="HIV-appendix.html"><a href="HIV-appendix.html#classifiers"><i class="fa fa-check"></i><b>C.1.2</b> Classifiers</a></li>
<li class="chapter" data-level="C.1.3" data-path="HIV-appendix.html"><a href="HIV-appendix.html#scoring"><i class="fa fa-check"></i><b>C.1.3</b> Scoring</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s1-fig."><i class="fa fa-check"></i><b>C.2</b> S1 Fig.</a></li>
<li class="chapter" data-level="C.3" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s2-fig."><i class="fa fa-check"></i><b>C.3</b> S2 Fig.</a></li>
<li class="chapter" data-level="C.4" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s3-fig."><i class="fa fa-check"></i><b>C.4</b> S3 Fig.</a></li>
<li class="chapter" data-level="C.5" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S1-Table"><i class="fa fa-check"></i><b>C.5</b> S1 Table.</a></li>
<li class="chapter" data-level="C.6" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S2-Appendix"><i class="fa fa-check"></i><b>C.6</b> S2 Appendix. (Fisher exact tests)</a></li>
<li class="chapter" data-level="C.7" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s1-data."><i class="fa fa-check"></i><b>C.7</b> S1 Data.</a></li>
<li class="chapter" data-level="C.8" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s2-data."><i class="fa fa-check"></i><b>C.8</b> S2 Data.</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://lucblassel.com" target="blank">Back to main website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">From sequences to knowledge, improving and learning from sequence alignments</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="learning-alignments-an-interesting-perspective" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Learning alignments, an interesting perspective<a href="learning-alignments-an-interesting-perspective.html#learning-alignments-an-interesting-perspective" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Recently, machine learning methods have been increasingly applied to the process of alignment. Learning an alignment method through machine learning could result in methods with less design biases and with data-driven insights.</p>
<div id="deep-learning-and-sequences" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Deep learning and sequences<a href="learning-alignments-an-interesting-perspective.html#deep-learning-and-sequences" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As many of these techniques are based on deep learning, which I did not introduce in Chapter <a href="learning-from-sequences-and-alignments.html#learning-from-sequences-and-alignments">4</a>, I will first introduce deep learning very shortly. I will then introduce the concept of learned sequence embeddings which have become very useful for machine sequence alignment.</p>
<div id="intro-to-deep-learning" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Intro to deep learning<a href="learning-alignments-an-interesting-perspective.html#intro-to-deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Deep learning is the process of learning using neural networks. Neural all started in 1958 when Rosenblatt proposed the <em>perceptron</em><span class="citation"><sup><a href="#ref-rosenblattPerceptronProbabilisticModel1958" role="doc-biblioref">671</a></sup></span>. This learning algorithm was loosely inspired by biological neurons, which led to the name: <em>neural networks.</em> The perceptron takes as input <span class="math inline">\(n\)</span> values, these are used in a weighted sum that is then fed through an <em>activation function.</em> The output of this function is the output of the perceptron. Originally, to replicate biological neurons, the activation function was a step function where, the perceptron has an output only if the weighted sum crosses a given threshold. This structure is often represented through a computational graph like in Figure <a href="learning-alignments-an-interesting-perspective.html#fig:perceptron">7.1</a>. By tweaking the weights of the inputs, the perceptron can be used to solve linear separation problems.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:perceptron"></span>
<img src="figures/Learn-alignments/perceptron.png" alt="**Computational graph of a perceptron.**  
Here, $n$ inputs are passed into the perceptron where they are summed, weighted by $w_1,\ldots,w_n$. This sum is then fed through the perceptron's activation function (here a binary step function) which is the output of the perceptron. Often the sum is implicitely considered part of the activation function." width="60%" />
<p class="caption">
Figure 7.1: <strong>Computational graph of a perceptron.</strong><br />
Here, <span class="math inline">\(n\)</span> inputs are passed into the perceptron where they are summed, weighted by <span class="math inline">\(w_1,\ldots,w_n\)</span>. This sum is then fed through the perceptron’s activation function (here a binary step function) which is the output of the perceptron. Often the sum is implicitely considered part of the activation function.
</p>
</div>
<p>While the perceptron could be useful, some other methods could solve more complex problems. However it was discovered that by linking several perceptrons together, as in a biological brain, some complex problems could also be solved. These structure, called <em>multilayer perceptrons</em> (MLP), are organized in layers, where the outputs of perceptrons on a layer are used as inputs by perceptrons is the next layer (c.f. Figure <a href="learning-alignments-an-interesting-perspective.html#fig:mlp">7.2</a>). The perceptrons, when in this form, are often called <em>neurons</em>, and the MLP a <em>neural network</em> (NN). These neural networks are organized in layers, with an input and output layer on either end, and hidden layers in the middle. With the large number of weights to tune, these models were very difficult to train and therefore not practically useful.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlp"></span>
<img src="figures/Learn-alignments/mlp.png" alt="**Computational graph of a multilayer perceptron.**  
This MLP, also called feedforward neural network, has $n$ inputs represented as the input layer, 2 hidden layers of 3 neurons each and an output layer of 2 neurons (e.g. suitable to binary classification). It is fully connected meaning that each node of a given layer is used as input for every neuron of the following layer. Each edge in this graph is paired to a weight, these are the tunable parameters during the training process." width="60%" />
<p class="caption">
Figure 7.2: <strong>Computational graph of a multilayer perceptron.</strong><br />
This MLP, also called feedforward neural network, has <span class="math inline">\(n\)</span> inputs represented as the input layer, 2 hidden layers of 3 neurons each and an output layer of 2 neurons (e.g. suitable to binary classification). It is fully connected meaning that each node of a given layer is used as input for every neuron of the following layer. Each edge in this graph is paired to a weight, these are the tunable parameters during the training process.
</p>
</div>
<p>There was a great resurgence of these models in the nineties due to the invention of <em>backpropagation</em><span class="citation"><sup><a href="#ref-rumelhartLearningRepresentationsBackpropagating1986" role="doc-biblioref">672</a></sup></span>. By replacing the step functions of neurons with continuous, differentiable activation functions like sigmoid or hyperbolic tangent functions. With backpropagation, a gradient of the output could be computed w.r.t to each weight, enabling gradient descent procedures for automatically learning the optimal weights from data as described in Section <a href="learning-from-sequences-and-alignments.html#supervised-learning">4.2.1</a>. With this, method, neural networks could be efficiently trained on complex classification and regression problems<span class="citation"><sup><a href="#ref-murtaghMultilayerPerceptronsClassification1991" role="doc-biblioref">673</a></sup></span>. It was also proven that with hidden layers, neural networks are universal function approximators<span class="citation"><sup><a href="#ref-cybenkoApproximationSuperpositionsSigmoidal1989" role="doc-biblioref">674</a>–<a href="#ref-hornikApproximationCapabilitiesMultilayer1991a" role="doc-biblioref">676</a></sup></span>, suitable for all types of tasks. One notable caveat for neural networks is that, due to the large amount of weights to tune, they require large amounts of training data, which also explained their low usage before the internet and resulting data collection.</p>
<p>In the following years, neural networks saw an explosion in usage, with more complex architectures like convolutional neural networks (CNN) achieving state of the art result in computer vision tasks<span class="citation"><sup><a href="#ref-lecunBackpropagationAppliedHandwritten1989" role="doc-biblioref">677</a>,<a href="#ref-lecunGradientbasedLearningApplied1998" role="doc-biblioref">678</a></sup></span>. By representing an input variable as a linear combination of its neighbors some form of contextual information can be passed to the NN and improve performance. CNNs can also have good results in non computer-vision tasks like: drug resistance prediction<span class="citation"><sup><a href="#ref-steinerDrugResistancePrediction2020a" role="doc-biblioref">329</a></sup></span>, protein subcellular localization<span class="citation"><sup><a href="#ref-weiPredictionHumanProtein2018" role="doc-biblioref">348</a></sup></span>, or epidemiological model parameter estimation<span class="citation"><sup><a href="#ref-voznicaDeepLearningPhylogenies2022" role="doc-biblioref">679</a></sup></span>.</p>
<p>More recently, as computational power and the amount of training data grew, larger and deeper (i.e. more hidden layers) architecture were able to be trained and achieved state of the art performance in many fields: image recognition with deep CNNs like <code>Alexnet</code><span class="citation"><sup><a href="#ref-krizhevskyImageNetClassificationDeep2017" role="doc-biblioref">680</a></sup></span> or <code>Resnet</code><span class="citation"><sup><a href="#ref-heDeepResidualLearning2016" role="doc-biblioref">681</a></sup></span>, translation with Recurrent NNs<span class="citation"><sup><a href="#ref-bahdanauNeuralMachineTranslation2016" role="doc-biblioref">682</a></sup></span> and Transformers<span class="citation"><sup><a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">683</a></sup></span> (more on that in Section <a href="learning-alignments-an-interesting-perspective.html#learned-sequence-embeddings">7.1.2</a>) or protein structure prediction with <code>Alphafold2</code><span class="citation"><sup><a href="#ref-jumperHighlyAccurateProtein2021" role="doc-biblioref">136</a></sup></span></p>
</div>
<div id="learned-sequence-embeddings" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Learned sequence embeddings<a href="learning-alignments-an-interesting-perspective.html#learned-sequence-embeddings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An area that in which deep learning has recently proved particularly useful is the creation relevant learned embeddings. These embeddings, similarly to the encodings discussed in Section <a href="learning-from-sequences-and-alignments.html#preprocessing-the-alignment-for-machine-learning">4.4</a>, transform a sequence of tokens in a numerical vector which can then be used in other machine learning tasks. By learning these embeddings, the hope is that the resulting vector will retain the most important information in the sequence and keep some context.</p>
<div id="x-2vec" class="section level4 hasAnchor" number="7.1.2.1">
<h4><span class="header-section-number">7.1.2.1</span> <span class="math inline">\(x\)</span>-<code>2vec</code><a href="learning-alignments-an-interesting-perspective.html#x-2vec" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Learned embeddings were principally developed in the field of natural language processing (NLP), where machine learning algorithms use text in languages such as English or French as input. In these contexts, simple encodings like OHE are not very practical because of the very high dimensionality of a language. For example, Merriam-Webster English dictionary contains 470,000 words<span class="citation"><sup><a href="#ref-HowManyWords" role="doc-biblioref">684</a></sup></span> so to encode a single word as a One-Hot encoded vector would result in a 470,000-dimensional sparse vector. Encoding a whole text or even a single sentence is wildly unpractical. Therefore, as a field, NLP needed to come up with ways of efficiently representing words in lower-dimensional vectors than naive encoding methods, while retaining semantic meaning.</p>
<p>One of the early methods for sequence embedding was called <code>word2vec</code><span class="citation"><sup><a href="#ref-mikolovEfficientEstimationWord2013" role="doc-biblioref">685</a>,<a href="#ref-mikolovDistributedRepresentationsWords2013" role="doc-biblioref">686</a></sup></span>, proposed by researchers at Google, that learns an word-embedding method on a particular text corpus. This method is designed to make embeddings that contain semantically relevant information, and example of the article is that the vector corresponding to <span class="math inline">\(vec(Madrid) - vec(Spain)\)</span> should be very similar to the vector <span class="math inline">\(vec(Paris)\)</span>, and that similar words result in similar vectors.</p>
<p>The way this method works is by considering a word within its context, i.e. a window of length <span class="math inline">\(k\)</span> centered around the word. In a corpus of words (i.e. our training data), each word is encoded as a One Hot Vector, which is possible since the corpus contains only a subset of the words in the English language. A neural is then trained on one one of two tasks<span class="citation"><sup><a href="#ref-goldbergWord2vecExplainedDeriving2014" role="doc-biblioref">687</a></sup></span>:</p>
<ul>
<li><p>Continuous Bag of words: where the word is predicted given the context of the word as input</p></li>
<li><p>Skip-gram: where the context is predicted given the encoded word vector</p></li>
</ul>
<p>After having sufficiently trained the neural network on the corpus on one of these tasks, one of the hidden layers of the network can be extracted and used as a vector representation of the input word, this results in an embedding method that is specific to a given corpus and the embedded vectors can be used in downstream learning tasks.</p>
<p><code>word2vec</code> was very successful and widely used in the field of NLP, it is perhaps no surprise that the ideas behind it were adapted and reused in the field of bioinformatics. <code>dna2vec</code><span class="citation"><sup><a href="#ref-ngDna2vecConsistentVector2017" role="doc-biblioref">688</a></sup></span> uses similar ideas and was used to embed <span class="math inline">\(k\)</span>-mers, and predict methylation sites on DNA sequences<span class="citation"><sup><a href="#ref-liangHyb4mCHybridDNA2vecbased2022" role="doc-biblioref">689</a></sup></span>. Similar embedding methods like <code>seq2vec</code><span class="citation"><sup><a href="#ref-kimothiDistributedRepresentationsBiological2016" role="doc-biblioref">690</a></sup></span> as well as <code>bioVec</code> (including the protein specific <code>protVec</code>)<span class="citation"><sup><a href="#ref-asgariContinuousDistributedRepresentation2015" role="doc-biblioref">691</a></sup></span> were also developed to embed whole biological sequences. They were successfully used in biological sequence classification problems<span class="citation"><sup><a href="#ref-kimothiMetricLearningBiological2017" role="doc-biblioref">692</a></sup></span>.</p>
</div>
<div id="the-attention-revolution" class="section level4 hasAnchor" number="7.1.2.2">
<h4><span class="header-section-number">7.1.2.2</span> The attention revolution<a href="learning-alignments-an-interesting-perspective.html#the-attention-revolution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>While <code>word2vec</code> was widely used for many NLP tasks where word embeddings were needed, a lot of interesting developments on word embeddings were made in the field of automated machine translation. In this application, the desired embedding characteristics are slightly different. While semantic relevance is useful, in machine translation the embedding method needs to be able to capture dependencies, e.g. within a sentence where the link between the subject and the verb must be captured even though they are not necessarily next to each other. This was initially done by using recurrent neural networks, but they were hard to train and had trouble properly capturing long-range dependencies<span class="citation"><sup><a href="#ref-songPretrainingModelBiological2021" role="doc-biblioref">693</a></sup></span>.</p>
<p>One of the most successful methods developed for this task it the transformer<span class="citation"><sup><a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">683</a></sup></span>, also created by Google researchers. The main mechanisms of the transformer is the <em>self-attention</em> mechanisms: each input token, usually encoded as a One-Hot vector, is represented as a weighted sum of all the other tokens in a sequence <em>(here a token is a word and the sequence is a sentence)</em>. The weights of this sum are trained along with the rest of this network. by stacking several of these self-attention blocks, transformers can learn to represent and leverage long-range dependencies. This mechanism, and the transformer in general have had very successful application in machine translation, while being easier to train than recurrent networks<span class="citation"><sup><a href="#ref-wangProgressMachineTranslation2021" role="doc-biblioref">694</a></sup></span>.</p>
<p>This architecture was used to create very large pre-trained language models, that is to say models that perform word embedding. These models like <code>GPT-3</code><span class="citation"><sup><a href="#ref-brownLanguageModelsAre2020" role="doc-biblioref">695</a></sup></span> or <code>BERT</code><span class="citation"><sup><a href="#ref-devlinBERTPretrainingDeep2019" role="doc-biblioref">696</a></sup></span> are huge, with millions or even billions of learned weights, and have been trained on huge quantities of data in order to produce word embeddings useful in a wide range of contexts. <code>BERT</code> was trained using Masked Language Modelling (MLM), where some percentage of the tokens <em>(words)</em> in an input sequence <em>(sentence)</em> are replaced by a special <code>[MASK]</code> token, and the model is trained to predict the whole sentence, effectively guessing what tokens are missing based on the context of the whole sequence. This process allows the model to learn relevant dependencies between tokens in the training data.</p>
<p>As was the case with <code>word2vec</code>, these methods have been adapted to bioinformatics tasks with state of the art results, proving the versatility of the transformer model. Several <em>protein language models</em> similar to <code>BERT</code> were trained on various training sets of protein data. Some of these are <code>ProGen</code><span class="citation"><sup><a href="#ref-madaniProGenLanguageModeling2020" role="doc-biblioref">697</a></sup></span>, <code>ProGen2</code><span class="citation"><sup><a href="#ref-eriknijkampProGen2ExploringBoundaries2022" role="doc-biblioref">698</a></sup></span> and <code>ProtBERT</code><span class="citation"><sup><a href="#ref-elnaggarProtTransCrackingLanguage2021" role="doc-biblioref">369</a></sup></span>. These large protein language models have been studied and interesting properties have been observed<span class="citation"><sup><a href="#ref-beplerLearningProteinLanguage2021" role="doc-biblioref">699</a></sup></span>, and some specific characteristics of proteins can be inferred from these models without specifying them in the training step. For example, protein language models seem to learn some information about the protein structure and attention maps can be used to infer residue contact maps<span class="citation"><sup><a href="#ref-raoTransformerProteinLanguage2020" role="doc-biblioref">700</a>–<a href="#ref-bhattacharyaSingleLayersAttention2020" role="doc-biblioref">702</a></sup></span>. Similarly these models capture some information about protein function<span class="citation"><sup><a href="#ref-huExploringEvolutionbasedFree2022" role="doc-biblioref">703</a></sup></span>, mutational effects<span class="citation"><sup><a href="#ref-meierLanguageModelsEnable2021" role="doc-biblioref">704</a></sup></span>, evolutionary characteristics<span class="citation"><sup><a href="#ref-hieEvolutionaryVelocityProtein2022" role="doc-biblioref">705</a></sup></span> and can even be used to generate new protein with desired properties<span class="citation"><sup><a href="#ref-madaniProGenLanguageModeling2020" role="doc-biblioref">697</a></sup></span>. Some large language models have also been trained on DNA sequences like <code>DNABert</code><span class="citation"><sup><a href="#ref-jiDNABERTPretrainedBidirectional2021" role="doc-biblioref">706</a></sup></span> and also seem to capture some information without explicit specification during training, like variant effects<span class="citation"><sup><a href="#ref-benegasDNALanguageModels2022" role="doc-biblioref">707</a></sup></span>.</p>
<p>While, these protein language models have shown very useful for embedding sequences, some developments have been made to embed multiple sequence alignments as learning inputs. In some cases this is done by including information on the alignment in the tokens and then using a regular language model to embed them<span class="citation"><sup><a href="#ref-caiGenomewidePredictionSmall2020" role="doc-biblioref">708</a></sup></span>. In the case of the MSA transformer<span class="citation"><sup><a href="#ref-raoMSATransformer2021" role="doc-biblioref">709</a></sup></span>, the attention mechanism was extended to include a weighted sum between aligned sequences effectively taking the alignment into account when embedding sequences. An attention-like mechanism was also used to train a protein structural model directly on MSAs<span class="citation"><sup><a href="#ref-sercuNeuralPottsModel2021" role="doc-biblioref">710</a></sup></span>. Similarly, by pre-training language models on profiles derived from MSAs, some information about the alignment can also be included in the resulting embeddings<span class="citation"><sup><a href="#ref-sturmfelsProfilePredictionAlignmentBased2020" role="doc-biblioref">711</a></sup></span>. Finally aligned sequences can be used as inputs in a regular transformer as was done <code>DeepConsensus</code><span class="citation"><sup><a href="#ref-baidDeepConsensusImprovesAccuracy2022" role="doc-biblioref">712</a></sup></span>, a transformer-based polisher to improve PacBio HiFi reads even further. Finally the EvoFormer model included in <code>AlphaFold2</code><span class="citation"><sup><a href="#ref-jumperHighlyAccurateProtein2021" role="doc-biblioref">136</a></sup></span>, which embeds MSAs to predict protein structure, is partly responsible for the leap in performance between the two generations of the <code>AlphaFold</code> model.</p>
<p>It is important to note that while these transformer models are very powerful and useful in practice, their complexity and size makes it very hard to study and understand what the model actually learns. There is work to peer inside this “black box”, notably by interpreting the learn attention maps<span class="citation"><sup><a href="#ref-vigBERTologyMeetsBiology2021" role="doc-biblioref">713</a></sup></span> to decipher biologically relevant information contained within.</p>
</div>
</div>
</div>
<div id="learning-sequence-alignment" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Learning sequence alignment<a href="learning-alignments-an-interesting-perspective.html#learning-sequence-alignment" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="predicting-a-substitution-matrix" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Predicting a substitution matrix<a href="learning-alignments-an-interesting-perspective.html#predicting-a-substitution-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One approach is to learn a substitution matrix (specific position/position scoring matrix) and plug it in a differentiable SW or NW algorithm (for end-to-end learning):</p>
<ul>
<li><p>SAdLSA<span class="citation"><sup><a href="#ref-gaoNovelSequenceAlignment2021" role="doc-biblioref">714</a></sup></span></p>
<ul>
<li><p>sequences are encoded as PSI blast profiles</p></li>
<li><p>Fed through a deep CNN</p></li>
<li><p>predict a scoring matrix</p></li>
<li><p>No differentiable alignment algorithm -&gt; cross entropy between alignment and structural alignment.</p></li>
</ul></li>
<li><p>DeepBLAST<span class="citation"><sup><a href="#ref-mortonProteinStructuralAlignments2020" role="doc-biblioref">715</a></sup></span></p>
<ul>
<li><p>Embed sequences with LSTM-based language model (trained on PFAM)</p></li>
<li><p>predict substitution/gap score</p></li>
<li><p>differentiable NW (not to learn parameters but only to backpropagate the error)</p></li>
</ul></li>
<li><p>DEDAL<span class="citation"><sup><a href="#ref-llinares-lopezDeepEmbeddingAlignment2022" role="doc-biblioref">716</a></sup></span></p>
<ul>
<li><p>2 seqs are embedded with encoder-only transformer</p></li>
<li><p>Trained on TPUs with a fast differentiable algo (SW ?)</p></li>
<li><p>Training set: parwise alignments extracted from PFAM</p></li>
<li><p>predicts substitution, gap open and extend scoring matrices (position per position)</p></li>
<li><p>improves alignment for remote homologies</p></li>
</ul></li>
<li><p>The Learned Alignement module<span class="citation"><sup><a href="#ref-pettiEndtoendLearningMultiple2022" role="doc-biblioref">717</a></sup></span></p>
<ul>
<li><p>Learns a “context specific scoring matrix”, i.e. a 20x20 matrix for a window around a given position.</p></li>
<li><p>Differentiable SW</p></li>
<li><p>Uses convolutional NN</p></li>
<li><p>Learns an “MSA”, actually outputs all to one pairwise alignments.</p></li>
<li><p>Used to as plugin to alphafold2 and improved some metrics</p></li>
</ul></li>
<li><p>Prediction of PSSM with RNN + LSTM<span class="citation"><sup><a href="#ref-guoComprehensiveStudyEnhancing2021" role="doc-biblioref">718</a></sup></span> , not directly used on alignment but structure prediction.</p></li>
</ul>
</div>
<div id="predicting-an-alignment" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> predicting an alignment<a href="learning-alignments-an-interesting-perspective.html#predicting-an-alignment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>BetaAlign<span class="citation"><sup><a href="#ref-dotanHarnessingMachineTranslation2022" role="doc-biblioref">719</a></sup></span></p>
<ul>
<li><p>Unaligned sequences = a “language”</p></li>
<li><p>Aligned sequences = another “language”</p></li>
<li><p>So use transformers to translate one into the other (tried several ways to represent these “languages”)</p></li>
<li><p>Tested with both DNA and protein</p></li>
<li><p>limitations:</p>
<ul>
<li><p>length of sequences</p></li>
<li><p>Training / testing set</p></li>
</ul></li>
</ul></li>
<li><p>Another direction could be to predict the state from 2 residues (like a PSSM but directly match/indel).</p></li>
</ul>
<!-- -->
<ul>
<li><p>To counter the space limitations (i.e. sequence length limitations) induced by attention, other types of transformers used:</p>
<ul>
<li><p>with linear scale attention maps not quadratic<span class="citation"><sup><a href="#ref-choromanskiMaskedLanguageModeling2020" role="doc-biblioref">720</a></sup></span></p></li>
<li><p>single layer attention or factored attention<span class="citation"><sup><a href="#ref-bhattacharyaInterpretingPottsTransformer2021" role="doc-biblioref">721</a></sup></span> which lowers the number of parameters to estimate but keeps relevant information.</p></li>
</ul></li>
<li><p>This is not a problem limited to bioinformatics, other fields have tried to come up with solutions:</p>
<ul>
<li><p>adaptive attention span<span class="citation"><sup><a href="#ref-sukhbaatarAdaptiveAttentionSpan2019" role="doc-biblioref">722</a></sup></span></p></li>
<li><p>Long-Short range attention<span class="citation"><sup><a href="#ref-wuLiteTransformerLongShort2020" role="doc-biblioref">723</a></sup></span></p></li>
<li><p>sparse transformers<span class="citation"><sup><a href="#ref-childGeneratingLongSequences2019" role="doc-biblioref">724</a>,<a href="#ref-correiaAdaptivelySparseTransformers2019" role="doc-biblioref">725</a></sup></span></p></li>
<li><p>Reformer replace dot product to reduce memory from quadratic to linear<span class="citation"><sup><a href="#ref-kitaevReformerEfficientTransformer2020" role="doc-biblioref">726</a></sup></span></p></li>
</ul></li>
</ul>
<p>For learning DNA alignment several challenges:</p>
<ul>
<li><p>longer sequences</p></li>
<li><p>less information in a single residue than a single nucleotide</p></li>
<li><p>In mapping size discrepancy between sequences.</p></li>
</ul>
</div>
<div id="learning-seeds" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Learning seeds<a href="learning-alignments-an-interesting-perspective.html#learning-seeds" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Some work has been done in learning seeding procedures</p>
<ul>
<li><p>Learn index structures on a specific reference (not necessarily DL), although fairly recent developments already thought of in 2018<span class="citation"><sup><a href="#ref-kraskaCaseLearnedIndex2018" role="doc-biblioref">727</a></sup></span>:</p>
<ul>
<li><p>BWA-MEME<span class="citation"><sup><a href="#ref-jungBWAMEMEBWAMEMEmulated2022" role="doc-biblioref">728</a></sup></span> predicts the position in a suffix array, lowering query time and no need to compute the whole suffix array</p></li>
<li><p>Sapling<span class="citation"><sup><a href="#ref-kirscheSaplingAcceleratingSuffix2021" role="doc-biblioref">729</a></sup></span> same thing</p></li>
<li><p>LISA<span class="citation"><sup><a href="#ref-hoLISALearnedIndexes2021" role="doc-biblioref">730</a></sup></span> predict position in FM-index</p></li>
</ul></li>
</ul>
<!-- -->
<ul>
<li><p>DeepMinimizer<span class="citation"><sup><a href="#ref-hoangDifferentiableLearningSequenceSpecific2022" role="doc-biblioref">731</a></sup></span>:</p>
<ul>
<li><p>Train a neural network to select minimizers</p></li>
<li><p>Results in a better density, seeds are spread out evenly accross sequences</p></li>
</ul></li>
<li><p>Select candidate alignment sites in mRNA-miRNA pairs with DL: TargetNet<span class="citation"><sup><a href="#ref-minTargetNetFunctionalMicroRNA2022" role="doc-biblioref">732</a></sup></span></p></li>
</ul>
<p>Final note, we could also learn a pre-processing function as in Chapter <a href="HPC-paper.html#HPC-paper">3</a> in an end to end fashion: either by learning the connections in MSRs or by learning transformations with sequence to sequence models (like transformers). This is still a little abstract and would need a differentiable read mapping algo for end-to-end learning (SW is possible but seeding -&gt; deepminimizer ?).</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" line-spacing="2">
<div id="ref-jumperHighlyAccurateProtein2021" class="csl-entry">
<div class="csl-left-margin">136. </div><div class="csl-right-inline">Jumper, J. <em>et al.</em> <a href="https://doi.org/10.1038/s41586-021-03819-2">Highly accurate protein structure prediction with AlphaFold</a>. <em>Nature</em> <strong>596</strong>, 583–589 (2021).</div>
</div>
<div id="ref-steinerDrugResistancePrediction2020a" class="csl-entry">
<div class="csl-left-margin">329. </div><div class="csl-right-inline">Steiner, M. C., Gibson, K. M. &amp; Crandall, K. A. <a href="https://doi.org/10.3390/v12050560">Drug <span>Resistance Prediction Using Deep Learning Techniques</span> on <span>HIV</span>-1 <span>Sequence Data</span></a>. <em>Viruses</em> <strong>12</strong>, 560 (2020).</div>
</div>
<div id="ref-weiPredictionHumanProtein2018" class="csl-entry">
<div class="csl-left-margin">348. </div><div class="csl-right-inline">Wei, L., Ding, Y., Su, R., Tang, J. &amp; Zou, Q. <a href="https://doi.org/10.1016/j.jpdc.2017.08.009">Prediction of human protein subcellular localization using deep learning</a>. <em>Journal of Parallel and Distributed Computing</em> <strong>117</strong>, 212–217 (2018).</div>
</div>
<div id="ref-elnaggarProtTransCrackingLanguage2021" class="csl-entry">
<div class="csl-left-margin">369. </div><div class="csl-right-inline">Elnaggar, A. <em>et al.</em> ProtTrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing. doi:<a href="https://doi.org/10.48550/arXiv.2007.06225">10.48550/arXiv.2007.06225</a>.</div>
</div>
<div id="ref-rosenblattPerceptronProbabilisticModel1958" class="csl-entry">
<div class="csl-left-margin">671. </div><div class="csl-right-inline">Rosenblatt, F. <a href="https://doi.org/10.1037/h0042519">The perceptron: A probabilistic model for information storage and organization in the brain</a>. <em>Psychological Review</em> <strong>65</strong>, 386–408 (1958).</div>
</div>
<div id="ref-rumelhartLearningRepresentationsBackpropagating1986" class="csl-entry">
<div class="csl-left-margin">672. </div><div class="csl-right-inline">Rumelhart, D. E., Hinton, G. E. &amp; Williams, R. J. <a href="https://doi.org/10.1038/323533a0">Learning representations by back-propagating errors</a>. <em>Nature</em> <strong>323</strong>, 533–536 (1986).</div>
</div>
<div id="ref-murtaghMultilayerPerceptronsClassification1991" class="csl-entry">
<div class="csl-left-margin">673. </div><div class="csl-right-inline">Murtagh, F. <a href="https://doi.org/10.1016/0925-2312(91)90023-5">Multilayer perceptrons for classification and regression</a>. <em>Neurocomputing</em> <strong>2</strong>, 183–197 (1991).</div>
</div>
<div id="ref-cybenkoApproximationSuperpositionsSigmoidal1989" class="csl-entry">
<div class="csl-left-margin">674. </div><div class="csl-right-inline">Cybenko, G. <a href="https://doi.org/10.1007/BF02551274">Approximation by superpositions of a sigmoidal function</a>. <em>Mathematics of Control, Signals and Systems</em> <strong>2</strong>, 303–314 (1989).</div>
</div>
<div id="ref-hornikApproximationCapabilitiesMultilayer1991a" class="csl-entry">
<div class="csl-left-margin">676. </div><div class="csl-right-inline">Hornik, K. <a href="https://doi.org/10.1016/0893-6080(91)90009-T">Approximation capabilities of multilayer feedforward networks</a>. <em>Neural Networks</em> <strong>4</strong>, 251–257 (1991).</div>
</div>
<div id="ref-lecunBackpropagationAppliedHandwritten1989" class="csl-entry">
<div class="csl-left-margin">677. </div><div class="csl-right-inline">LeCun, Y. <em>et al.</em> <a href="https://doi.org/10.1162/neco.1989.1.4.541">Backpropagation applied to handwritten zip code recognition</a>. <em>Neural Computation</em> <strong>1</strong>, 541–551 (1989).</div>
</div>
<div id="ref-lecunGradientbasedLearningApplied1998" class="csl-entry">
<div class="csl-left-margin">678. </div><div class="csl-right-inline">Lecun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. <a href="https://doi.org/10.1109/5.726791">Gradient-based learning applied to document recognition</a>. <em>Proceedings of the IEEE</em> <strong>86</strong>, 2278–2324 (1998).</div>
</div>
<div id="ref-voznicaDeepLearningPhylogenies2022" class="csl-entry">
<div class="csl-left-margin">679. </div><div class="csl-right-inline">Voznica, J. <em>et al.</em> <a href="https://doi.org/10.1038/s41467-022-31511-0">Deep learning from phylogenies to uncover the epidemiological dynamics of outbreaks</a>. <em>Nature Communications</em> <strong>13</strong>, 3896 (2022).</div>
</div>
<div id="ref-krizhevskyImageNetClassificationDeep2017" class="csl-entry">
<div class="csl-left-margin">680. </div><div class="csl-right-inline">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. <a href="https://doi.org/10.1145/3065386">ImageNet classification with deep convolutional neural networks</a>. <em>Communications of the ACM</em> <strong>60</strong>, 8490 (2017).</div>
</div>
<div id="ref-heDeepResidualLearning2016" class="csl-entry">
<div class="csl-left-margin">681. </div><div class="csl-right-inline">He, K., Zhang, X., Ren, S. &amp; Sun, J. <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Proceedings of the IEEE conference on computer vision and pattern recognition</a>. in 770–778 (2016).</div>
</div>
<div id="ref-bahdanauNeuralMachineTranslation2016" class="csl-entry">
<div class="csl-left-margin">682. </div><div class="csl-right-inline">Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. doi:<a href="https://doi.org/10.48550/arXiv.1409.0473">10.48550/arXiv.1409.0473</a>.</div>
</div>
<div id="ref-vaswaniAttentionAllYou2017" class="csl-entry">
<div class="csl-left-margin">683. </div><div class="csl-right-inline">Vaswani, A. <em>et al.</em> Attention is all you need. in vol. 30 (Curran Associates, Inc., 2017).</div>
</div>
<div id="ref-HowManyWords" class="csl-entry">
<div class="csl-left-margin">684. </div><div class="csl-right-inline"><a href="https://www.merriam-webster.com/help/faq-how-many-english-words">How many words are there in english? | merriam-webster</a>.</div>
</div>
<div id="ref-mikolovEfficientEstimationWord2013" class="csl-entry">
<div class="csl-left-margin">685. </div><div class="csl-right-inline">Mikolov, T., Chen, K., Corrado, G. &amp; Dean, J. Efficient estimation of word representations in vector space. doi:<a href="https://doi.org/10.48550/arXiv.1301.3781">10.48550/arXiv.1301.3781</a>.</div>
</div>
<div id="ref-mikolovDistributedRepresentationsWords2013" class="csl-entry">
<div class="csl-left-margin">686. </div><div class="csl-right-inline">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. &amp; Dean, J. Distributed representations of words and phrases and their compositionality. in vol. 26 (Curran Associates, Inc., 2013).</div>
</div>
<div id="ref-goldbergWord2vecExplainedDeriving2014" class="csl-entry">
<div class="csl-left-margin">687. </div><div class="csl-right-inline">Goldberg, Y. &amp; Levy, O. word2vec explained: Deriving mikolov et al.’s negative-sampling word-embedding method. doi:<a href="https://doi.org/10.48550/arXiv.1402.3722">10.48550/arXiv.1402.3722</a>.</div>
</div>
<div id="ref-ngDna2vecConsistentVector2017" class="csl-entry">
<div class="csl-left-margin">688. </div><div class="csl-right-inline">Ng, P. dna2vec: Consistent vector representations of variable-length k-mers. doi:<a href="https://doi.org/10.48550/arXiv.1701.06279">10.48550/arXiv.1701.06279</a>.</div>
</div>
<div id="ref-liangHyb4mCHybridDNA2vecbased2022" class="csl-entry">
<div class="csl-left-margin">689. </div><div class="csl-right-inline">Liang, Y. <em>et al.</em> <a href="https://doi.org/10.1186/s12859-022-04789-6">Hyb4mC: a hybrid DNA2vec-based model for DNA N4-methylcytosine sites prediction</a>. <em>BMC Bioinformatics</em> <strong>23</strong>, 258 (2022).</div>
</div>
<div id="ref-kimothiDistributedRepresentationsBiological2016" class="csl-entry">
<div class="csl-left-margin">690. </div><div class="csl-right-inline">Kimothi, D., Soni, A., Biyani, P. &amp; Hogan, J. M. Distributed representations for biological sequence analysis. doi:<a href="https://doi.org/10.48550/arXiv.1608.05949">10.48550/arXiv.1608.05949</a>.</div>
</div>
<div id="ref-asgariContinuousDistributedRepresentation2015" class="csl-entry">
<div class="csl-left-margin">691. </div><div class="csl-right-inline">Asgari, E. &amp; Mofrad, M. R. K. <a href="https://doi.org/10.1371/journal.pone.0141287">Continuous distributed representation of biological sequences for deep proteomics and genomics</a>. <em>PLoS ONE</em> <strong>10</strong>, e0141287 (2015).</div>
</div>
<div id="ref-kimothiMetricLearningBiological2017" class="csl-entry">
<div class="csl-left-margin">692. </div><div class="csl-right-inline">Kimothi, D., Shukla, A., Biyani, P., Anand, S. &amp; Hogan, J. M. 2017 IEEE 18th international workshop on signal processing advances in wireless communications (SPAWC). in 1–5 (2017). doi:<a href="https://doi.org/10.1109/SPAWC.2017.8227769">10.1109/SPAWC.2017.8227769</a>.</div>
</div>
<div id="ref-songPretrainingModelBiological2021" class="csl-entry">
<div class="csl-left-margin">693. </div><div class="csl-right-inline">Song, B. <em>et al.</em> <a href="https://doi.org/10.1093/bfgp/elab025">Pretraining model for biological sequence data.</a> <em>Briefings in Functional Genomics</em> <strong>20</strong>, 181–195 (2021).</div>
</div>
<div id="ref-wangProgressMachineTranslation2021" class="csl-entry">
<div class="csl-left-margin">694. </div><div class="csl-right-inline">Wang, H., Wu, H., He, Z., Huang, L. &amp; Ward Church, K. Progress in Machine Translation. <em>Engineering</em> (2021) doi:<a href="https://doi.org/10.1016/j.eng.2021.03.023">10.1016/j.eng.2021.03.023</a>.</div>
</div>
<div id="ref-brownLanguageModelsAre2020" class="csl-entry">
<div class="csl-left-margin">695. </div><div class="csl-right-inline">Brown, T. <em>et al.</em> Language models are few-shot learners. in vol. 33 18771901 (Curran Associates, Inc., 2020).</div>
</div>
<div id="ref-devlinBERTPretrainingDeep2019" class="csl-entry">
<div class="csl-left-margin">696. </div><div class="csl-right-inline">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. doi:<a href="https://doi.org/10.48550/arXiv.1810.04805">10.48550/arXiv.1810.04805</a>.</div>
</div>
<div id="ref-madaniProGenLanguageModeling2020" class="csl-entry">
<div class="csl-left-margin">697. </div><div class="csl-right-inline">Madani, A. <em>et al.</em> ProGen: Language modeling for protein generation. <em>bioRxiv</em> (2020) doi:<a href="https://doi.org/10.1101/2020.03.07.982272">10.1101/2020.03.07.982272</a>.</div>
</div>
<div id="ref-eriknijkampProGen2ExploringBoundaries2022" class="csl-entry">
<div class="csl-left-margin">698. </div><div class="csl-right-inline">Erik Nijkamp, Jeffrey A. Ruffolo, Eli N. Weinstein, Nikhil Naik &amp; Ali Madani. ProGen2: Exploring the boundaries of protein language models. <em>ArXiv</em> (2022) doi:<a href="https://doi.org/10.48550/arxiv.2206.13517">10.48550/arxiv.2206.13517</a>.</div>
</div>
<div id="ref-beplerLearningProteinLanguage2021" class="csl-entry">
<div class="csl-left-margin">699. </div><div class="csl-right-inline">Bepler, T. &amp; Berger, B. <a href="https://doi.org/10.1016/j.cels.2021.05.017">Learning the protein language: Evolution, structure, and function.</a> <em>Cell systems</em> <strong>12</strong>, (2021).</div>
</div>
<div id="ref-raoTransformerProteinLanguage2020" class="csl-entry">
<div class="csl-left-margin">700. </div><div class="csl-right-inline">Rao, R., Meier, J., Sercu, T., Ovchinnikov, S. &amp; Rives, A. Transformer protein language models are unsupervised structure learners. doi:<a href="https://doi.org/10.1101/2020.12.15.422761">10.1101/2020.12.15.422761</a>.</div>
</div>
<div id="ref-bhattacharyaSingleLayersAttention2020" class="csl-entry">
<div class="csl-left-margin">702. </div><div class="csl-right-inline">Bhattacharya, N. <em>et al.</em> Single Layers of Attention Suffice to Predict Protein Contacts. doi:<a href="https://doi.org/10.1101/2020.12.21.423882">10.1101/2020.12.21.423882</a>.</div>
</div>
<div id="ref-huExploringEvolutionbasedFree2022" class="csl-entry">
<div class="csl-left-margin">703. </div><div class="csl-right-inline">Hu, M. <em>et al.</em> Exploring evolution-based &amp; -free protein language models as protein function predictors. doi:<a href="https://doi.org/10.48550/arXiv.2206.06583">10.48550/arXiv.2206.06583</a>.</div>
</div>
<div id="ref-meierLanguageModelsEnable2021" class="csl-entry">
<div class="csl-left-margin">704. </div><div class="csl-right-inline">Meier, J. <em>et al.</em> <a href="https://doi.org/10.1101/2021.07.09.450648">Language models enable zero-shot prediction of the effects of mutations on protein function</a>. <em>bioRxiv</em> <strong>34</strong>, (2021).</div>
</div>
<div id="ref-hieEvolutionaryVelocityProtein2022" class="csl-entry">
<div class="csl-left-margin">705. </div><div class="csl-right-inline">Hie, B., Kevin K Yang &amp; Kim, S. K. Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins. <em>Cell systems</em> (2022) doi:<a href="https://doi.org/10.1016/j.cels.2022.01.003">10.1016/j.cels.2022.01.003</a>.</div>
</div>
<div id="ref-jiDNABERTPretrainedBidirectional2021" class="csl-entry">
<div class="csl-left-margin">706. </div><div class="csl-right-inline">Ji, Y., Zhou, Z., Liu, H. &amp; Davuluri, R. V. <a href="https://doi.org/10.1093/bioinformatics/btab083">DNABERT: Pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</a>. <em>Bioinformatics</em> <strong>37</strong>, 2112–2120 (2021).</div>
</div>
<div id="ref-benegasDNALanguageModels2022" class="csl-entry">
<div class="csl-left-margin">707. </div><div class="csl-right-inline">Benegas, G., Batra, S. S. &amp; Song, Y. S. DNA language models are powerful zero-shot predictors of non-coding variant effects. doi:<a href="https://doi.org/10.1101/2022.08.22.504706">10.1101/2022.08.22.504706</a>.</div>
</div>
<div id="ref-caiGenomewidePredictionSmall2020" class="csl-entry">
<div class="csl-left-margin">708. </div><div class="csl-right-inline">Cai, T. <em>et al.</em> Genome-wide Prediction of Small Molecule Binding to Remote Orphan Proteins Using Distilled Sequence Alignment Embedding. doi:<a href="https://doi.org/10.1101/2020.08.04.236729">10.1101/2020.08.04.236729</a>.</div>
</div>
<div id="ref-raoMSATransformer2021" class="csl-entry">
<div class="csl-left-margin">709. </div><div class="csl-right-inline">Rao, R. <em>et al.</em> MSA transformer. <em>bioRxiv</em> (2021) doi:<a href="https://doi.org/10.1101/2021.02.12.430858">10.1101/2021.02.12.430858</a>.</div>
</div>
<div id="ref-sercuNeuralPottsModel2021" class="csl-entry">
<div class="csl-left-margin">710. </div><div class="csl-right-inline">Sercu, T. <em>et al.</em> Neural Potts Model. doi:<a href="https://doi.org/10.1101/2021.04.08.439084">10.1101/2021.04.08.439084</a>.</div>
</div>
<div id="ref-sturmfelsProfilePredictionAlignmentBased2020" class="csl-entry">
<div class="csl-left-margin">711. </div><div class="csl-right-inline">Sturmfels, P., Vig, J., Madani, A. &amp; Rajani, N. F. Profile prediction: An alignment-based pre-training task for protein sequence models. doi:<a href="https://doi.org/10.48550/arXiv.2012.00195">10.48550/arXiv.2012.00195</a>.</div>
</div>
<div id="ref-baidDeepConsensusImprovesAccuracy2022" class="csl-entry">
<div class="csl-left-margin">712. </div><div class="csl-right-inline">Baid, G. <em>et al.</em> DeepConsensus improves the accuracy of sequences with a gap-aware sequence transformer. <em>Nature Biotechnology</em> 1–7 (2022) doi:<a href="https://doi.org/10.1038/s41587-022-01435-7">10.1038/s41587-022-01435-7</a>.</div>
</div>
<div id="ref-vigBERTologyMeetsBiology2021" class="csl-entry">
<div class="csl-left-margin">713. </div><div class="csl-right-inline">Vig, J. <em>et al.</em> BERTology meets biology: Interpreting attention in protein language models. doi:<a href="https://doi.org/10.48550/arXiv.2006.15222">10.48550/arXiv.2006.15222</a>.</div>
</div>
<div id="ref-gaoNovelSequenceAlignment2021" class="csl-entry">
<div class="csl-left-margin">714. </div><div class="csl-right-inline">Gao, M. &amp; Skolnick, J. <a href="https://doi.org/10.1093/bioinformatics/btaa810">A novel sequence alignment algorithm based on deep learning of the protein folding code</a>. <em>Bioinformatics</em> <strong>37</strong>, 490–496 (2021).</div>
</div>
<div id="ref-mortonProteinStructuralAlignments2020" class="csl-entry">
<div class="csl-left-margin">715. </div><div class="csl-right-inline">Morton, J. T. <em>et al.</em> Protein Structural Alignments From Sequence. doi:<a href="https://doi.org/10.1101/2020.11.03.365932">10.1101/2020.11.03.365932</a>.</div>
</div>
<div id="ref-llinares-lopezDeepEmbeddingAlignment2022" class="csl-entry">
<div class="csl-left-margin">716. </div><div class="csl-right-inline">Llinares-López, F., Berthet, Q., Blondel, M., Teboul, O. &amp; Vert, J.-P. Deep embedding and alignment of protein sequences. doi:<a href="https://doi.org/10.1101/2021.11.15.468653">10.1101/2021.11.15.468653</a>.</div>
</div>
<div id="ref-pettiEndtoendLearningMultiple2022" class="csl-entry">
<div class="csl-left-margin">717. </div><div class="csl-right-inline">Petti, S. <em>et al.</em> End-to-end learning of multiple sequence alignments with differentiable Smith-Waterman. doi:<a href="https://doi.org/10.1101/2021.10.23.465204">10.1101/2021.10.23.465204</a>.</div>
</div>
<div id="ref-guoComprehensiveStudyEnhancing2021" class="csl-entry">
<div class="csl-left-margin">718. </div><div class="csl-right-inline">Guo, Y., Wu, J., Ma, H., Wang, S. &amp; Huang, J. <a href="https://doi.org/10.1089/cmb.2020.0416">Comprehensive study on enhancing low-quality position-specific scoring matrix with deep learning for accurate protein structure property prediction: Using bagging multiple sequence alignment learning</a>. <em>Journal of Computational Biology</em> <strong>28</strong>, 346–361 (2021).</div>
</div>
<div id="ref-dotanHarnessingMachineTranslation2022" class="csl-entry">
<div class="csl-left-margin">719. </div><div class="csl-right-inline">Dotan, E. <em>et al.</em> Harnessing machine translation methods for sequence alignment. doi:<a href="https://doi.org/10.1101/2022.07.22.501063">10.1101/2022.07.22.501063</a>.</div>
</div>
<div id="ref-choromanskiMaskedLanguageModeling2020" class="csl-entry">
<div class="csl-left-margin">720. </div><div class="csl-right-inline">Choromanski, K. <em>et al.</em> Masked language modeling for proteins via linearly scalable long-context transformers. doi:<a href="https://doi.org/10.48550/arXiv.2006.03555">10.48550/arXiv.2006.03555</a>.</div>
</div>
<div id="ref-bhattacharyaInterpretingPottsTransformer2021" class="csl-entry">
<div class="csl-left-margin">721. </div><div class="csl-right-inline">Bhattacharya, N. <em>et al.</em> Interpreting potts and transformer protein models through the lens of simplified attention. in 34–45 (WORLD SCIENTIFIC, 2021). doi:<a href="https://doi.org/10.1142/9789811250477_0004">10.1142/9789811250477_0004</a>.</div>
</div>
<div id="ref-sukhbaatarAdaptiveAttentionSpan2019" class="csl-entry">
<div class="csl-left-margin">722. </div><div class="csl-right-inline">Sukhbaatar, S., Grave, E., Bojanowski, P. &amp; Joulin, A. Adaptive attention span in transformers. doi:<a href="https://doi.org/10.48550/arXiv.1905.07799">10.48550/arXiv.1905.07799</a>.</div>
</div>
<div id="ref-wuLiteTransformerLongShort2020" class="csl-entry">
<div class="csl-left-margin">723. </div><div class="csl-right-inline">Wu, Z., Liu, Z., Lin, J., Lin, Y. &amp; Han, S. Lite transformer with long-short range attention. doi:<a href="https://doi.org/10.48550/arXiv.2004.11886">10.48550/arXiv.2004.11886</a>.</div>
</div>
<div id="ref-childGeneratingLongSequences2019" class="csl-entry">
<div class="csl-left-margin">724. </div><div class="csl-right-inline">Child, R., Gray, S., Radford, A. &amp; Sutskever, I. Generating long sequences with sparse transformers. doi:<a href="https://doi.org/10.48550/arXiv.1904.10509">10.48550/arXiv.1904.10509</a>.</div>
</div>
<div id="ref-correiaAdaptivelySparseTransformers2019" class="csl-entry">
<div class="csl-left-margin">725. </div><div class="csl-right-inline">Correia, G. M., Niculae, V. &amp; Martins, A. F. T. EMNLP-IJCNLP 2019. in 21742184 (Association for Computational Linguistics, 2019). doi:<a href="https://doi.org/10.18653/v1/D19-1223">10.18653/v1/D19-1223</a>.</div>
</div>
<div id="ref-kitaevReformerEfficientTransformer2020" class="csl-entry">
<div class="csl-left-margin">726. </div><div class="csl-right-inline">Kitaev, N., Kaiser, Ł. &amp; Levskaya, A. Reformer: The efficient transformer. doi:<a href="https://doi.org/10.48550/arXiv.2001.04451">10.48550/arXiv.2001.04451</a>.</div>
</div>
<div id="ref-kraskaCaseLearnedIndex2018" class="csl-entry">
<div class="csl-left-margin">727. </div><div class="csl-right-inline">Kraska, T., Beutel, A., Chi, E. H., Dean, J. &amp; Polyzotis, N. The case for learned index structures. in 489504 (Association for Computing Machinery, 2018). doi:<a href="https://doi.org/10.1145/3183713.3196909">10.1145/3183713.3196909</a>.</div>
</div>
<div id="ref-jungBWAMEMEBWAMEMEmulated2022" class="csl-entry">
<div class="csl-left-margin">728. </div><div class="csl-right-inline">Jung, Y. &amp; Han, D. <a href="https://doi.org/10.1093/bioinformatics/btac137">BWA-MEME: BWA-MEM emulated with a machine learning approach</a>. <em>Bioinformatics</em> <strong>38</strong>, 2404–2413 (2022).</div>
</div>
<div id="ref-kirscheSaplingAcceleratingSuffix2021" class="csl-entry">
<div class="csl-left-margin">729. </div><div class="csl-right-inline">Kirsche, M., Das, A. &amp; Schatz, M. C. <a href="https://doi.org/10.1093/bioinformatics/btaa911">Sapling: Accelerating suffix array queries with learned data models</a>. <em>Bioinformatics</em> <strong>37</strong>, 744–749 (2021).</div>
</div>
<div id="ref-hoLISALearnedIndexes2021" class="csl-entry">
<div class="csl-left-margin">730. </div><div class="csl-right-inline">Ho, D. <em>et al.</em> LISA: Learned Indexes for Sequence Analysis. doi:<a href="https://doi.org/10.1101/2020.12.22.423964">10.1101/2020.12.22.423964</a>.</div>
</div>
<div id="ref-hoangDifferentiableLearningSequenceSpecific2022" class="csl-entry">
<div class="csl-left-margin">731. </div><div class="csl-right-inline">Hoang, M., Zheng, H. &amp; Kingsford, C. Differentiable learning of sequence-specific minimizer schemes with DeepMinimizer. <em>Journal of Computational Biology</em> (2022) doi:<a href="https://doi.org/10.1089/cmb.2022.0275">10.1089/cmb.2022.0275</a>.</div>
</div>
<div id="ref-minTargetNetFunctionalMicroRNA2022" class="csl-entry">
<div class="csl-left-margin">732. </div><div class="csl-right-inline">Min, S., Lee, B. &amp; Yoon, S. <a href="https://doi.org/10.1093/bioinformatics/btab733">TargetNet: Functional microRNA target prediction with deep neural networks</a>. <em>Bioinformatics</em> <strong>38</strong>, 671–677 (2022).</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="HIV-paper.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="global-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lucblassel/phd-manuscript/edit/main/07-learning-alignments.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

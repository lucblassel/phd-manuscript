<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Learning from sequences and alignments | From sequences to knowledge, improving and learning from sequence alignments</title>
  <meta name="description" content="Chapter 4 Learning from sequences and alignments | From sequences to knowledge, improving and learning from sequence alignments" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Learning from sequences and alignments | From sequences to knowledge, improving and learning from sequence alignments" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="lucblassel/phd-manuscript" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Learning from sequences and alignments | From sequences to knowledge, improving and learning from sequence alignments" />
  
  
  

<meta name="author" content="Luc Blassel" />


<meta name="date" content="2022-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="HPC-paper.html"/>
<link rel="next" href="viruses-hiv-and-drug-resistance.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<link href="libs/tabwid-1.0.0/scrool.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">From sequences to knowledge,</br> improving and learning from sequence alignments.</a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#résumé"><i class="fa fa-check"></i>Résumé</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="list-of-acronyms-and-abbreviations.html"><a href="list-of-acronyms-and-abbreviations.html"><i class="fa fa-check"></i>List of Acronyms and abbreviations</a></li>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i>General Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#organization-of-this-manuscript"><i class="fa fa-check"></i>Organization of this manuscript</a></li>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#research-output"><i class="fa fa-check"></i>Research output</a>
<ul>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#journal-publications"><i class="fa fa-check"></i>Journal publications</a></li>
<li class="chapter" data-level="" data-path="general-introduction.html"><a href="general-introduction.html#presentations-and-posters"><i class="fa fa-check"></i>Presentations and posters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html"><i class="fa fa-check"></i><b>1</b> What is Sequence data ?</a>
<ul>
<li class="chapter" data-level="1.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#biological-sequences-a-primer"><i class="fa fa-check"></i><b>1.1</b> Biological sequences, a primer</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#what-is-dna"><i class="fa fa-check"></i><b>1.1.1</b> What is DNA ?</a></li>
<li class="chapter" data-level="1.1.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#from-information-to-action"><i class="fa fa-check"></i><b>1.1.2</b> From Information to action</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#obtaining-sequence-data"><i class="fa fa-check"></i><b>1.2</b> Obtaining sequence data</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#sanger-sequencing-a-breakthrough"><i class="fa fa-check"></i><b>1.2.1</b> Sanger sequencing, a breakthrough</a></li>
<li class="chapter" data-level="1.2.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#next-generation-sequencing"><i class="fa fa-check"></i><b>1.2.2</b> Next-generation sequencing</a></li>
<li class="chapter" data-level="1.2.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#long-read-sequencing"><i class="fa fa-check"></i><b>1.2.3</b> Long read sequencing</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#sequencing-errors-how-to-account-for-them"><i class="fa fa-check"></i><b>1.3</b> Sequencing errors, how to account for them ?</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#error-correction-methods"><i class="fa fa-check"></i><b>1.3.1</b> Error correction methods</a></li>
<li class="chapter" data-level="1.3.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#more-accurate-sequencing-methods"><i class="fa fa-check"></i><b>1.3.2</b> More accurate sequencing methods</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#the-special-case-of-homopolymers"><i class="fa fa-check"></i><b>1.4</b> The special case of homopolymers</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#homopolymers-and-the-human-genome"><i class="fa fa-check"></i><b>1.4.1</b> Homopolymers and the human genome</a></li>
<li class="chapter" data-level="1.4.2" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#homopolymers-and-long-reads"><i class="fa fa-check"></i><b>1.4.2</b> Homopolymers and long reads</a></li>
<li class="chapter" data-level="1.4.3" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#accounting-for-homopolymers"><i class="fa fa-check"></i><b>1.4.3</b> Accounting for homopolymers</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="what-is-sequence-data.html"><a href="what-is-sequence-data.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html"><i class="fa fa-check"></i><b>2</b> Aligning sequence data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#what-is-an-alignment"><i class="fa fa-check"></i><b>2.1</b> What is an alignment ?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#why-align"><i class="fa fa-check"></i><b>2.1.1</b> Why align ?</a></li>
<li class="chapter" data-level="2.1.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#how-to-align-two-sequences"><i class="fa fa-check"></i><b>2.1.2</b> How to align two sequences ?</a></li>
<li class="chapter" data-level="2.1.3" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#scoring-and-substitution-models"><i class="fa fa-check"></i><b>2.1.3</b> Scoring and substitution models</a></li>
<li class="chapter" data-level="2.1.4" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#dealing-with-gaps"><i class="fa fa-check"></i><b>2.1.4</b> Dealing with gaps</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#how-do-we-speed-up-pairwise-alignment"><i class="fa fa-check"></i><b>2.2</b> How do we speed up pairwise alignment ?</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#changing-the-method"><i class="fa fa-check"></i><b>2.2.1</b> Changing the method</a></li>
<li class="chapter" data-level="2.2.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#seed-and-extend-with-data-structures"><i class="fa fa-check"></i><b>2.2.2</b> Seed and extend with data structures</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#the-specificities-of-read-mapping"><i class="fa fa-check"></i><b>2.3</b> The specificities of read-mapping</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#what-is-read-mapping"><i class="fa fa-check"></i><b>2.3.1</b> What is read-mapping ?</a></li>
<li class="chapter" data-level="2.3.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#challenges-of-read-mapping"><i class="fa fa-check"></i><b>2.3.2</b> Challenges of read-mapping</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#multiple-sequence-alignment"><i class="fa fa-check"></i><b>2.4</b> Multiple sequence alignment</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#progressive-alignment"><i class="fa fa-check"></i><b>2.4.1</b> Progressive alignment</a></li>
<li class="chapter" data-level="2.4.2" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#other-methods"><i class="fa fa-check"></i><b>2.4.2</b> Other methods</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="aligning-sequence-data.html"><a href="aligning-sequence-data.html#conclusion-1"><i class="fa fa-check"></i><b>2.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="HPC-paper.html"><a href="HPC-paper.html"><i class="fa fa-check"></i><b>3</b> Contribution 1: Improving read alignment by exploring a sequence transformation space</a>
<ul>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#highlights"><i class="fa fa-check"></i>Highlights</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#graphical-abstract"><i class="fa fa-check"></i>Graphical Abstract</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="3.1" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="HPC-paper.html"><a href="HPC-paper.html#methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="HPC-paper.html"><a href="HPC-paper.html#sec:msr-def"><i class="fa fa-check"></i><b>3.2.1</b> Streaming sequence reductions</a></li>
<li class="chapter" data-level="3.2.2" data-path="HPC-paper.html"><a href="HPC-paper.html#sec:enum"><i class="fa fa-check"></i><b>3.2.2</b> Restricting the space of streaming sequence reductions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="HPC-paper.html"><a href="HPC-paper.html#datasets-and-pipelines"><i class="fa fa-check"></i><b>3.3</b> Datasets and Pipelines</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="HPC-paper.html"><a href="HPC-paper.html#datasets"><i class="fa fa-check"></i><b>3.3.1</b> Datasets</a></li>
<li class="chapter" data-level="3.3.2" data-path="HPC-paper.html"><a href="HPC-paper.html#simulation-pipeline"><i class="fa fa-check"></i><b>3.3.2</b> Simulation pipeline</a></li>
<li class="chapter" data-level="3.3.3" data-path="HPC-paper.html"><a href="HPC-paper.html#evaluation-pipeline"><i class="fa fa-check"></i><b>3.3.3</b> Evaluation pipeline</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="HPC-paper.html"><a href="HPC-paper.html#hpc-results"><i class="fa fa-check"></i><b>3.4</b> Results</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="HPC-paper.html"><a href="HPC-paper.html#selection-of-mapping-friendly-sequence-reductions"><i class="fa fa-check"></i><b>3.4.1</b> Selection of mapping-friendly sequence reductions</a></li>
<li class="chapter" data-level="3.4.2" data-path="HPC-paper.html"><a href="HPC-paper.html#mapping-friendly-sequence-reductions-lead-to-lower-mapping-errors-on-whole-genomes"><i class="fa fa-check"></i><b>3.4.2</b> Mapping-friendly sequence reductions lead to lower mapping errors on whole genomes</a></li>
<li class="chapter" data-level="3.4.3" data-path="HPC-paper.html"><a href="HPC-paper.html#mapping-friendly-sequence-reductions-increase-mapping-quality-on-repeated-regions-of-the-human-genome"><i class="fa fa-check"></i><b>3.4.3</b> Mapping-friendly sequence reductions increase mapping quality on repeated regions of the human genome</a></li>
<li class="chapter" data-level="3.4.4" data-path="HPC-paper.html"><a href="HPC-paper.html#raw-mapping-improves-upon-hpc-on-centromeric-regions"><i class="fa fa-check"></i><b>3.4.4</b> Raw mapping improves upon HPC on centromeric regions</a></li>
<li class="chapter" data-level="3.4.5" data-path="HPC-paper.html"><a href="HPC-paper.html#positions-of-incorrectly-mapped-reads-across-the-entire-human-genome"><i class="fa fa-check"></i><b>3.4.5</b> Positions of incorrectly mapped reads across the entire human genome</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="HPC-paper.html"><a href="HPC-paper.html#discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
<li class="chapter" data-level="3.6" data-path="HPC-paper.html"><a href="HPC-paper.html#limitations-of-this-study"><i class="fa fa-check"></i><b>3.6</b> Limitations of this study</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#author-contributions"><i class="fa fa-check"></i>Author contributions</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#declaration-of-interests"><i class="fa fa-check"></i>Declaration of interests</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#star-methods"><i class="fa fa-check"></i>STAR Methods</a>
<ul>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#lead-contact"><i class="fa fa-check"></i>Lead contact</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#materials-availability"><i class="fa fa-check"></i>Materials availability</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#data-and-code-availability"><i class="fa fa-check"></i>Data and code availability</a></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#method-details"><i class="fa fa-check"></i>Method details</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="HPC-paper.html"><a href="HPC-paper.html#supplementary-information"><i class="fa fa-check"></i>Supplementary information</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html"><i class="fa fa-check"></i><b>4</b> Learning from sequences and alignments</a>
<ul>
<li class="chapter" data-level="4.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#why-learn-from-alignments"><i class="fa fa-check"></i><b>4.1</b> Why learn from alignments ?</a></li>
<li class="chapter" data-level="4.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#what-to-learn"><i class="fa fa-check"></i><b>4.2</b> What to learn ?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#supervised-learning"><i class="fa fa-check"></i><b>4.2.1</b> Supervised learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#unsupervised-learning"><i class="fa fa-check"></i><b>4.2.2</b> Unsupervised learning</a></li>
<li class="chapter" data-level="4.2.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#others-paradigms"><i class="fa fa-check"></i><b>4.2.3</b> Others paradigms</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#how-to-learn"><i class="fa fa-check"></i><b>4.3</b> How to learn ?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#general-setting"><i class="fa fa-check"></i><b>4.3.1</b> General setting</a></li>
<li class="chapter" data-level="4.3.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#tests-and-statistical-learning"><i class="fa fa-check"></i><b>4.3.2</b> Tests and statistical learning</a></li>
<li class="chapter" data-level="4.3.3" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#more-complex-methods"><i class="fa fa-check"></i><b>4.3.3</b> More complex methods</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#preprocessing-the-alignment-for-machine-learning"><i class="fa fa-check"></i><b>4.4</b> Preprocessing the alignment for machine learning</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#general-purpose-encodings"><i class="fa fa-check"></i><b>4.4.1</b> General purpose encodings</a></li>
<li class="chapter" data-level="4.4.2" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#biological-sequence-specific-encodings"><i class="fa fa-check"></i><b>4.4.2</b> Biological sequence-specific encodings</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="learning-from-sequences-and-alignments.html"><a href="learning-from-sequences-and-alignments.html#conclusion-2"><i class="fa fa-check"></i><b>4.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html"><i class="fa fa-check"></i><b>5</b> Viruses, HIV and drug resistance</a>
<ul>
<li class="chapter" data-level="5.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#what-are-viruses"><i class="fa fa-check"></i><b>5.1</b> What are viruses ?</a></li>
<li class="chapter" data-level="5.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#getting-to-know-hiv"><i class="fa fa-check"></i><b>5.2</b> Getting to know HIV</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#quick-presentation-of-hiv"><i class="fa fa-check"></i><b>5.2.1</b> Quick Presentation of HIV</a></li>
<li class="chapter" data-level="5.2.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#the-replication-cycle-of-hiv"><i class="fa fa-check"></i><b>5.2.2</b> The replication cycle of HIV</a></li>
<li class="chapter" data-level="5.2.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#genetics-of-hiv"><i class="fa fa-check"></i><b>5.2.3</b> Genetics of HIV</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#drug-resistance-in-hiv"><i class="fa fa-check"></i><b>5.3</b> Drug resistance in HIV</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#a-quick-history-of-art"><i class="fa fa-check"></i><b>5.3.1</b> A quick history of ART</a></li>
<li class="chapter" data-level="5.3.2" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#drug-mechanisms"><i class="fa fa-check"></i><b>5.3.2</b> Main mechanisms of viral proteins, antiretroviral drugs and associated resistance.</a></li>
<li class="chapter" data-level="5.3.3" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#consequences-of-resistance-on-global-health"><i class="fa fa-check"></i><b>5.3.3</b> Consequences of resistance on global health</a></li>
<li class="chapter" data-level="5.3.4" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#finding-drms"><i class="fa fa-check"></i><b>5.3.4</b> Finding DRMs </a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="viruses-hiv-and-drug-resistance.html"><a href="viruses-hiv-and-drug-resistance.html#conclusion-3"><i class="fa fa-check"></i><b>5.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="HIV-paper.html"><a href="HIV-paper.html"><i class="fa fa-check"></i><b>6</b> Contribution 2: Inferring mutation roles from sequence alignments using machine learning</a>
<ul>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#abstract-paper"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#author-summary"><i class="fa fa-check"></i>Author summary</a></li>
<li class="chapter" data-level="6.1" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="HIV-paper.html"><a href="HIV-paper.html#materials-and-methods"><i class="fa fa-check"></i><b>6.2</b> Materials and methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="HIV-paper.html"><a href="HIV-paper.html#data"><i class="fa fa-check"></i><b>6.2.1</b> Data</a></li>
<li class="chapter" data-level="6.2.2" data-path="HIV-paper.html"><a href="HIV-paper.html#classifier-training"><i class="fa fa-check"></i><b>6.2.2</b> Classifier training</a></li>
<li class="chapter" data-level="6.2.3" data-path="HIV-paper.html"><a href="HIV-paper.html#measuring-classifier-performance"><i class="fa fa-check"></i><b>6.2.3</b> Measuring classifier performance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-results"><i class="fa fa-check"></i><b>6.3</b> Results</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="HIV-paper.html"><a href="HIV-paper.html#classifier-performance-interpretation"><i class="fa fa-check"></i><b>6.3.1</b> Classifier performance &amp; interpretation</a></li>
<li class="chapter" data-level="6.3.2" data-path="HIV-paper.html"><a href="HIV-paper.html#additional-classification-results"><i class="fa fa-check"></i><b>6.3.2</b> Additional classification results</a></li>
<li class="chapter" data-level="6.3.3" data-path="HIV-paper.html"><a href="HIV-paper.html#identifying-new-mutations-from-classifiers"><i class="fa fa-check"></i><b>6.3.3</b> Identifying new mutations from classifiers</a></li>
<li class="chapter" data-level="6.3.4" data-path="HIV-paper.html"><a href="HIV-paper.html#detailed-analysis-of-potentially-resistance-associated-mutations"><i class="fa fa-check"></i><b>6.3.4</b> Detailed analysis of potentially resistance-associated mutations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="HIV-paper.html"><a href="HIV-paper.html#discussion-and-perspectives"><i class="fa fa-check"></i><b>6.4</b> Discussion and perspectives</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#hiv-acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="HIV-paper.html"><a href="HIV-paper.html#supporting-information"><i class="fa fa-check"></i>Supporting Information</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html"><i class="fa fa-check"></i><b>7</b> Learning alignments, an interesting perspective</a>
<ul>
<li class="chapter" data-level="7.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learning-sequence-embeddings"><i class="fa fa-check"></i><b>7.1</b> Learning sequence embeddings</a></li>
<li class="chapter" data-level="7.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learning-pairwise-alignment"><i class="fa fa-check"></i><b>7.2</b> Learning pairwise alignment</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#dedal"><i class="fa fa-check"></i><b>7.2.1</b> DEDAL</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#predicting-an-alignment"><i class="fa fa-check"></i><b>7.2.2</b> predicting an alignment</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#what-else-could-we-learn"><i class="fa fa-check"></i><b>7.3</b> What else could we learn ?</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learn-to-predict-seeds-or-starting-positions"><i class="fa fa-check"></i><b>7.3.1</b> Learn to predict seeds or starting positions</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning-alignments-an-interesting-perspective.html"><a href="learning-alignments-an-interesting-perspective.html#learn-pre-processing-functions"><i class="fa fa-check"></i><b>7.3.2</b> Learn pre-processing functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html"><i class="fa fa-check"></i>Global conclusion</a>
<ul>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#hpc-part"><i class="fa fa-check"></i>HPC part</a></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#hiv-part"><i class="fa fa-check"></i>HIV part</a></li>
<li class="chapter" data-level="" data-path="global-conclusion.html"><a href="global-conclusion.html#final-words"><i class="fa fa-check"></i>Final words</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="global-references.html"><a href="global-references.html"><i class="fa fa-check"></i>Global References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="HPC-appendix.html"><a href="HPC-appendix.html"><i class="fa fa-check"></i><b>A</b> Supporting Information for “Mapping-friendly sequence reductions: going beyond homopolymer compression”</a>
<ul>
<li class="chapter" data-level="A.1" data-path="HPC-appendix.html"><a href="HPC-appendix.html#appendix:tandemtools"><i class="fa fa-check"></i><b>A.1</b> “TandemTools” dataset generation</a></li>
<li class="chapter" data-level="A.2" data-path="HPC-appendix.html"><a href="HPC-appendix.html#msr-performance-comparison"><i class="fa fa-check"></i><b>A.2</b> MSR performance comparison</a></li>
<li class="chapter" data-level="A.3" data-path="HPC-appendix.html"><a href="HPC-appendix.html#analyzing-read-origin-on-whole-human-genome"><i class="fa fa-check"></i><b>A.3</b> Analyzing read origin on whole human genome</a></li>
<li class="chapter" data-level="A.4" data-path="HPC-appendix.html"><a href="HPC-appendix.html#performance-of-msrs-on-the-drosophila-genome"><i class="fa fa-check"></i><b>A.4</b> Performance of MSRs on the Drosophila genome</a></li>
<li class="chapter" data-level="A.5" data-path="HPC-appendix.html"><a href="HPC-appendix.html#key-resource-table"><i class="fa fa-check"></i><b>A.5</b> Key Resource Table</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html"><i class="fa fa-check"></i><b>B</b> Supporting Information for “HIV and DRMs”</a>
<ul>
<li class="chapter" data-level="B.1" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html#detailed-list-of-hiv-1-protein-structures-used-for-figure-generation."><i class="fa fa-check"></i><b>B.1</b> Detailed list of HIV-1 protein structures used for figure generation.</a></li>
<li class="chapter" data-level="B.2" data-path="HIV-intro-appendix.html"><a href="HIV-intro-appendix.html#list-of-all-antiretroviral-drugs"><i class="fa fa-check"></i><b>B.2</b> List of all antiretroviral drugs</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="HIV-appendix.html"><a href="HIV-appendix.html"><i class="fa fa-check"></i><b>C</b> Supporting Information for “Using Machine Learning and Big Data to Explore the Drug Resistance Landscape in HIV”</a>
<ul>
<li class="chapter" data-level="C.1" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S1-Appendix"><i class="fa fa-check"></i><b>C.1</b> S1 Appendix (Technical appendix).</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="HIV-appendix.html"><a href="HIV-appendix.html#data-appendix"><i class="fa fa-check"></i><b>C.1.1</b> Data</a></li>
<li class="chapter" data-level="C.1.2" data-path="HIV-appendix.html"><a href="HIV-appendix.html#classifiers"><i class="fa fa-check"></i><b>C.1.2</b> Classifiers</a></li>
<li class="chapter" data-level="C.1.3" data-path="HIV-appendix.html"><a href="HIV-appendix.html#scoring"><i class="fa fa-check"></i><b>C.1.3</b> Scoring</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s1-fig."><i class="fa fa-check"></i><b>C.2</b> S1 Fig.</a></li>
<li class="chapter" data-level="C.3" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s2-fig."><i class="fa fa-check"></i><b>C.3</b> S2 Fig.</a></li>
<li class="chapter" data-level="C.4" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s3-fig."><i class="fa fa-check"></i><b>C.4</b> S3 Fig.</a></li>
<li class="chapter" data-level="C.5" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S1-Table"><i class="fa fa-check"></i><b>C.5</b> S1 Table.</a></li>
<li class="chapter" data-level="C.6" data-path="HIV-appendix.html"><a href="HIV-appendix.html#S2-Appendix"><i class="fa fa-check"></i><b>C.6</b> S2 Appendix. (Fisher exact tests)</a></li>
<li class="chapter" data-level="C.7" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s1-data."><i class="fa fa-check"></i><b>C.7</b> S1 Data.</a></li>
<li class="chapter" data-level="C.8" data-path="HIV-appendix.html"><a href="HIV-appendix.html#s2-data."><i class="fa fa-check"></i><b>C.8</b> S2 Data.</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://lucblassel.com" target="blank">Back to main website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">From sequences to knowledge, improving and learning from sequence alignments</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="learning-from-sequences-and-alignments" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Learning from sequences and alignments<a href="learning-from-sequences-and-alignments.html#learning-from-sequences-and-alignments" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="why-learn-from-alignments" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Why learn from alignments ?<a href="learning-from-sequences-and-alignments.html#why-learn-from-alignments" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sequences and sequence alignments are a very rich source of information. As was sated in Chapters <a href="aligning-sequence-data.html#aligning-sequence-data">2</a> and <a href="HPC-paper.html#HPC-paper">3</a>, many downstream analyses rely on sequence alignments.</p>
<p>In whole genome assembly, where sequencing reads are combined together to deduce the sequence genome, pairwise sequence alignment is used in reference-based assembly<span class="citation"><sup><a href="#ref-martinNextgenerationTranscriptomeAssembly2011" role="doc-biblioref">317</a>,<a href="#ref-kyriakidouCurrentStrategiesPolyploid2018" role="doc-biblioref">318</a></sup></span> as well as <em>de novo</em><span class="citation"><sup><a href="#ref-paszkiewiczNovoAssemblyShort2010" role="doc-biblioref">319</a>,<a href="#ref-sohnPresentFutureNovo2018" role="doc-biblioref">320</a></sup></span> assembly. It has also been used to deduce protein function<span class="citation"><sup><a href="#ref-sleatorOverviewSilicoProtein2010" role="doc-biblioref">321</a></sup></span>. It has been used for sequence clustering<span class="citation"><sup><a href="#ref-sahlinNovoClusteringLongRead2020" role="doc-biblioref">115</a></sup></span> as well as detecting genetic<span class="citation"><sup><a href="#ref-koboldtBestPracticesVariant2020" role="doc-biblioref">322</a></sup></span> and structural variants<span class="citation"><sup><a href="#ref-alkanGenomeStructuralVariation2011" role="doc-biblioref">323</a>,<a href="#ref-hoStructuralVariationSequencing2020" role="doc-biblioref">324</a></sup></span>. Multiple sequence alignments are also very widely used, mainly in phylogenetic analyses where the evolutionary history of a set of sequences are studied and represented as trees<span class="citation"><sup><a href="#ref-morrisonPhylogeneticTreebuilding1996" role="doc-biblioref">325</a>,<a href="#ref-kapliPhylogeneticTreeBuilding2020" role="doc-biblioref">326</a></sup></span>, but they have also been used extensively in protein structure prediction<span class="citation"><sup><a href="#ref-kuhlmanAdvancesProteinStructure2019" role="doc-biblioref">327</a></sup></span>.</p>
<p>More recently, as computational power and datasets have grown, more and more machine learning methods are being used on sequence alignments in order to gain biological insight. In this chapter, we will explore how this can be done as an introduction to Chapter <a href="HIV-paper.html#HIV-paper">6</a> where we present an application: predicting HIV drug resistance mutations.</p>
</div>
<div id="what-to-learn" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> What to learn ?<a href="learning-from-sequences-and-alignments.html#what-to-learn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the first questions one might ask themselves when wishing to use machine learning with sequence data is “what can I learn?”. A simplistic answer to this question would be “a lot of things” as the following section will strive to show. To choose what we learn we must first choose a learning paradigm.</p>
<div id="supervised-learning" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Supervised learning<a href="learning-from-sequences-and-alignments.html#supervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Supervised learning is one of the main machine learning paradigms, here we have data that consist of a collection of input and output pairs (e.g. a DNA sequence and an associated species). By feeding these pairs to our algorithm of choice it will learn to predict the output based on the input alone. This is a very powerful way of learning something interesting. We can consider the link between inputs and outputs as extra knowledge that the dataset creator or curator can infuse in the learning algorithm. Within the supervised learning paradigm there are two possible tasks: <em>regression</em> and <em>classification</em>.</p>
<div id="regression-tasks" class="section level4 hasAnchor" number="4.2.1.1">
<h4><span class="header-section-number">4.2.1.1</span> Regression tasks<a href="learning-from-sequences-and-alignments.html#regression-tasks" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For regression tasks, the outputs of our input-output pairs are encoded by a continuous numerical value. Regression models will therefore output continuous real values. Fortunately, many interesting continuous values can be computed from aligned sequences, and in many cases machine learning models can be trained to predict these variables.</p>
<p>Regression methods have been used to predict drug response in cancer patients<span class="citation"><sup><a href="#ref-ammad-ud-dinSystematicIdentificationFeature2017" role="doc-biblioref">328</a></sup></span> and resistance levels to drugs in HIV<span class="citation"><sup><a href="#ref-steinerDrugResistancePrediction2020a" role="doc-biblioref">329</a></sup></span>. These methods are also extensively used in protein structure prediction, where methods are trained to predict residue angles or values in protein contact maps from aligned sequences.<span class="citation"><sup><a href="#ref-noeMachineLearningProtein2020" role="doc-biblioref">330</a>–<a href="#ref-alquraishiMachineLearningProtein2021" role="doc-biblioref">334</a></sup></span> Or directly from an MSA<span class="citation"><sup><a href="#ref-jumperHighlyAccurateProtein2021" role="doc-biblioref">136</a></sup></span>.Regression algorithms have been used to predict protein fitness <em>in silico</em><span class="citation"><sup><a href="#ref-wittmannAdvancesMachineLearning2021" role="doc-biblioref">335</a>–<a href="#ref-liCanMachineLearning2019" role="doc-biblioref">337</a></sup></span> to speed up protein engineering, and make some processes like drug development faster and cheaper. They have also been used in many other tasks such as predicting gene expression levels<span class="citation"><sup><a href="#ref-xieDeepAutoencoderModel2017" role="doc-biblioref">338</a></sup></span> or predicting multiple sequence alignment scores<span class="citation"><sup><a href="#ref-ortunoComparingDifferentMachine2015" role="doc-biblioref">339</a></sup></span>.</p>
<p>In many cases these methods use an encoded representation of the sequences (c.f. Section <a href="learning-from-sequences-and-alignments.html#preprocessing-the-alignment-for-machine-learning">4.4</a>) as input, but some represent the inputs as values computed from alignments. For example, protein structure can be predicted from contact maps<span class="citation"><sup><a href="#ref-wangAccurateNovoPrediction2017" role="doc-biblioref">340</a></sup></span> derived from MSAs, and gene expression levels can be predicted from lists of mutations that are obtained through alignment to a reference sequence<span class="citation"><sup><a href="#ref-xieDeepAutoencoderModel2017" role="doc-biblioref">338</a></sup></span>, this last approach is also used in Chapter <a href="HIV-paper.html#HIV-paper">6</a> to predict drug resistance in HIV.</p>
</div>
<div id="classification-tasks" class="section level4 hasAnchor" number="4.2.1.2">
<h4><span class="header-section-number">4.2.1.2</span> Classification tasks<a href="learning-from-sequences-and-alignments.html#classification-tasks" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For classification tasks, the outputs of our input-output pairs are categorical in nature and often represented as discrete integer values. Originally, most classification methods were designed for binary classification with only two possible outputs: a “positive” and a “negative” class. This is a simpler problem to solve than multiclass classification problem where more than two outputs are possible, however most methods that can handle binary classification have been adapted to multiclass classification.</p>
<p>In biology, categorizing and classifying is often at the root of several research problems, as such machine learning classifiers have obvious applications and have been widely used, with sequence data as inputs. Classifiers have been used to predict if a particular virus<span class="citation"><sup><a href="#ref-hagaMachineLearningbasedTreatment2020" role="doc-biblioref">341</a>,<a href="#ref-zazziPredictingResponseAntiretroviral2012" role="doc-biblioref">342</a></sup></span> (also Chapter <a href="HIV-paper.html#HIV-paper">6</a>), or bacteria<span class="citation"><sup><a href="#ref-renPredictionAntimicrobialResistance2022" role="doc-biblioref">343</a>,<a href="#ref-kimMachineLearningAntimicrobial2022" role="doc-biblioref">344</a></sup></span> is resistant to antiviral or antimicrobial drugs respectively. Some classifier models have also been used to predict characteristics at positions in a sequence, like methylation site prediction<span class="citation"><sup><a href="#ref-wangPredictingDNAMethylation2016" role="doc-biblioref">345</a></sup></span>, splicing site detection<span class="citation"><sup><a href="#ref-ratschLearningInterpretableSVMs2006" role="doc-biblioref">346</a></sup></span> or secondary structure at a particular amino acid residue<span class="citation"><sup><a href="#ref-jonesProteinSecondaryStructure1999" role="doc-biblioref">347</a></sup></span>. Finally, classifiers have also been used to predict predict more general characteristics of given sequence, like the cellular localization<span class="citation"><sup><a href="#ref-weiPredictionHumanProtein2018" role="doc-biblioref">348</a></sup></span> or putative function<span class="citation"><sup><a href="#ref-wangProteinSequenceProtein2017" role="doc-biblioref">349</a></sup></span> of a protein, or the cellular localization of gene expression data<span class="citation"><sup><a href="#ref-kelleyBassetLearningRegulatory2016" role="doc-biblioref">350</a></sup></span>.</p>
<p>I have presented here only a fraction of what is possible to learn from sequences in the supervised learning paradigm, and I hope you will agree with me that there is no shortage of problems in computational biology that are suited to this sort of approach. By using machine learning here, instead of more formal statistical approaches, there is a lower amount of upfront assumptions and the algorithm is tasked with figuring out what features of the data are important or not for the task at hand.</p>
</div>
</div>
<div id="unsupervised-learning" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Unsupervised learning<a href="learning-from-sequences-and-alignments.html#unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The second main machine learning paradigm is called, by contrast to supervised learning, unsupervised learning. In this paradigm we do not have input-output pairs but only inputs. The goal of unsupervised machine learning methods is to extract some structure or patterns from the given input without additional guidance.</p>
<p>One of the main tasks in the unsupervised learning paradigm is clustering, wherein similar inputs are grouped together, methods like k-means of hierarchical clustering<span class="citation"><sup><a href="#ref-hastieElementsStatisticalLearning2009" role="doc-biblioref">351</a></sup></span> often use some type of distance metric between inputs to define these clusters of similar inputs. Clustering can be used for classification tasks, indeed if some characteristics of sequences in a given cluster are known then we can make the assumptions that sequences in the same cluster will be similar and share these characteristics, this has been used to group proteins in families<span class="citation"><sup><a href="#ref-kriventsevaClusteringAnalysisProtein2001" role="doc-biblioref">352</a></sup></span> for example. Clustering methods can also be used to remove duplicate or near-duplicate sequences in datasets<span class="citation"><sup><a href="#ref-fuCDHITAcceleratedClustering2012" role="doc-biblioref">353</a></sup></span>. Phylogenetic trees can be considered as a specific type of clustering methods, and they have been used to cluster biological sequences<span class="citation"><sup><a href="#ref-balabanTreeClusterClusteringBiological2019" role="doc-biblioref">354</a></sup></span>.</p>
<p>One of the main obstacles to clustering biological sequences is the need for computing distances between sequences. As stated in Chapter <a href="aligning-sequence-data.html#aligning-sequence-data">2</a>, obtaining a biologically relevant distance metric between two sequences, such as the edit-distance, is no easy task. Additionally, in many cases all pairwise distances are needed for clustering, meaning at least a quadratic time and space complexity for a naive clustering algorithm. Two approaches can be used to resolve this problem: devise methods that do not need all pairwise distances<span class="citation"><sup><a href="#ref-zoritaStarcodeSequenceClustering2015" role="doc-biblioref">355</a></sup></span>, or find a way to speed up distance computation. Some methods have been developed to devise distance metrics that are biologically relevant and less expensive to compute that the edit-distance: like the hashing based MASH<span class="citation"><sup><a href="#ref-ondovMashFastGenome2016" role="doc-biblioref">356</a></sup></span> or dashing<span class="citation"><sup><a href="#ref-bakerDashingFastAccurate2019" role="doc-biblioref">357</a></sup></span>, or the neural network based NeuroSEED<span class="citation"><sup><a href="#ref-corsoNeuralDistanceEmbeddings2021" role="doc-biblioref">358</a></sup></span>.</p>
<p>Unsupervised learning can also be used without clustering, for example unsupervised methods based on maximum likelihood approaches have been used to predict mutational effects in protein sequence<span class="citation"><sup><a href="#ref-hopfMutationEffectsPredicted2017" role="doc-biblioref">359</a></sup></span> as well as predict recombination hotspots in human genomic sequences<span class="citation"><sup><a href="#ref-castroModelSelectionApproach2018" role="doc-biblioref">360</a></sup></span>.</p>
<p>In many cases, unsupervised learning can be done as a preliminary dimensionality reduction step to a supervised learning task. Indeed biological data is often high-dimensional, and it is often useful to lower the amount of dimensions to speed up computations. Some unsupervised methods can reduce the number of dimensions while retaining most of the information. One such method, Principal Component Analysis (PCA), has been widely used. PCA was applied to distance matrices to compute phylogenetic trees<span class="citation"><sup><a href="#ref-haschkaMNHNTreeToolsToolboxTree2021" role="doc-biblioref">361</a></sup></span>, and work has been done to apply PCA directly to MSAs without needing to go through a distance matrix<span class="citation"><sup><a href="#ref-konishiPrincipalComponentAnalysis2019" role="doc-biblioref">362</a></sup></span>. PCA is also widely used in clustering applications<span class="citation"><sup><a href="#ref-ben-hurDetectingStableClusters2003" role="doc-biblioref">363</a>–<a href="#ref-clampJalviewJavaAlignment2004" role="doc-biblioref">366</a></sup></span>.</p>
</div>
<div id="others-paradigms" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Others paradigms<a href="learning-from-sequences-and-alignments.html#others-paradigms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>More recently, other learning paradigms have gained popularity in machine learning circles. Within the <em>semi-supervised</em> paradigm, a small amount of labelled data (i.e. input-output pairs) is included in a large un-labelled dataset, and methods can leverage both. This approach has been used to predict drug to protein interactions<span class="citation"><sup><a href="#ref-xiaSemisupervisedDrugproteinInteraction2010" role="doc-biblioref">367</a></sup></span> and predict the secondary structure of specific transmembrane proteins<span class="citation"><sup><a href="#ref-tamposisSemisupervisedLearningHidden2019" role="doc-biblioref">368</a></sup></span>.</p>
<p>In the <em>self-supervised</em> paradigm, models are first trained on a proxy task that hopefully makes use of important information in the data. Through this pre-training step, self-supervised models extract important information from the data and create internal featutres and models that can then be leveraged in a a supervized or uinsupervized fine-tuning task. This paradigm has exploded lately within the field of natural language processing and machine translation with the rise of transformers, but has also been widely used to create protein language models like ProtBert<span class="citation"><sup><a href="#ref-elnaggarProtTransCrackingLanguage2021" role="doc-biblioref">369</a></sup></span> and extract information from disordered protein regions<span class="citation"><sup><a href="#ref-luDiscoveringMolecularFeatures2022" role="doc-biblioref">370</a></sup></span>. We will look at self-supervised leraning with a little more detail in Chapter <a href="learning-alignments-an-interesting-perspective.html#learning-alignments-an-interesting-perspective">7</a>.</p>
<p>Finally, the end-to-end learning paradigm designates the process of chaining several machine learnign tasks together and optimizing the algorithms simultaneously using the error from the loss of the last task of the group. This has been successfully used to predict protein-protein interaction surfaces in three dimensions<span class="citation"><sup><a href="#ref-townshendEndtoEndLearning3D2019" role="doc-biblioref">371</a></sup></span> as well as predict micro-RNA targets sequences<span class="citation"><sup><a href="#ref-leeDeepTargetEndtoendLearning2016" role="doc-biblioref">372</a></sup></span>. This paradigm can also be used in a task-based fashion, where a differentiable loss function is crafted on a traditionally non-machine learning task and used to train preceeding models, this has been explored for sequence alignment and is further detailed in Chapter <a href="learning-alignments-an-interesting-perspective.html#learning-alignments-an-interesting-perspective">7</a></p>
</div>
</div>
<div id="how-to-learn" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> How to learn ?<a href="learning-from-sequences-and-alignments.html#how-to-learn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Machine learning regroups a multitude of techniques and methods to extract knowledge and make data-driven predictions. In this section we will quickly go over some of the main supervised-learning methods, and go into more detail for techniques used in Chapter <a href="HIV-paper.html#HIV-paper">6</a>: Logistic regression, naive Bayes and Random Forests.</p>
<div id="general-setting" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> General setting<a href="learning-from-sequences-and-alignments.html#general-setting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Supervised machine learning is an optimization process. A given algorithm, which I will refer to as a <em>model</em>, has an associated loss function that can be evaluated on a dataset. This loss represents how well the model is predicting outputs from inputs on known input-output pairs. Through an iterative process, this loss is optimized <em>(in our case minimized)</em> over all pairs forming a dataset. Often in the literature, loss and cost are used interchangeably<span class="citation"><sup><a href="#ref-Goodfellow-et-al-2016" role="doc-biblioref">373</a></sup></span>, however I will favor loss in the following sections.</p>
<p>There is no shortage of loss functions<span class="citation"><sup><a href="#ref-wangComprehensiveSurveyLoss2022" role="doc-biblioref">374</a></sup></span>, some of them are specifically crafted for a given model while some are widely used in regression tasks like the Root Mean Square Error (RMSE). Others like the cross-entropy loss are used in classification tasks.</p>
<p>Often after training a machine learning model on a dataset it is important to compute a performance measure to get an idea of how well this model is performing. We could do this on the same data on which the model was trained, this would however be wrong. Indeed it gives an unfair advantage to the model since it predicts outputs for examples it has already seen, furthermore it gives us no insight into the generalizability of the model since it could just learn the dataset by heart, getting a perfect score on it while being completely useless on new unseen data, this situation is known as <em>overfitting</em><span class="citation"><sup><a href="#ref-hastieElementsStatisticalLearning2009" role="doc-biblioref">351</a></sup></span>, shown in Figure <a href="learning-from-sequences-and-alignments.html#fig:overfitting">4.1</a>. Since being able to predict outcomes on unseen data is the main goal of a machine learning model we need another way of measuring model performance. The way machine learning practitioners can measure the performance of their model in a more unbiased manner is by separating the dataset into two parts before even starting to train the model: one part (usually the majority of the data) is used as the <em>training set</em>, and the other as the <em>testing set</em>. Logically, the training set is used to train the model while the testing set is used to evaluate the performance of the model after training.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:overfitting"></span>
<img src="figures/Encode-seqs/Overfitting.png" alt="**Overfitting behaviour in loss functions.**  
The two curves show how the loss calculated on the training set (blue) and the testing set (red) evolve as training time increases. At first both decrease showing that the model learns informative and generalizable features. At some point, training loss keeps decreasing and testing loss increases, meaning that the model is learning over-specific features on the training set and is no longer generalizable: it is overfitting." width="70%" />
<p class="caption">
Figure 4.1: <strong>Overfitting behaviour in loss functions.</strong><br />
The two curves show how the loss calculated on the training set (blue) and the testing set (red) evolve as training time increases. At first both decrease showing that the model learns informative and generalizable features. At some point, training loss keeps decreasing and testing loss increases, meaning that the model is learning over-specific features on the training set and is no longer generalizable: it is overfitting.
</p>
</div>
<p>As there is a multitude of loss functions, there are many performance metrics to asses how the model is doing on the testing data, especially for classification tasks<span class="citation"><sup><a href="#ref-jiaoPerformanceMeasuresEvaluating2016" role="doc-biblioref">375</a></sup></span>. For regression, RMSE is also widely used as a performance metric, along with the Mean Absolute Error (MAE). For classification, accuracy is most widely used performance metric. Accuracy is the ratio of the number of correctly classified examples divided by the total number of examples. Accuracy has also been adapted to specific settings like unbalanced data where the different possible output classes are not represented equally<span class="citation"><sup><a href="#ref-brodersenBalancedAccuracyIts2010" role="doc-biblioref">376</a></sup></span>. The testing set must however stay completely separate from the training set and decisions about model settings or input features used must be made without help of the testing data. If these stringent conditions are not respected this can lead to data leakage and artificially increase performance of the model on the testing data, giving us a biased view of the model performance and generalizability<span class="citation"><sup><a href="#ref-kaufmanLeakageDataMining2011" role="doc-biblioref">377</a></sup></span>. This leaking of testing data into the training process is a common pitfall of machine learning<span class="citation"><sup><a href="#ref-whalenNavigatingPitfallsApplying2022" role="doc-biblioref">378</a></sup></span>.</p>
<p>In many cases, machine learning models have a number of parameters that guide model behavior. These parameters are chosen before training and are different from the internal parameters of the model that are optimized during training, as such they are often called <em>hyper-parameters</em>, these can be the number of levels in a decision tree, some learning rate or a stopping threshold for example. The value of these hyper-parameters is often very influential on model performance, however setting hyper-parameter values based on the model’s test set performance would lead to data leakage as stated earlier. To still be able to tune hyper-parameters for optimal performance <span class="math inline">\(k\)</span><em>-fold cross-validation</em> is used<span class="citation"><sup><a href="#ref-hastieElementsStatisticalLearning2009" role="doc-biblioref">351</a></sup></span>. In This setting, shown in Figure <a href="learning-from-sequences-and-alignments.html#fig:crossValidation">4.2</a>, the testing set is set aside before model training and reserved for the final model performance evaluation. The training set is then further subdivided into <span class="math inline">\(k\)</span> equally-sized subsets, called folds. Each of the <span class="math inline">\(k\)</span> folds is then used to create a validation split, the fold acting as a within-split testing set and the rest of the general training set is used as the within-split training set. This results in <span class="math inline">\(k\)</span> pairs of disjoint training and testing sets, and each example of the general training data is used exactly once in a within-split testing set. An idea of the model performance can be obtained by measuring performance on the within-split testing sets and averaging the measures. This cross-validation performance can be used to inform hyper-parameter value choice without using the reserved testing set and avoiding data leakage.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crossValidation"></span>
<img src="figures/Encode-seqs/Cross-validation.png" alt="**Example of data splits into training, testing and validation sets with 6-fold cross-validation.**  
In this setting the whole data set is first split into a training and testing set. The testing set is kept separate to assess final model performance. The training set is split into 6 folds resulting in 6 splits. In each split of the training set, the correspoding fold is used as the within-split test set (green), and the rest of the training set is used as the within-split training set (blue). You can get an idea of the model performance by averaging measures on within-fold testing sets and adjusting hyper-parameters accordingly, without using the global, reserved testing set. Adapted from the [scikit-learn website](https://scikit-learn.org/stable/modules/cross_validation.html)" width="70%" />
<p class="caption">
Figure 4.2: <strong>Example of data splits into training, testing and validation sets with 6-fold cross-validation.</strong><br />
In this setting the whole data set is first split into a training and testing set. The testing set is kept separate to assess final model performance. The training set is split into 6 folds resulting in 6 splits. In each split of the training set, the correspoding fold is used as the within-split test set (green), and the rest of the training set is used as the within-split training set (blue). You can get an idea of the model performance by averaging measures on within-fold testing sets and adjusting hyper-parameters accordingly, without using the global, reserved testing set. Adapted from the <a href="https://scikit-learn.org/stable/modules/cross_validation.html">scikit-learn website</a>
</p>
</div>
<p>This is the general setting in which a lot of the supervised learning approaches in computational biology reside, cross-validation was used to tune hyper-parameters for the models in Chapter <a href="HIV-paper.html#HIV-paper">6</a>.</p>
</div>
<div id="tests-and-statistical-learning" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Tests and statistical learning<a href="learning-from-sequences-and-alignments.html#tests-and-statistical-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Some of the simplest models possible are derived from statistics and based on probabilities. One of the simplest ways to classify data is with a statistical test, like Fisher’s exact test<span class="citation"><sup><a href="#ref-fisherInterpretationH2Contingency1922" role="doc-biblioref">379</a></sup></span> or a <span class="math inline">\(\chi^2\)</span> test<span class="citation"><sup><a href="#ref-pearsonCriterionThatGiven1900" role="doc-biblioref">380</a></sup></span> depending on the number of training examples. If one of the input variables is significantly related to the output then one can make a crude prediction on the output based solely on the value of one input variable. By testing several features and predicting the output from a set of significantly related input variables (e.g. through a vote), then the prediction can be a little more accurate. This approach is used as a baseline in the study presented in Chapter <a href="HIV-paper.html#HIV-paper">6</a>, it is however not very sophisticated and does not have the best predictive power.</p>
<p>A model that fits more squarely in the process of supervised learning described above is linear regression. This regression model assumes that the output value results from a linear combination of the input features and an intercept value. The coefficients of this linear combination and the intercept are the parameters that the models optimizes during the learning process. Often the loss function used to fit this model is the RMSE mentioned above. The gradient of the RMSE w.r.t. all the coefficients of the model is easily derived and can be used for optimization. Since this model is very simple there is an exact analytical solution to find the minium gradient value<span class="citation"><sup><a href="#ref-hastieElementsStatisticalLearning2009" role="doc-biblioref">351</a></sup></span>, however in some cases a gradient descent approach can be beneficial to train this model. This model has also been adapted to binary classification, by considering that the output value results from a linear combination of input models, passed through a logistic function. The resulting model is called logistic regression, and is one of the classifiers used in Chapter <a href="HIV-paper.html#HIV-paper">6</a>. Equations <a href="learning-from-sequences-and-alignments.html#eq:linReg">(4.1)</a> and <a href="learning-from-sequences-and-alignments.html#eq:logReg">(4.2)</a> show the mathematical model of linear and logistic regression respectively. In these equations, <span class="math inline">\(\hat{y}^{(i)}\)</span> represents the predicted output of the i<sup>th</sup> example and <span class="math inline">\(x_j^{(i)}\)</span> the j<sup>th</sup> variable of the i<sup>th</sup> example input. <span class="math inline">\(\theta_0\)</span> is the intercept and <span class="math inline">\(\theta_j\)</span> the coefficient corresponding to the j<sup>th</sup> input variable.</p>
<span class="math display" id="eq:linReg">\[\begin{equation}
  \hat{y}^{(i)} = \theta_0 + \sum_{j=1}^k \theta_j \cdot x_j^{(i)}
  \tag{4.1}
\end{equation}\]</span>
<span class="math display" id="eq:logReg">\[\begin{equation}
  \hat{y}^{(i)} = \frac{1}{
    1 + e^{
      -(\theta_0 + \sum_{j=1}^k \theta_j \cdot x_j^{(i)})
    }
  }
  \tag{4.2}
\end{equation}\]</span>
<p>The model in Equation <a href="learning-from-sequences-and-alignments.html#eq:linReg">(4.1)</a> outputs a continuous value used in regression, and the model in Equation <a href="learning-from-sequences-and-alignments.html#eq:logReg">(4.2)</a> outputs a continuous value bounded between 0 and 1, that we can consider a probability of being in one of the classes. With this probability it is easy to classify a given example in one of the two classes. It is easy to extend the logistic regression model to multiclass classification, by training several models and predicting the class with the maximal probability.</p>
<p>These linear models are simple, however can achieve good performance. They can however be prone to overfitting, this often translates by very large values for the <span class="math inline">\(\theta\)</span> coefficients. In order to counter this regularized versions of linear and logistic regression were introduced by adding the weights to the loss function in some way. By adding the coefficient values to the loss they are kept small through the optimization process, reducing the risk of overfitting. The two main regularization strategies are the ridge<span class="citation"><sup><a href="#ref-hoerlRidgeRegressionBiased1970" role="doc-biblioref">381</a></sup></span> and Lasso<span class="citation"><sup><a href="#ref-tibshiraniRegressionShrinkageSelection1996" role="doc-biblioref">382</a></sup></span> penalties.</p>
<p>The final supervised model I will present in this section is the Naive Bayes classifier. As its name indicates, it is based on Bayes’ theorem of conditional probabilities. By making a strong assumption, that all variables of the input examples are mutually independent we can derive the probability of the i<sup>th</sup> input example belonging to class <span class="math inline">\(C_{\alpha}\)</span> as:</p>
<span class="math display">\[\begin{equation}
p(C_{\alpha} | x^{(i)}_1, \ldots, x^{(i)}_k) = Z \cdot p(C_{\alpha}) \prod_{j=1}^k p(x^{(i)}_j | C_{\alpha})
\end{equation}\]</span>
<p>With <span class="math inline">\(Z\)</span> a constant that can be computed from the training data. Therefore it is very easy to use this to build a classifier by computing the probabilities of an example belonging to a class for all possible classes in the training data and assign the one with the maximal probability. In practice this is a very flexible model, since any probability distribution can be used for each feature and class, and the parameters of these distributions learned, with a maximum likelihood approach for example. This model builds upon the naive assumption (hence the name): that all input variables are mutually independent. This assumption is very often violated, especially in biological sequence data where independence is not at all guaranteed by the evolutionary process. This model is however quite robust to this, and stays quite performant despite the violations of this assumption<span class="citation"><sup><a href="#ref-zhangOptimalityNaiveBayes" role="doc-biblioref">383</a>,<a href="#ref-rishEmpiricalStudyNaive" role="doc-biblioref">384</a></sup></span>.</p>
</div>
<div id="more-complex-methods" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> More complex methods<a href="learning-from-sequences-and-alignments.html#more-complex-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While these simple methods are quite useful in many settings, more complex methods were developed. One of the most popular methods up until recently were Support Vector Machines (SVM). This classifier was first developed in 1982<span class="citation"><sup><a href="#ref-vapnikEstimationDependencesBased1982" role="doc-biblioref">385</a></sup></span> and it functions by finding the optimal separation hyperplane between 2 classes, i.e. a linear boundary in high-dimensional space between training examples of two classes. What made it so popular is when it was associated with the so-called <em>kernel trick</em><span class="citation"><sup><a href="#ref-boserTrainingAlgorithmOptimal1992" role="doc-biblioref">386</a>,<a href="#ref-cortesSupportvectorNetworks1995" role="doc-biblioref">387</a></sup></span>. With the kernel trick, training examples that cannot be linearly separated can be cheaply projected into a higher dimensional space where linear separation is possible. This made SVMs very powerful and popular, and it was quickly adapted for regression tasks as well<span class="citation"><sup><a href="#ref-druckerSupportVectorRegression1996" role="doc-biblioref">388</a></sup></span>. The main model that will interest us in this section however is not the SVM.</p>
<p>Random Forests are another very popular model used for both classification and regression, as it is used in Chapter <a href="HIV-paper.html#HIV-paper">6</a> we will go over it in more detail. Developed in the early 2000’s<span class="citation"><sup><a href="#ref-breimanRandomForests2001" role="doc-biblioref">389</a></sup></span>, it builds upon previous work: Classification And Regression Trees (CART)<span class="citation"><sup><a href="#ref-breimanClassificationRegressionTrees1983" role="doc-biblioref">390</a></sup></span>. CART Decision trees are very useful for both supervised learning tasks. To use CART trees, start at the root and at each node there is a condition on a single input feature. This condition decides if the considered example goes to the right child or the left child. By traversing this tree, choosing the path through the conditions at all the nodes met by the example, we can assign the example to one of the leaves, corresponding to a class or a predicted value. An example of such a tree is given in Figure <a href="learning-from-sequences-and-alignments.html#fig:cart">4.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart"></span>
<img src="figures/Encode-seqs/cart.png" alt="**Example of a decision tree on DNA data.**   
Here we have a dataset of 4 DNA sequences $S_1, \ldots, S_4$. Each sequence has 4 input variables $x_1,\ldots,x_4$, and an output variable $y$ indicating the strain of a sequence. Each sequence can be classified by the decision tree on the right by following a path from root to leaf, according to the conditions in internal nodes. The predicted strain is shown in the leaves. Sequences that end up in a given leaf node are indicated underneath that leaf node. This tree can classify these 4 sequences perfectly." width="80%" />
<p class="caption">
Figure 4.3: <strong>Example of a decision tree on DNA data.</strong><br />
Here we have a dataset of 4 DNA sequences <span class="math inline">\(S_1, \ldots, S_4\)</span>. Each sequence has 4 input variables <span class="math inline">\(x_1,\ldots,x_4\)</span>, and an output variable <span class="math inline">\(y\)</span> indicating the strain of a sequence. Each sequence can be classified by the decision tree on the right by following a path from root to leaf, according to the conditions in internal nodes. The predicted strain is shown in the leaves. Sequences that end up in a given leaf node are indicated underneath that leaf node. This tree can classify these 4 sequences perfectly.
</p>
</div>
<p>It is actually quite simple to build these CART trees, the whole methods lies upon the principle of minimizing impurity <em>(or maximizing purity)</em> on a given input variable in child nodes. Impurity can be defined in many ways<span class="citation"><sup><a href="#ref-hastieElementsStatisticalLearning2009" role="doc-biblioref">351</a></sup></span>: for regression it is often the Residual Sum of Squares (RSS), for classification it is often the Gini index or an entropy measure. Regardless of the chosen metric, a high impurity denotes a heterogeneous collection of examples and a low impurity indicates a homogeneous set of examples. When building the tree, recursively from the root, we find the condition and the input variable on which the condition relies by looking at all possible splits and choosing the one that decreases impurity the most in the child nodes. This process is continued recursively until the leaves are completely pure (likely resulting in overfitting) or until a certain stopping condition is met <em>(e.g. purity threshold, maximum depth, …)</em>. To avoid overfitting, trees can also be pruned after the building phase. CART trees have the distinct advantage of being interpretable: it is easy to figure out <em>why</em> an input has been assigned to a certain class, which can be very useful in biology or medicine<span class="citation"><sup><a href="#ref-kingsfordWhatAreDecision2008" role="doc-biblioref">391</a></sup></span>.</p>
<p>Despite these good properties, it is easy to overfit with decision trees, and small changes to the training data can induce large changes in the resulting tree<span class="citation"><sup><a href="#ref-hastieElementsStatisticalLearning2009" role="doc-biblioref">351</a>,<a href="#ref-kingsfordWhatAreDecision2008" role="doc-biblioref">391</a></sup></span>, hurting interpretability. This is why the Random Forest (RF) model was created. RFs are essentially an ensemble of decision tress built, <em>a forest</em>, from the training data. To build one of the decision trees in a random forest, the training data is first bootstrapped: a new training set is sampled with replacement from the original training data with the same number of examples. This process is called <em>bagging</em> for “bootstrap aggregating”. With this procedure, each decision tree is built from slightly different training data and will therefore likely be slightly different. An additional step to ensure some variability between the trees is in the choice of the splitting condition at each tree node. Where in CART trees all input variables are considered to find the optimal split, in RF trees, only a random subset of the input variables are considered at each node. This results in a set of decision trees that are all trained from slightly different data, with slightly different features at each node but that all have the same task on the training data. We can get a prediction from all these trees, by taking the majority predicted class for classification trees, or the average of predictions for regression trees.</p>
<p>All these measures to reduce the variance linked to decision trees, and to yield more generalizable models, make random forests very popular. They are often very competitive and often have better performance than the models presented above<span class="citation"><sup><a href="#ref-caruanaEmpiricalComparisonSupervised2006a" role="doc-biblioref">392</a>,<a href="#ref-yangReviewEnsembleMethods2010" role="doc-biblioref">393</a></sup></span>. Furthermore, by only considering a subset of features at each tree node, RF often deals better with high-dimensional data than other methods<span class="citation"><sup><a href="#ref-yangReviewEnsembleMethods2010" role="doc-biblioref">393</a></sup></span>. Further refinements to the algorithm such as boosting, where misclassified examples are more likely to be selected in the bagging training sets have been very useful as well.</p>
<p>Deep learning has been use more frequently and more broadly to get good results across a large number of tasks. This is also true in biological contexts, however Chapter <a href="HIV-paper.html#HIV-paper">6</a> does not make use of deep learning methods so they will no be discussed here. A short introduction to deep learning will however be presented in Chapter <a href="learning-alignments-an-interesting-perspective.html#learning-alignments-an-interesting-perspective">7</a>.</p>
</div>
</div>
<div id="preprocessing-the-alignment-for-machine-learning" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Preprocessing the alignment for machine learning<a href="learning-from-sequences-and-alignments.html#preprocessing-the-alignment-for-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By now you will surely have noticed that all the models I presented above (with the exception of RFs) need to be trained on a collection of numerical variables, i.e. numerical vectors. Biological sequences, however, are not vectors of numbers. We therefore need to transform our sequences of letters into numerical sequences that we can feed to the machine learning model in this digestible form. Most supervised machine learning algorithms expect as training inputs a matrix, where the rows are individual training examples and the columns numerical variables, an a vector where each entry corresponds to the expected output value. In this section I will present a few encoding methods, that transform a multiple sequence alignment in a matrix. Most of the encoding methods are not defined on an alignment, but on sequences alone. However to represent these sequences they must have the same length, and for models to learn anything meaningful the values in features should encode the same information across sequences. Therefore prior to the encoding methods described below the sequences must be aligned to each other so that a specific position in a sequence is homologous to that position in all other training sequences.</p>
<div id="general-purpose-encodings" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> General purpose encodings<a href="learning-from-sequences-and-alignments.html#general-purpose-encodings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The letters making up biological sequences are a form of categorical data, this type of variable is not specific to biology and as such there exists many encoding schemes<span class="citation"><sup><a href="#ref-potdarComparativeStudyCategorical2017" role="doc-biblioref">394</a></sup></span> to transform categorical variables into numerical vectors.</p>
<p>The most basic and conceptually simple ways to do so is to use the labeling scheme, often called ordinal encoding, where each level of the categorical variable is assigned an integer label. For example, when dealing with DNA sequences, we could have A=1, C=2, G=3 and T=4. This scheme outputs vectors that have the same size as the input sequence and going from the sequence to the encoded vector (and vice versa) is very easy. This encoding scheme has been used to predict resistance levels of HIV to antiviral drugs from sequencing data<span class="citation"><sup><a href="#ref-steinerDrugResistancePrediction2020a" role="doc-biblioref">329</a></sup></span>. There is however a major disadvantage with using this method, as its name indicates, using ordinal encoding implies that there is an order to the categorical variable levels (e.g. T&gt;A) which, by definition, does not exist<span class="citation"><sup><a href="#ref-hassanisaadiInterpretiveTimefrequencyAnalysis2017" role="doc-biblioref">395</a>–<a href="#ref-kunanbayevComplexEncoding2021" role="doc-biblioref">397</a></sup></span>. Another option is to use what I will refer to as binary labeling, where the categorical levels are first assigned an integer label which is then converted to a binary vector. If we use the ordinal DNA encoding from above and convert it to binary vectors we would get: A=<span class="math inline">\([0,0]\)</span>, C=<span class="math inline">\([0,1]\)</span>, G=<span class="math inline">\([1,0]\)</span> and T=<span class="math inline">\([1,1]\)</span>. This type of representation is frequently used to represent gapless sequences, like <span class="math inline">\(k\)</span>-mers, in a compressed form<span class="citation"><sup><a href="#ref-dufresneKmerFileFormat2022" role="doc-biblioref">398</a>,<a href="#ref-wrightUsingDECIPHERV22016" role="doc-biblioref">399</a></sup></span> (a letdoodter now only needs 2 bits instead of a full byte). For amino acids, since there are more characters, this encoding yields vectors of 5 bits<span class="citation"><sup><a href="#ref-zamaniAminoAcidEncoding2011" role="doc-biblioref">400</a></sup></span>. Fundamentally, this encoding scheme has the same problem as the ordinal encoding, creating an order that does not exist, although with th eorder being split into separate values it can mitigate the effects a little bit.</p>
<p>On of the most widely used categorical encoding schemes, One-Hot encoding (OHE) (sometimes called orthonormal encoding<span class="citation"><sup><a href="#ref-singhEvolutionaryBasedOptimal2018" role="doc-biblioref">401</a></sup></span>), does not have this ordering issue. The way OHE works is by creating a sparse binary vector of length <span class="math inline">\(d\)</span> to represent a variable with <span class="math inline">\(d\)</span> levels <em>(for DNA</em> <span class="math inline">\(d=4\)</span>). If the i<sup>th</sup> level of the categorical variable is to be encoded, then the i<sup>th</sup> position in the vector is set to 1 and the rest set to 0. For example if we consider that A is the first level of our variable then OHE would yield the following vector: <span class="math inline">\([1,0,0,0]\)</span>. This encoding scheme has been used from the 1980’s<span class="citation"><sup><a href="#ref-qianPredictingSecondaryStructure1988" role="doc-biblioref">402</a></sup></span> to now<span class="citation"><sup><a href="#ref-budachPyssterClassificationBiological2018" role="doc-biblioref">403</a></sup></span>, and is the scheme used in Chapter <a href="HIV-paper.html#HIV-paper">6</a>. The performance of OHE can be on par with ordinal encoding<span class="citation"><sup><a href="#ref-choongEvaluationConvolutionaryNeural2017" role="doc-biblioref">404</a></sup></span>, however it is easily interpretable, which is often very important in biology, since there is a one to one correspondence between a categorical value and a numerical feature. The main problem with OHE however is that it tends to increase the number of features quite a lot, since the encoded vector representation of a length <span class="math inline">\(n\)</span> sequence is of length <span class="math inline">\(n\times d\)</span>. An example comparing Ordinal, Binary and One-Hot encodings can be seen in Figure <a href="learning-from-sequences-and-alignments.html#fig:generalEncoding">4.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:generalEncoding"></span>
<img src="figures/Encode-seqs/General-purpose.png" alt="**Example of 3 general categorical encoding schemes.**  
Two sequences, `ATCG` and `TAAT` are shown encoded in three different encoding schemes: ordinal, binary and one-hot encoding. In the ordinal encoding each character is assigned an integer value, here A=0, C=1, G=3 and T=4. In the binary encoding these integer values are encoded with 2 bits. In the one-hot encoding scheme, a character corresponds to a sparse vector indicating which level of the variable is present: here A=[1,0,0,0]. Ordinal encoding preserves the dimension of the sequence while binary and one-hot encoding result in vectors woth a bigger dimension than the original sequence. 
" width="100%" />
<p class="caption">
Figure 4.4: <strong>Example of 3 general categorical encoding schemes.</strong><br />
Two sequences, <code>ATCG</code> and <code>TAAT</code> are shown encoded in three different encoding schemes: ordinal, binary and one-hot encoding. In the ordinal encoding each character is assigned an integer value, here A=0, C=1, G=3 and T=4. In the binary encoding these integer values are encoded with 2 bits. In the one-hot encoding scheme, a character corresponds to a sparse vector indicating which level of the variable is present: here A=[1,0,0,0]. Ordinal encoding preserves the dimension of the sequence while binary and one-hot encoding result in vectors woth a bigger dimension than the original sequence.
</p>
</div>
<p>These three general purpose encodings are but some of many<span class="citation"><sup><a href="#ref-potdarComparativeStudyCategorical2017" role="doc-biblioref">394</a></sup></span>, and since categorical variables are often used in machine learning applications, these encodings are available in widely used software libraries<span class="citation"><sup><a href="#ref-mcginnisScikitLearnContribCategoricalEncodingRelease2018" role="doc-biblioref">405</a></sup></span>.</p>
</div>
<div id="biological-sequence-specific-encodings" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Biological sequence-specific encodings<a href="learning-from-sequences-and-alignments.html#biological-sequence-specific-encodings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While the general-purpose encoding schemes presented above work well enough in practice, some specific encoding methods were developed to include some biological information in the sequence encodings that hopefully machine learning models will be able to leverage during training. These encodings, have mostly been developed for encoding protein sequences, using physicochemical properties of amino acids<span class="citation"><sup><a href="#ref-zamaniAminoAcidEncoding2011" role="doc-biblioref">400</a></sup></span>.</p>
<p>AAIndex<span class="citation"><sup><a href="#ref-kawashima2008" role="doc-biblioref">406</a></sup></span> is a public database containing amino acid indices, <em>i.e.</em> sets of 20 numerical values (one for each AA) measuring some physicochemical property. There is a wide range of 566 of these indices, from hydrophobicity to flexibility or residue volume measures. By selecting an informative subset of 9 of these measures<span class="citation"><sup><a href="#ref-liPredictionProteinStructural2008" role="doc-biblioref">407</a></sup></span>, an amino acid can be represented by a length 9 numerical vector. In some cases, amino acids can be represented by all the 566 properties of AAIndex, and through PCA the dimension of the resulting numerical vectors can be reduced<span class="citation"><sup><a href="#ref-nanniNewEncodingTechnique2011" role="doc-biblioref">408</a></sup></span>. This biological sequence specific encoding has been implemented in a software library for biological sequence encoding<span class="citation"><sup><a href="#ref-chenIFeaturePythonPackage2018" role="doc-biblioref">409</a></sup></span>.</p>
<p>Another biological sequence-specific encoding is based on the Amino Acid classification Venn diagram defined by Taylor in 1986<span class="citation"><sup><a href="#ref-taylorClassificationAminoAcid1986" role="doc-biblioref">410</a></sup></span>, which groups amino acids into eight different groups based on physicochemical properties: aliphatic, aromatic, hydrophobic, polar, charged, positive, small and tiny . With this classification, a single amino acid can be represented by a vector of length 8, each element representing a group, set to one when the amino acid belongs to the group and to zero when it does not. This encoding methods was used as early as 1987 to predict secondary structures of proteins<span class="citation"><sup><a href="#ref-zvelebilPredictionProteinSecondary1987" role="doc-biblioref">411</a></sup></span>. Later on another five groups were proposed and used to encode each amino acid with longer vectors<span class="citation"><sup><a href="#ref-kremerMethodSystemComputer2009" role="doc-biblioref">412</a></sup></span>.</p>
<p>A third encoding method, named BLOMAP<span class="citation"><sup><a href="#ref-maetschkeBlomapEncodingAmino2005" role="doc-biblioref">413</a></sup></span>, encodes sequences based on values from the BLOSUM62 substitution matrix presented in Section <a href="aligning-sequence-data.html#scoring-and-substitution-models">2.1.3</a>. BLOMAP is defined by using a non-linear projection algorithm to generate vectors of length five, that capture the similarity measures contained in BLOSUM62. This encoding has been used to successfully predict cleavage sites of the HIV-1 protease<span class="citation"><sup><a href="#ref-singhEvolutionaryBasedOptimal2018" role="doc-biblioref">401</a>,<a href="#ref-maetschkeBlomapEncodingAmino2005" role="doc-biblioref">413</a></sup></span> <em>(c.f. Section</em> <a href="viruses-hiv-and-drug-resistance.html#protease">5.3.2.2</a>). Other encodings such as OETMAP<span class="citation"><sup><a href="#ref-gokNewFeatureEncoding2013" role="doc-biblioref">414</a></sup></span> have been derived from BLOMAP.</p>
<p>These three encodings are far from being the only ones specific to biological sequence. Many other encoding schemes were developed to learn from this type of sequence data. Some schemes do not encode positional data, and as such can be applied to unaligned sequences. The simplest of these would be to represent a sequence by its amino acid, or <span class="math inline">\(k\)</span>-mer frequencies. The latter, is often referred to as <span class="math inline">\(n\)</span>-gram encoding<span class="citation"><sup><a href="#ref-sahaNovelApproachFind2019" role="doc-biblioref">415</a></sup></span> and widely used although with very short <span class="math inline">\(k\)</span>-mers since the dimension of the encoding grows exponentially with <span class="math inline">\(k\)</span>, with 20 amino acids this encoding results in vectors that have a length of <span class="math inline">\(20^k\)</span>. Other encoding schemes use codon information to encode amino acids. One such scheme was proposed in<span class="citation"><sup><a href="#ref-zamaniAminoAcidEncoding2011" role="doc-biblioref">400</a></sup></span>, where an amino acid is represented by a directed graph where vertices are nucleotides and edges represent paths needed to represent codons that code for that amino acid. This graph can then be converted to a 16-dimensional vector by flattening the corresponding adjacency matrix, and be used as an encoding method.</p>
<p>During the work that led to Chapter <a href="HIV-paper.html#HIV-paper">6</a>, several encoding methods were tested: Ordinal, Binary, OHE, AAIndex and Group encodings. The same two training sets of sequences were encoded using each of these methods, and 10 RF models were trained on each of the encoded datasets. Accuracy, precision and recall metrics were used to evaluate the performance of the RF on each encoded dataset. According to these metrics, the RF model had the best performance on the datasets encoded with OHE. OHE, also has the advantage of being easily interpretable, as such it was chosen for the work presented in Chapter <a href="HIV-paper.html#HIV-paper">6</a>.</p>
<p>Other encodings have been used to convert a biological sequence into a single real value. This encoding method based on chaos game theory<span class="citation"><sup><a href="#ref-jeffreyChaosGameRepresentation1990" role="doc-biblioref">416</a></sup></span> allows for a bijective mapping between the DNA sequence set and the Real numbers set. This encoding is not specific to alignments and can be used to do alignment-free comparisons, as such it has been used often in bioinformatics applications<span class="citation"><sup><a href="#ref-lochelChaosGameRepresentation2021" role="doc-biblioref">417</a></sup></span>. Recently, this encoding scheme has been used to classify SARS-CoV2 sequences<span class="citation"><sup><a href="#ref-cartesAccurateFastClade2022" role="doc-biblioref">418</a></sup></span>, predict anti-microbial resistance from sequence data<span class="citation"><sup><a href="#ref-renPredictionAntimicrobialResistance2022" role="doc-biblioref">343</a></sup></span> and for phylogenetic analysis<span class="citation"><sup><a href="#ref-niApplyingFrequencyChaos2021" role="doc-biblioref">419</a></sup></span>.</p>
<p>In recent years algorithmic developments, computing power increase and the massive amounts of available data have made deep learning methods useful, possible to train and veru popular. This has given rise to new sequence encoding methods, that are learned on the training data. These are often referred to as embeddings rather than encodings. Since these learned embeddings are not used in Chapter <a href="HIV-paper.html#HIV-paper">6</a>, for the sake of thematical coherence I will not be mentioning them here. I will however go over these embedding methods, as well as the deep learning framework, shortly in Chapter <a href="learning-alignments-an-interesting-perspective.html#learning-alignments-an-interesting-perspective">7</a>.</p>
</div>
</div>
<div id="conclusion-2" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Conclusion<a href="learning-from-sequences-and-alignments.html#conclusion-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Alignments, and the sequences in them are a rich source of information, that have long been exploited widely for many different types of analyses. With the rise of machine learning in the last years, it is logical that machine learning models have been applied more and more frequently to biological sequence data. Machine Learning, is a wide field with many different methods and paradigms. Even simple methods like linear regression or Naive Bayes can be very useful, and more complex models like Random Forests have been able to make very good predictions on biological data. The model of choice, however, is not the only variable to take into account when looking to apply machine learning method on sequence data. Different encoding methods will yield different vector representations, with different characteristics and applications. Special care must therefore be given to the choice of biological sequence encoding scheme, prior to starting a machine learning analysis.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" line-spacing="2">
<div id="ref-sahlinNovoClusteringLongRead2020" class="csl-entry">
<div class="csl-left-margin">115. </div><div class="csl-right-inline">Sahlin, K. &amp; Medvedev, P. <a href="https://doi.org/10.1089/cmb.2019.0299">De <span>Novo Clustering</span> of <span>Long-Read Transcriptome Data Using</span> a <span>Greedy</span>, <span>Quality Value-Based Algorithm</span></a>. <em>Journal of Computational Biology</em> <strong>27</strong>, 472–484 (2020).</div>
</div>
<div id="ref-jumperHighlyAccurateProtein2021" class="csl-entry">
<div class="csl-left-margin">136. </div><div class="csl-right-inline">Jumper, J. <em>et al.</em> <a href="https://doi.org/10.1038/s41586-021-03819-2">Highly accurate protein structure prediction with AlphaFold</a>. <em>Nature</em> <strong>596</strong>, 583–589 (2021).</div>
</div>
<div id="ref-martinNextgenerationTranscriptomeAssembly2011" class="csl-entry">
<div class="csl-left-margin">317. </div><div class="csl-right-inline">Martin, J. A. &amp; Wang, Z. <a href="https://doi.org/10.1038/nrg3068">Next-generation transcriptome assembly</a>. <em>Nature Reviews Genetics</em> <strong>12</strong>, 671–682 (2011).</div>
</div>
<div id="ref-kyriakidouCurrentStrategiesPolyploid2018" class="csl-entry">
<div class="csl-left-margin">318. </div><div class="csl-right-inline">Kyriakidou, M., Tai, H. H., Anglin, N. L., Ellis, D. &amp; Strömvik, M. V. <a href="https://www.frontiersin.org/articles/10.3389/fpls.2018.01660">Current strategies of polyploid plant genome sequence assembly</a>. <em>Frontiers in Plant Science</em> <strong>9</strong>, (2018).</div>
</div>
<div id="ref-paszkiewiczNovoAssemblyShort2010" class="csl-entry">
<div class="csl-left-margin">319. </div><div class="csl-right-inline">Paszkiewicz, K. &amp; Studholme, D. J. <a href="https://doi.org/10.1093/bib/bbq020">De novo assembly of short sequence reads</a>. <em>Briefings in Bioinformatics</em> <strong>11</strong>, 457–472 (2010).</div>
</div>
<div id="ref-sohnPresentFutureNovo2018" class="csl-entry">
<div class="csl-left-margin">320. </div><div class="csl-right-inline">Sohn, J. &amp; Nam, J.-W. <a href="https://doi.org/10.1093/bib/bbw096">The present and future of de novo whole-genome assembly</a>. <em>Briefings in Bioinformatics</em> <strong>19</strong>, 23–40 (2018).</div>
</div>
<div id="ref-sleatorOverviewSilicoProtein2010" class="csl-entry">
<div class="csl-left-margin">321. </div><div class="csl-right-inline">Sleator, R. D. &amp; Walsh, P. <a href="https://doi.org/10.1007/s00203-010-0549-9">An overview of in silico protein function prediction</a>. <em>Archives of Microbiology</em> <strong>192</strong>, 151–155 (2010).</div>
</div>
<div id="ref-koboldtBestPracticesVariant2020" class="csl-entry">
<div class="csl-left-margin">322. </div><div class="csl-right-inline">Koboldt, D. C. <a href="https://doi.org/10.1186/s13073-020-00791-w">Best practices for variant calling in clinical sequencing</a>. <em>Genome Medicine</em> <strong>12</strong>, 91 (2020).</div>
</div>
<div id="ref-alkanGenomeStructuralVariation2011" class="csl-entry">
<div class="csl-left-margin">323. </div><div class="csl-right-inline">Alkan, C., Coe, B. P. &amp; Eichler, E. E. <a href="https://doi.org/10.1038/nrg2958">Genome structural variation discovery and genotyping</a>. <em>Nature Reviews Genetics</em> <strong>12</strong>, 363–376 (2011).</div>
</div>
<div id="ref-hoStructuralVariationSequencing2020" class="csl-entry">
<div class="csl-left-margin">324. </div><div class="csl-right-inline">Ho, S. S., Urban, A. E. &amp; Mills, R. E. <a href="https://doi.org/10.1038/s41576-019-0180-9">Structural variation in the sequencing era</a>. <em>Nature Reviews Genetics</em> <strong>21</strong>, 171–189 (2020).</div>
</div>
<div id="ref-morrisonPhylogeneticTreebuilding1996" class="csl-entry">
<div class="csl-left-margin">325. </div><div class="csl-right-inline">Morrison, D. A. <a href="https://doi.org/10.1016/0020-7519(96)00044-6">Phylogenetic tree-building</a>. <em>International Journal for Parasitology</em> <strong>26</strong>, 589–617 (1996).</div>
</div>
<div id="ref-kapliPhylogeneticTreeBuilding2020" class="csl-entry">
<div class="csl-left-margin">326. </div><div class="csl-right-inline">Kapli, P., Yang, Z. &amp; Telford, M. J. <a href="https://doi.org/10.1038/s41576-020-0233-0">Phylogenetic tree building in the genomic age</a>. <em>Nature Reviews Genetics</em> <strong>21</strong>, 428–444 (2020).</div>
</div>
<div id="ref-kuhlmanAdvancesProteinStructure2019" class="csl-entry">
<div class="csl-left-margin">327. </div><div class="csl-right-inline">Kuhlman, B. &amp; Bradley, P. <a href="https://doi.org/10.1038/s41580-019-0163-x">Advances in protein structure prediction and design</a>. <em>Nature Reviews Molecular Cell Biology</em> <strong>20</strong>, 681–697 (2019).</div>
</div>
<div id="ref-ammad-ud-dinSystematicIdentificationFeature2017" class="csl-entry">
<div class="csl-left-margin">328. </div><div class="csl-right-inline">Ammad-ud-din, M., Khan, S. A., Wennerberg, K. &amp; Aittokallio, T. <a href="https://doi.org/10.1093/bioinformatics/btx266">Systematic identification of feature combinations for predicting drug response with bayesian multi-view multi-task linear regression</a>. <em>Bioinformatics</em> <strong>33</strong>, i359–i368 (2017).</div>
</div>
<div id="ref-steinerDrugResistancePrediction2020a" class="csl-entry">
<div class="csl-left-margin">329. </div><div class="csl-right-inline">Steiner, M. C., Gibson, K. M. &amp; Crandall, K. A. <a href="https://doi.org/10.3390/v12050560">Drug <span>Resistance Prediction Using Deep Learning Techniques</span> on <span>HIV</span>-1 <span>Sequence Data</span></a>. <em>Viruses</em> <strong>12</strong>, 560 (2020).</div>
</div>
<div id="ref-noeMachineLearningProtein2020" class="csl-entry">
<div class="csl-left-margin">330. </div><div class="csl-right-inline">Noé, F., De Fabritiis, G. &amp; Clementi, C. <a href="https://doi.org/10.1016/j.sbi.2019.12.005">Machine learning for protein folding and dynamics</a>. <em>Current Opinion in Structural Biology</em> <strong>60</strong>, 77–84 (2020).</div>
</div>
<div id="ref-alquraishiMachineLearningProtein2021" class="csl-entry">
<div class="csl-left-margin">334. </div><div class="csl-right-inline">AlQuraishi, M. <a href="https://doi.org/10.1016/j.cbpa.2021.04.005">Machine learning in protein structure prediction</a>. <em>Current Opinion in Chemical Biology</em> <strong>65</strong>, 1–8 (2021).</div>
</div>
<div id="ref-wittmannAdvancesMachineLearning2021" class="csl-entry">
<div class="csl-left-margin">335. </div><div class="csl-right-inline">Wittmann, B. J., Johnston, K. E., Wu, Z. &amp; Arnold, F. H. <a href="https://doi.org/10.1016/j.sbi.2021.01.008">Advances in machine learning for directed evolution</a>. <em>Current Opinion in Structural Biology</em> <strong>69</strong>, 11–18 (2021).</div>
</div>
<div id="ref-liCanMachineLearning2019" class="csl-entry">
<div class="csl-left-margin">337. </div><div class="csl-right-inline">Li, G., Dong, Y. &amp; Reetz, M. T. <a href="https://doi.org/10.1002/adsc.201900149">Can Machine Learning Revolutionize Directed Evolution of Selective Enzymes?</a> <em>Advanced Synthesis &amp; Catalysis</em> <strong>361</strong>, 2377–2386 (2019).</div>
</div>
<div id="ref-xieDeepAutoencoderModel2017" class="csl-entry">
<div class="csl-left-margin">338. </div><div class="csl-right-inline">Xie, R., Wen, J., Quitadamo, A., Cheng, J. &amp; Shi, X. <a href="https://doi.org/10.1186/s12864-017-4226-0">A deep auto-encoder model for gene expression prediction</a>. <em>BMC Genomics</em> <strong>18</strong>, 845 (2017).</div>
</div>
<div id="ref-ortunoComparingDifferentMachine2015" class="csl-entry">
<div class="csl-left-margin">339. </div><div class="csl-right-inline">Ortuño, F. M. <em>et al.</em> <a href="https://doi.org/10.1016/j.neucom.2015.01.080">Comparing different machine learning and mathematical regression models to evaluate multiple sequence alignments</a>. <em>Neurocomputing</em> <strong>164</strong>, 123–136 (2015).</div>
</div>
<div id="ref-wangAccurateNovoPrediction2017" class="csl-entry">
<div class="csl-left-margin">340. </div><div class="csl-right-inline">Wang, S., Sun, S., Li, Z., Zhang, R. &amp; Xu, J. <a href="https://doi.org/10.1371/journal.pcbi.1005324">Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model</a>. <em>PLOS Computational Biology</em> <strong>13</strong>, e1005324 (2017).</div>
</div>
<div id="ref-hagaMachineLearningbasedTreatment2020" class="csl-entry">
<div class="csl-left-margin">341. </div><div class="csl-right-inline">Haga, H. <em>et al.</em> <a href="https://doi.org/10.1371/journal.pone.0242028">A machine learning-based treatment prediction model using whole genome variants of hepatitis C virus</a>. <em>PLOS ONE</em> <strong>15</strong>, e0242028 (2020).</div>
</div>
<div id="ref-zazziPredictingResponseAntiretroviral2012" class="csl-entry">
<div class="csl-left-margin">342. </div><div class="csl-right-inline">Zazzi, M. <em>et al.</em> <a href="https://doi.org/10.1159/000332008">Predicting response to antiretroviral treatment by machine learning: The EuResist project</a>. <em>Intervirology</em> <strong>55</strong>, 123–127 (2012).</div>
</div>
<div id="ref-renPredictionAntimicrobialResistance2022" class="csl-entry">
<div class="csl-left-margin">343. </div><div class="csl-right-inline">Ren, Y. <em>et al.</em> <a href="https://doi.org/10.1093/bioinformatics/btab681">Prediction of antimicrobial resistance based on whole-genome sequencing and machine learning</a>. <em>Bioinformatics</em> <strong>38</strong>, 325–334 (2022).</div>
</div>
<div id="ref-kimMachineLearningAntimicrobial2022" class="csl-entry">
<div class="csl-left-margin">344. </div><div class="csl-right-inline">Kim, J. I. <em>et al.</em> <a href="https://doi.org/10.1128/cmr.00179-21">Machine learning for antimicrobial resistance prediction: Current practice, limitations, and clinical perspective</a>. <em>Clinical Microbiology Reviews</em> <strong>0</strong>, e00179–21 (2022).</div>
</div>
<div id="ref-wangPredictingDNAMethylation2016" class="csl-entry">
<div class="csl-left-margin">345. </div><div class="csl-right-inline">Wang, Y. <em>et al.</em> <a href="https://doi.org/10.1038/srep19598">Predicting DNA Methylation State of CpG Dinucleotide Using Genome Topological Features and Deep Networks</a>. <em>Scientific Reports</em> <strong>6</strong>, 19598 (2016).</div>
</div>
<div id="ref-ratschLearningInterpretableSVMs2006" class="csl-entry">
<div class="csl-left-margin">346. </div><div class="csl-right-inline">Rätsch, G., Sonnenburg, S. &amp; Schäfer, C. <a href="https://doi.org/10.1186/1471-2105-7-S1-S9">Learning Interpretable SVMs for Biological Sequence Classification</a>. <em>BMC Bioinformatics</em> <strong>7</strong>, S9 (2006).</div>
</div>
<div id="ref-jonesProteinSecondaryStructure1999" class="csl-entry">
<div class="csl-left-margin">347. </div><div class="csl-right-inline">Jones, D. T. <a href="https://doi.org/10.1006/jmbi.1999.3091">Protein secondary structure prediction based on position-specific scoring matrices</a>. <em>Journal of Molecular Biology</em> <strong>292</strong>, 195–202 (1999).</div>
</div>
<div id="ref-weiPredictionHumanProtein2018" class="csl-entry">
<div class="csl-left-margin">348. </div><div class="csl-right-inline">Wei, L., Ding, Y., Su, R., Tang, J. &amp; Zou, Q. <a href="https://doi.org/10.1016/j.jpdc.2017.08.009">Prediction of human protein subcellular localization using deep learning</a>. <em>Journal of Parallel and Distributed Computing</em> <strong>117</strong>, 212–217 (2018).</div>
</div>
<div id="ref-wangProteinSequenceProtein2017" class="csl-entry">
<div class="csl-left-margin">349. </div><div class="csl-right-inline">Wang, H., Yan, L., Huang, H. &amp; Ding, C. <a href="https://doi.org/10.1109/TCBB.2016.2591529">From protein sequence to protein function via multi-label linear discriminant analysis</a>. <em>IEEE/ACM Transactions on Computational Biology and Bioinformatics</em> <strong>14</strong>, 503–513 (2017).</div>
</div>
<div id="ref-kelleyBassetLearningRegulatory2016" class="csl-entry">
<div class="csl-left-margin">350. </div><div class="csl-right-inline">Kelley, D. R., Snoek, J. &amp; Rinn, J. L. <a href="https://doi.org/10.1101/gr.200535.115">Basset: Learning the regulatory code of the accessible genome with deep convolutional neural networks</a>. <em>Genome Research</em> <strong>26</strong>, 990–999 (2016).</div>
</div>
<div id="ref-hastieElementsStatisticalLearning2009" class="csl-entry">
<div class="csl-left-margin">351. </div><div class="csl-right-inline">Hastie, T., Tibshirani, R. &amp; Friedman, J. <em>The <span>Elements</span> of <span>Statistical Learning</span>: <span>Data Mining</span>, <span>Inference</span>, and <span>Prediction</span>, <span>Second Edition</span></em>. (<span>Springer Science &amp; Business Media</span>, 2009).</div>
</div>
<div id="ref-kriventsevaClusteringAnalysisProtein2001" class="csl-entry">
<div class="csl-left-margin">352. </div><div class="csl-right-inline">Kriventseva, E. V., Biswas, M. &amp; Apweiler, R. <a href="https://doi.org/10.1016/S0959-440X(00)00211-6">Clustering and analysis of protein families</a>. <em>Current Opinion in Structural Biology</em> <strong>11</strong>, 334–339 (2001).</div>
</div>
<div id="ref-fuCDHITAcceleratedClustering2012" class="csl-entry">
<div class="csl-left-margin">353. </div><div class="csl-right-inline">Fu, L., Niu, B., Zhu, Z., Wu, S. &amp; Li, W. <a href="https://doi.org/10.1093/bioinformatics/bts565">CD-HIT: Accelerated for clustering the next-generation sequencing data</a>. <em>Bioinformatics</em> <strong>28</strong>, 3150–3152 (2012).</div>
</div>
<div id="ref-balabanTreeClusterClusteringBiological2019" class="csl-entry">
<div class="csl-left-margin">354. </div><div class="csl-right-inline">Balaban, M., Moshiri, N., Mai, U., Jia, X. &amp; Mirarab, S. <a href="https://doi.org/10.1371/journal.pone.0221068">TreeCluster: Clustering biological sequences using phylogenetic trees</a>. <em>PLOS ONE</em> <strong>14</strong>, e0221068 (2019).</div>
</div>
<div id="ref-zoritaStarcodeSequenceClustering2015" class="csl-entry">
<div class="csl-left-margin">355. </div><div class="csl-right-inline">Zorita, E., Cuscó, P. &amp; Filion, G. J. <a href="https://doi.org/10.1093/bioinformatics/btv053">Starcode: Sequence clustering based on all-pairs search</a>. <em>Bioinformatics</em> <strong>31</strong>, 1913–1919 (2015).</div>
</div>
<div id="ref-ondovMashFastGenome2016" class="csl-entry">
<div class="csl-left-margin">356. </div><div class="csl-right-inline">Ondov, B. D. <em>et al.</em> <a href="https://doi.org/10.1186/s13059-016-0997-x">Mash: Fast genome and metagenome distance estimation using MinHash</a>. <em>Genome Biology</em> <strong>17</strong>, 132 (2016).</div>
</div>
<div id="ref-bakerDashingFastAccurate2019" class="csl-entry">
<div class="csl-left-margin">357. </div><div class="csl-right-inline">Baker, D. N. &amp; Langmead, B. <a href="https://doi.org/10.1186/s13059-019-1875-0">Dashing: Fast and accurate genomic distances with HyperLogLog</a>. <em>Genome Biology</em> <strong>20</strong>, 265 (2019).</div>
</div>
<div id="ref-corsoNeuralDistanceEmbeddings2021" class="csl-entry">
<div class="csl-left-margin">358. </div><div class="csl-right-inline">Corso, G. <em>et al.</em> <a href="https://proceedings.neurips.cc/paper/2021/hash/9a1de01f893e0d2551ecbb7ce4dc963e-Abstract.html">Neural distance embeddings for biological sequences</a>. in vol. 34 1853918551 (Curran Associates, Inc., 2021).</div>
</div>
<div id="ref-hopfMutationEffectsPredicted2017" class="csl-entry">
<div class="csl-left-margin">359. </div><div class="csl-right-inline">Hopf, T. A. <em>et al.</em> <a href="https://doi.org/10.1038/nbt.3769">Mutation effects predicted from sequence co-variation</a>. <em>Nature Biotechnology</em> <strong>35</strong>, 128–135 (2017).</div>
</div>
<div id="ref-castroModelSelectionApproach2018" class="csl-entry">
<div class="csl-left-margin">360. </div><div class="csl-right-inline">Castro, B. M., Lemes, R. B., Cesar, J., Hünemeier, T. &amp; Leonardi, F. <a href="https://doi.org/10.1016/j.jmva.2018.05.006">A model selection approach for multiple sequence segmentation and dimensionality reduction</a>. <em>Journal of Multivariate Analysis</em> <strong>167</strong>, 319–330 (2018).</div>
</div>
<div id="ref-haschkaMNHNTreeToolsToolboxTree2021" class="csl-entry">
<div class="csl-left-margin">361. </div><div class="csl-right-inline">Haschka, T., Ponger, L., Escudé, C. &amp; Mozziconacci, J. <a href="https://doi.org/10.1093/bioinformatics/btab430">MNHN-tree-tools: A toolbox for tree inference using multi-scale clustering of a set of sequences</a>. <em>Bioinformatics</em> <strong>37</strong>, 3947–3949 (2021).</div>
</div>
<div id="ref-konishiPrincipalComponentAnalysis2019" class="csl-entry">
<div class="csl-left-margin">362. </div><div class="csl-right-inline">Konishi, T. <em>et al.</em> <a href="https://doi.org/10.1038/s41598-019-55253-0">Principal Component Analysis applied directly to Sequence Matrix</a>. <em>Scientific Reports</em> <strong>9</strong>, 19297 (2019).</div>
</div>
<div id="ref-ben-hurDetectingStableClusters2003" class="csl-entry">
<div class="csl-left-margin">363. </div><div class="csl-right-inline">Ben-Hur, A. &amp; Guyon, I. Detecting Stable Clusters Using Principal Component Analysis. in (eds. Brownstein, M. J. &amp; Khodursky, A. B.) 159–182 (Humana Press, 2003). doi:<a href="https://doi.org/10.1385/1-59259-364-X:159">10.1385/1-59259-364-X:159</a>.</div>
</div>
<div id="ref-clampJalviewJavaAlignment2004" class="csl-entry">
<div class="csl-left-margin">366. </div><div class="csl-right-inline">Clamp, M., Cuff, J., Searle, S. M. &amp; Barton, G. J. <a href="https://doi.org/10.1093/bioinformatics/btg430">The jalview java alignment editor</a>. <em>Bioinformatics</em> <strong>20</strong>, 426–427 (2004).</div>
</div>
<div id="ref-xiaSemisupervisedDrugproteinInteraction2010" class="csl-entry">
<div class="csl-left-margin">367. </div><div class="csl-right-inline">Xia, Z., Wu, L.-Y., Zhou, X. &amp; Wong, S. T. <a href="https://doi.org/10.1186/1752-0509-4-S2-S6">Semi-supervised drug-protein interaction prediction from heterogeneous biological spaces</a>. <em>BMC Systems Biology</em> <strong>4</strong>, S6 (2010).</div>
</div>
<div id="ref-tamposisSemisupervisedLearningHidden2019" class="csl-entry">
<div class="csl-left-margin">368. </div><div class="csl-right-inline">Tamposis, I. A., Tsirigos, K. D., Theodoropoulou, M. C., Kontou, P. I. &amp; Bagos, P. G. <a href="https://doi.org/10.1093/bioinformatics/bty910">Semi-supervised learning of hidden markov models for biological sequence analysis</a>. <em>Bioinformatics</em> <strong>35</strong>, 2208–2215 (2019).</div>
</div>
<div id="ref-elnaggarProtTransCrackingLanguage2021" class="csl-entry">
<div class="csl-left-margin">369. </div><div class="csl-right-inline">Elnaggar, A. <em>et al.</em> ProtTrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing. doi:<a href="https://doi.org/10.48550/arXiv.2007.06225">10.48550/arXiv.2007.06225</a>.</div>
</div>
<div id="ref-luDiscoveringMolecularFeatures2022" class="csl-entry">
<div class="csl-left-margin">370. </div><div class="csl-right-inline">Lu, A. X. <em>et al.</em> <a href="https://doi.org/10.1371/journal.pcbi.1010238">Discovering molecular features of intrinsically disordered regions by using evolution for contrastive learning</a>. <em>PLOS Computational Biology</em> <strong>18</strong>, e1010238 (2022).</div>
</div>
<div id="ref-townshendEndtoEndLearning3D2019" class="csl-entry">
<div class="csl-left-margin">371. </div><div class="csl-right-inline">Townshend, R., Bedi, R., Suriana, P. &amp; Dror, R. <a href="https://proceedings.neurips.cc/paper/2019/hash/6c7de1f27f7de61a6daddfffbe05c058-Abstract.html">End-to-end learning on 3D protein structure for interface prediction</a>. in vol. 32 (Curran Associates, Inc., 2019).</div>
</div>
<div id="ref-leeDeepTargetEndtoendLearning2016" class="csl-entry">
<div class="csl-left-margin">372. </div><div class="csl-right-inline">Lee, B., Baek, J., Park, S. &amp; Yoon, S. deepTarget: End-to-end learning framework for microRNA target prediction using deep recurrent neural networks. in 434442 (Association for Computing Machinery, 2016). doi:<a href="https://doi.org/10.1145/2975167.2975212">10.1145/2975167.2975212</a>.</div>
</div>
<div id="ref-Goodfellow-et-al-2016" class="csl-entry">
<div class="csl-left-margin">373. </div><div class="csl-right-inline">Goodfellow, I., Bengio, Y. &amp; Courville, A. <em><a href="http://www.deeplearningbook.org">Deep learning</a></em>. (MIT Press, 2016).</div>
</div>
<div id="ref-wangComprehensiveSurveyLoss2022" class="csl-entry">
<div class="csl-left-margin">374. </div><div class="csl-right-inline">Wang, Q., Ma, Y., Zhao, K. &amp; Tian, Y. <a href="https://doi.org/10.1007/s40745-020-00253-5">A Comprehensive Survey of Loss Functions in Machine Learning</a>. <em>Annals of Data Science</em> <strong>9</strong>, 187–212 (2022).</div>
</div>
<div id="ref-jiaoPerformanceMeasuresEvaluating2016" class="csl-entry">
<div class="csl-left-margin">375. </div><div class="csl-right-inline">Jiao, Y. &amp; Du, P. <a href="https://doi.org/10.1007/s40484-016-0081-2">Performance measures in evaluating machine learning based bioinformatics predictors for classifications</a>. <em>Quantitative Biology</em> <strong>4</strong>, 320–330 (2016).</div>
</div>
<div id="ref-brodersenBalancedAccuracyIts2010" class="csl-entry">
<div class="csl-left-margin">376. </div><div class="csl-right-inline">Brodersen, K. H., Ong, C. S., Stephan, K. E. &amp; Buhmann, J. M. The <span>Balanced Accuracy</span> and <span>Its Posterior Distribution</span>. in <em>2010 20th <span>International Conference</span> on <span>Pattern Recognition</span></em> 3121–3124 (2010). doi:<a href="https://doi.org/10.1109/ICPR.2010.764">10.1109/ICPR.2010.764</a>.</div>
</div>
<div id="ref-kaufmanLeakageDataMining2011" class="csl-entry">
<div class="csl-left-margin">377. </div><div class="csl-right-inline">Kaufman, S., Rosset, S. &amp; Perlich, C. Leakage in data mining: Formulation, detection, and avoidance. in 556563 (Association for Computing Machinery, 2011). doi:<a href="https://doi.org/10.1145/2020408.2020496">10.1145/2020408.2020496</a>.</div>
</div>
<div id="ref-whalenNavigatingPitfallsApplying2022" class="csl-entry">
<div class="csl-left-margin">378. </div><div class="csl-right-inline">Whalen, S., Schreiber, J., Noble, W. S. &amp; Pollard, K. S. <a href="https://doi.org/10.1038/s41576-021-00434-9">Navigating the pitfalls of applying machine learning in genomics</a>. <em>Nature Reviews Genetics</em> <strong>23</strong>, 169–181 (2022).</div>
</div>
<div id="ref-fisherInterpretationH2Contingency1922" class="csl-entry">
<div class="csl-left-margin">379. </div><div class="csl-right-inline">Fisher, R. A. <a href="https://doi.org/10.2307/2340521">On the interpretation of χ2 from contingency tables, and the calculation of p</a>. <em>Journal of the Royal Statistical Society</em> <strong>85</strong>, 87–94 (1922).</div>
</div>
<div id="ref-pearsonCriterionThatGiven1900" class="csl-entry">
<div class="csl-left-margin">380. </div><div class="csl-right-inline">Pearson, K. <a href="https://doi.org/10.1080/14786440009463897">X. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling</a>. <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em> <strong>50</strong>, 157–175 (1900).</div>
</div>
<div id="ref-hoerlRidgeRegressionBiased1970" class="csl-entry">
<div class="csl-left-margin">381. </div><div class="csl-right-inline">Hoerl, A. E. &amp; Kennard, R. W. <a href="https://doi.org/10.1080/00401706.1970.10488634">Ridge regression: Biased estimation for nonorthogonal problems</a>. <em>Technometrics</em> <strong>12</strong>, 55–67 (1970).</div>
</div>
<div id="ref-tibshiraniRegressionShrinkageSelection1996" class="csl-entry">
<div class="csl-left-margin">382. </div><div class="csl-right-inline">Tibshirani, R. <a href="https://doi.org/10.1111/j.2517-6161.1996.tb02080.x">Regression <span>Shrinkage</span> and <span>Selection Via</span> the <span>Lasso</span></a>. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> <strong>58</strong>, 267–288 (1996).</div>
</div>
<div id="ref-zhangOptimalityNaiveBayes" class="csl-entry">
<div class="csl-left-margin">383. </div><div class="csl-right-inline">Zhang, H. The Optimality of Naive Bayes. 6.</div>
</div>
<div id="ref-rishEmpiricalStudyNaive" class="csl-entry">
<div class="csl-left-margin">384. </div><div class="csl-right-inline">Rish, I. An empirical study of the naive Bayes classi<span>fi</span>er. 6.</div>
</div>
<div id="ref-vapnikEstimationDependencesBased1982" class="csl-entry">
<div class="csl-left-margin">385. </div><div class="csl-right-inline">Vapnik, V. <em>Estimation of dependences based on empirical data: Springer series in statistics (springer series in statistics)</em>. (Springer-Verlag, 1982).</div>
</div>
<div id="ref-boserTrainingAlgorithmOptimal1992" class="csl-entry">
<div class="csl-left-margin">386. </div><div class="csl-right-inline">Boser, B. E., Guyon, I. M. &amp; Vapnik, V. N. A training algorithm for optimal margin classifiers. in 144152 (Association for Computing Machinery, 1992). doi:<a href="https://doi.org/10.1145/130385.130401">10.1145/130385.130401</a>.</div>
</div>
<div id="ref-cortesSupportvectorNetworks1995" class="csl-entry">
<div class="csl-left-margin">387. </div><div class="csl-right-inline">Cortes, C. &amp; Vapnik, V. <a href="https://doi.org/10.1007/BF00994018">Support-vector networks</a>. <em>Machine Learning</em> <strong>20</strong>, 273–297 (1995).</div>
</div>
<div id="ref-druckerSupportVectorRegression1996" class="csl-entry">
<div class="csl-left-margin">388. </div><div class="csl-right-inline">Drucker, H., Burges, C. J. C., Kaufman, L., Smola, A. &amp; Vapnik, V. <a href="https://proceedings.neurips.cc/paper/1996/hash/d38901788c533e8286cb6400b40b386d-Abstract.html">Support vector regression machines</a>. in vol. 9 (MIT Press, 1996).</div>
</div>
<div id="ref-breimanRandomForests2001" class="csl-entry">
<div class="csl-left-margin">389. </div><div class="csl-right-inline">Breiman, L. <a href="https://doi.org/10.1023/A:1010933404324">Random <span>Forests</span></a>. <em>Machine Learning</em> <strong>45</strong>, 5–32 (2001).</div>
</div>
<div id="ref-breimanClassificationRegressionTrees1983" class="csl-entry">
<div class="csl-left-margin">390. </div><div class="csl-right-inline">Breiman, L., Friedman, J. H., Olshen, R. A. &amp; Stone, C. J. <em>Classification and regression trees</em>. (1983).</div>
</div>
<div id="ref-kingsfordWhatAreDecision2008" class="csl-entry">
<div class="csl-left-margin">391. </div><div class="csl-right-inline">Kingsford, C. &amp; Salzberg, S. L. <a href="https://doi.org/10.1038/nbt0908-1011">What are decision trees?</a> <em>Nature Biotechnology</em> <strong>26</strong>, 1011–1013 (2008).</div>
</div>
<div id="ref-caruanaEmpiricalComparisonSupervised2006a" class="csl-entry">
<div class="csl-left-margin">392. </div><div class="csl-right-inline">Caruana, R. &amp; Niculescu-Mizil, A. An empirical comparison of supervised learning algorithms. in 161168 (Association for Computing Machinery, 2006). doi:<a href="https://doi.org/10.1145/1143844.1143865">10.1145/1143844.1143865</a>.</div>
</div>
<div id="ref-yangReviewEnsembleMethods2010" class="csl-entry">
<div class="csl-left-margin">393. </div><div class="csl-right-inline">Yang, P., Hwa Yang, Y., B. Zhou, B. &amp; Y. Zomaya, A. A review of ensemble methods in bioinformatics. <em>Current Bioinformatics</em> <strong>5</strong>, 296–308 (2010).</div>
</div>
<div id="ref-potdarComparativeStudyCategorical2017" class="csl-entry">
<div class="csl-left-margin">394. </div><div class="csl-right-inline">Potdar, K., S., T. &amp; D., C. <a href="https://doi.org/10.5120/ijca2017915495">A Comparative Study of Categorical Variable Encoding Techniques for Neural Network Classifiers</a>. <em>International Journal of Computer Applications</em> <strong>175</strong>, 7–9 (2017).</div>
</div>
<div id="ref-hassanisaadiInterpretiveTimefrequencyAnalysis2017" class="csl-entry">
<div class="csl-left-margin">395. </div><div class="csl-right-inline">Hassani Saadi, H., Sameni, R. &amp; Zollanvari, A. <a href="https://doi.org/10.1186/s12859-017-1524-0">Interpretive time-frequency analysis of genomic sequences</a>. <em>BMC Bioinformatics</em> <strong>18</strong>, 154 (2017).</div>
</div>
<div id="ref-kunanbayevComplexEncoding2021" class="csl-entry">
<div class="csl-left-margin">397. </div><div class="csl-right-inline">Kunanbayev, K., Temirbek, I. &amp; Zollanvari, A. 2021 international joint conference on neural networks (IJCNN). in 1–6 (2021). doi:<a href="https://doi.org/10.1109/IJCNN52387.2021.9534094">10.1109/IJCNN52387.2021.9534094</a>.</div>
</div>
<div id="ref-dufresneKmerFileFormat2022" class="csl-entry">
<div class="csl-left-margin">398. </div><div class="csl-right-inline">Dufresne, Y. <em>et al.</em> The k-mer file format: A standardized and compact disk representation of sets of k-mers. <em>Bioinformatics</em> btac528 (2022) doi:<a href="https://doi.org/10.1093/bioinformatics/btac528">10.1093/bioinformatics/btac528</a>.</div>
</div>
<div id="ref-wrightUsingDECIPHERV22016" class="csl-entry">
<div class="csl-left-margin">399. </div><div class="csl-right-inline">Wright, E. S. <a href="https://doi.org/10.32614/RJ-2016-025">Using DECIPHER v2.0 to analyze big biological sequence data in r</a>. <em>The R Journal</em> <strong>8</strong>, 352–359 (2016).</div>
</div>
<div id="ref-zamaniAminoAcidEncoding2011" class="csl-entry">
<div class="csl-left-margin">400. </div><div class="csl-right-inline">Zamani, M. &amp; Kremer, S. C. 2011 IEEE international conference on bioinformatics and biomedicine workshops (BIBMW). in 327–333 (2011). doi:<a href="https://doi.org/10.1109/BIBMW.2011.6112394">10.1109/BIBMW.2011.6112394</a>.</div>
</div>
<div id="ref-singhEvolutionaryBasedOptimal2018" class="csl-entry">
<div class="csl-left-margin">401. </div><div class="csl-right-inline">Singh, D., Singh, P. &amp; Sisodia, D. S. <a href="https://doi.org/10.1016/j.eswa.2018.05.003">Evolutionary based optimal ensemble classifiers for HIV-1 protease cleavage sites prediction</a>. <em>Expert Systems with Applications</em> <strong>109</strong>, 86–99 (2018).</div>
</div>
<div id="ref-qianPredictingSecondaryStructure1988" class="csl-entry">
<div class="csl-left-margin">402. </div><div class="csl-right-inline">Qian, N. &amp; Sejnowski, T. J. <a href="https://doi.org/10.1016/0022-2836(88)90564-5">Predicting the secondary structure of globular proteins using neural network models</a>. <em>Journal of Molecular Biology</em> <strong>202</strong>, 865–884 (1988).</div>
</div>
<div id="ref-budachPyssterClassificationBiological2018" class="csl-entry">
<div class="csl-left-margin">403. </div><div class="csl-right-inline">Budach, S. &amp; Marsico, A. <a href="https://doi.org/10.1093/bioinformatics/bty222">Pysster: Classification of biological sequences by learning sequence and structure motifs with convolutional neural networks</a>. <em>Bioinformatics</em> <strong>34</strong>, 3035–3037 (2018).</div>
</div>
<div id="ref-choongEvaluationConvolutionaryNeural2017" class="csl-entry">
<div class="csl-left-margin">404. </div><div class="csl-right-inline">Choong, A. C. H. &amp; Lee, N. K. 2017 international conference on computer and drone applications (IConDA). in 60–65 (2017). doi:<a href="https://doi.org/10.1109/ICONDA.2017.8270400">10.1109/ICONDA.2017.8270400</a>.</div>
</div>
<div id="ref-mcginnisScikitLearnContribCategoricalEncodingRelease2018" class="csl-entry">
<div class="csl-left-margin">405. </div><div class="csl-right-inline">McGinnis, W. <em>et al.</em> Scikit-<span>Learn</span>-<span>Contrib</span>/<span>Categorical</span>-<span>Encoding</span>: <span>Release For Zenodo</span>. (2018) doi:<a href="https://doi.org/10.5281/ZENODO.1157110">10.5281/ZENODO.1157110</a>.</div>
</div>
<div id="ref-kawashima2008" class="csl-entry">
<div class="csl-left-margin">406. </div><div class="csl-right-inline">Kawashima, S. <em>et al.</em> <a href="https://doi.org/10.1093/nar/gkm998">AAindex: amino acid index database, progress report 2008</a>. <em>Nucleic Acids Research</em> <strong>36</strong>, D202–D205 (2008).</div>
</div>
<div id="ref-liPredictionProteinStructural2008" class="csl-entry">
<div class="csl-left-margin">407. </div><div class="csl-right-inline">Li, Z.-C., Zhou, X.-B., Dai, Z. &amp; Zou, X.-Y. <a href="https://doi.org/10.1007/s00726-008-0170-2">Prediction of protein structural classes by Chou<span>’</span>s pseudo amino acid composition: approached using continuous wavelet transform and principal component analysis</a>. <em>Amino Acids</em> <strong>37</strong>, 415 (2008).</div>
</div>
<div id="ref-nanniNewEncodingTechnique2011" class="csl-entry">
<div class="csl-left-margin">408. </div><div class="csl-right-inline">Nanni, L. &amp; Lumini, A. <a href="https://doi.org/10.1016/j.eswa.2010.09.005">A new encoding technique for peptide classification</a>. <em>Expert Systems with Applications</em> <strong>38</strong>, 3185–3191 (2011).</div>
</div>
<div id="ref-chenIFeaturePythonPackage2018" class="csl-entry">
<div class="csl-left-margin">409. </div><div class="csl-right-inline">Chen, Z. <em>et al.</em> <a href="https://doi.org/10.1093/bioinformatics/bty140">iFeature: A python package and web server for features extraction and selection from protein and peptide sequences</a>. <em>Bioinformatics</em> <strong>34</strong>, 2499–2502 (2018).</div>
</div>
<div id="ref-taylorClassificationAminoAcid1986" class="csl-entry">
<div class="csl-left-margin">410. </div><div class="csl-right-inline">Taylor, W. R. <a href="https://doi.org/10.1016/S0022-5193(86)80075-3">The classification of amino acid conservation</a>. <em>Journal of Theoretical Biology</em> <strong>119</strong>, 205–218 (1986).</div>
</div>
<div id="ref-zvelebilPredictionProteinSecondary1987" class="csl-entry">
<div class="csl-left-margin">411. </div><div class="csl-right-inline">Zvelebil, M. J., Barton, G. J., Taylor, W. R. &amp; Sternberg, M. J. E. <a href="https://doi.org/10.1016/0022-2836(87)90501-8">Prediction of protein secondary structure and active sites using the alignment of homologous sequences</a>. <em>Journal of Molecular Biology</em> <strong>195</strong>, 957–961 (1987).</div>
</div>
<div id="ref-kremerMethodSystemComputer2009" class="csl-entry">
<div class="csl-left-margin">412. </div><div class="csl-right-inline">Kremer, S. &amp; Lac, H. <a href="https://patents.google.com/patent/US20090024375A1/en">Method, system and computer program product for levinthal process induction from known structure using machine learning</a>. (2009).</div>
</div>
<div id="ref-maetschkeBlomapEncodingAmino2005" class="csl-entry">
<div class="csl-left-margin">413. </div><div class="csl-right-inline">Maetschke, S., Towsey, M. &amp; Bodén, M. <a href="https://doi.org/10.1142/9781860947322_0014">Blomap: An encoding of amino acids which improves signal peptide cleavage site prediction</a>. in vols Volume 1 141–150 (PUBLISHED BY IMPERIAL COLLEGE PRESS AND DISTRIBUTED BY WORLD SCIENTIFIC PUBLISHING CO., 2005).</div>
</div>
<div id="ref-gokNewFeatureEncoding2013" class="csl-entry">
<div class="csl-left-margin">414. </div><div class="csl-right-inline">Gök, M. &amp; Özcerit, A. T. <a href="https://doi.org/10.1007/s00521-012-0967-5">A new feature encoding scheme for HIV-1 protease cleavage site prediction</a>. <em>Neural Computing and Applications</em> <strong>22</strong>, 1757–1761 (2013).</div>
</div>
<div id="ref-sahaNovelApproachFind2019" class="csl-entry">
<div class="csl-left-margin">415. </div><div class="csl-right-inline">Saha, S. &amp; Bhattacharya, T. A Novel Approach to Find the Saturation Point of n-Gram Encoding Method for Protein Sequence Classification Involving Data Mining. in (eds. Bhattacharyya, S., Hassanien, A. E., Gupta, D., Khanna, A. &amp; Pan, I.) 101–108 (Springer, 2019). doi:<a href="https://doi.org/10.1007/978-981-13-2354-6_12">10.1007/978-981-13-2354-6_12</a>.</div>
</div>
<div id="ref-jeffreyChaosGameRepresentation1990" class="csl-entry">
<div class="csl-left-margin">416. </div><div class="csl-right-inline">Jeffrey, H. J. <a href="https://doi.org/10.1093/nar/18.8.2163">Chaos game representation of gene structure</a>. <em>Nucleic Acids Research</em> <strong>18</strong>, 2163–2170 (1990).</div>
</div>
<div id="ref-lochelChaosGameRepresentation2021" class="csl-entry">
<div class="csl-left-margin">417. </div><div class="csl-right-inline">Löchel, H. F. &amp; Heider, D. <a href="https://doi.org/10.1016/j.csbj.2021.11.008">Chaos game representation and its applications in bioinformatics</a>. <em>Computational and Structural Biotechnology Journal</em> <strong>19</strong>, 6263–6271 (2021).</div>
</div>
<div id="ref-cartesAccurateFastClade2022" class="csl-entry">
<div class="csl-left-margin">418. </div><div class="csl-right-inline">Cartes, J. A., Anand, S., Ciccolella, S., Bonizzoni, P. &amp; Vedova, G. D. Accurate and Fast Clade Assignment via Deep Learning and Frequency Chaos Game Representation. doi:<a href="https://doi.org/10.1101/2022.06.13.495912">10.1101/2022.06.13.495912</a>.</div>
</div>
<div id="ref-niApplyingFrequencyChaos2021" class="csl-entry">
<div class="csl-left-margin">419. </div><div class="csl-right-inline">Ni, H., Mu, H. &amp; Qi, D. <a href="https://doi.org/10.1016/j.jmgm.2021.107942">Applying frequency chaos game representation with perceptual image hashing to gene sequence phylogenetic analyses</a>. <em>Journal of Molecular Graphics and Modelling</em> <strong>107</strong>, 107942 (2021).</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="HPC-paper.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="viruses-hiv-and-drug-resistance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lucblassel/phd-manuscript/edit/main/04-learning-from-alignments.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

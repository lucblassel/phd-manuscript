% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
  twoside]{scrbook}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={From sequences to knowledge, improving and learning from sequence alignments.},
  pdfauthor={Luc Blassel},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PhD Thesis Manuscript 
%% Copyright Luc Blassel 2022
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{booktabs}
\usepackage{comment}
\usepackage{float}
%\usepackage[twoside]{geometry}
\usepackage{fancyhdr}
\usepackage{xspace}
\usepackage{numprint}
\usepackage{lipsum}
\usepackage{uni-titlepage}
\usepackage{lscape}
\usepackage{xurl} % add line breaks to long urls

% remove weird paragraph spacing
\raggedbottom

\excludecomment{htmlonly}

\newcommand{\extcaption}[2]{
    \caption[#1]{
        \textbf{#1}\newline
        #2
    }
}

% Changing max bib names for full author list in global references
\makeatletter
\newrobustcmd*{\SetMaxBibnames}[1]{\numdef\blx@maxbibnames{#1}}
\makeatother

% KOMA-Script caption options
% \setcaphanging
\setcapmargin{0cm}
\setcapindent{0pt}

% \usepackage{layout}
\hypersetup {
  colorlinks = true,
}

\newenvironment{tpage}[3]{%
\KOMAoptions{twoside = false}
  \begin{fullsizetitle}
    
    \vskip 2em
    \hskip 2em \includegraphics[height=20mm]{figures/logos/SU.pdf}
    \begin{center}
    {%
      
        \usekomafont{title}{%
            \Large THÈSE DE DOCTORAT \\
            DE SORBONNE UNIVERSITÉ
        }%
      
        \vskip 2em
      
        {%
            \sffamily
            \mdseries
            \Large Spécialité: Bioinformatique
        }%
        
        \vskip 0.1em
        
        {%
            \sffamily
            \mdseries
            \large École doctorale n. 515: Complexité du vivant \\
        }%
        \vskip 2em
        \usekomafont{subtitle}{
        \mdseries
        \Large
        réalisée sous la direction de Rayan Chikhi
        }
        \vskip 0.5em
        \usekomafont{subtitle}{%
            \Large Sequence Bioinformatics\\
            Institut Pasteur/CNRS – USR3756 
        }%
        
        \vskip 2em
        
        {%
            \sffamily
            \mdseries
            \large présentée par
        }%
        \vskip 0.5em
        \usekomafont{author}{%
            \huge
            #2
        }%

        \vskip 3em
      
        \usekomafont{title}{%
            \Huge #1
        }%
        
        \vskip 3em
        
        \usekomafont{subtitle}{%
            \mdseries
            \huge Soutenue le #3
        }%
        
        \vskip 0.5em
        
        \usekomafont{subtitle}{%
            \mdseries
            \Large
            devant le jury composé de: \\
            \vskip1em
            \begin{tabular}{lllr}
                TOPOLINO Alfredo & Professeur & Univ. Genève & Rapporteur \\
                SE-YENG Fang     & Professeur & Univ. Shanghai & Rapporteur \\
                CASTAFIORE Bianca & Cantatrice & Scala di Milano & Examinatrice \\
                LAMPION Serafon & Assureur & & Invité \\
                CHIKHI Rayan & PhD & Institue Pasteur & Directeur de Thèse
            \end{tabular}
        }%
    
    }%
    
    
    \vskip 10em
    
    {%
        \includegraphics[height=10mm]{figures/logos/CC-BY-NC-ND.pdf}
    }%
    \end{center}
  \end{fullsizetitle}
\KOMAoptions{twoside}
}{}

\renewcommand{\headrulewidth}{0pt}
\usepackage{upquote}
\usepackage{microtype}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[style=authoryear,backref=true,refsegment=chapter,defernumbers=true,sorting=none,citestyle=numeric,bibstyle=numeric,maxnames=3,maxbibnames=3,minnames=2,uniquelist=false,backend=biber]{biblatex}
\addbibresource{./references/HPC-MSRs.bib}
\addbibresource{./references/HIV-DRMs.bib}
\addbibresource{./references/Sequence-Data.bib}
\addbibresource{./references/Aligning-Sequences.bib}
\addbibresource{./references/HIV-context.bib}

\title{From sequences to knowledge, improving and learning from sequence alignments.}
\author{Luc Blassel}
\date{2022-09-02}

\begin{document}
\maketitle

\DeclarePrintbibliographyDefaults{heading=bibintoc}

\input{./tex/titlepage.tex}

\pagestyle{empty}
\frontmatter

\chapter{Aknowledgments}
Here will go my aknowledgments
\lipsum[7-9]

\chapter{Glossary}
This is the glossary

\mainmatter

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables
\pagestyle{empty}

\begin{comment}

#  {.unnumbered}

## Abstract {.unnumbered}

This is the Abstract

## Résumé {.unnumbered}

Ceci est le résumé

## Acknowledgments {.unnumbered}

Thanks everybody

## Glossary {.unnumbered}

What are these terms ?


\end{comment}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{CHAPTER \thechapter} 
\fancyhead[RO]{\rightmark}
\fancyfoot[CO,CE]{\thepage}

\hypertarget{general-introduction}{%
\chapter*{General Introduction}\label{general-introduction}}
\addcontentsline{toc}{chapter}{General Introduction}

\begin{itemize}
\tightlist
\item
  Explain shortly that 2 quite different subjects linked by alignment and sequence data.
\end{itemize}

\hypertarget{organization-of-this-manuscript}{%
\section*{Organization of this manuscript}\label{organization-of-this-manuscript}}
\addcontentsline{toc}{section}{Organization of this manuscript}

\begin{itemize}
\tightlist
\item
  Organisation of the manuscript
\end{itemize}

\hypertarget{research-output}{%
\section*{Research output}\label{research-output}}
\addcontentsline{toc}{section}{Research output}

\hypertarget{journal-publications}{%
\subsection*{Journal publications}\label{journal-publications}}
\addcontentsline{toc}{subsection}{Journal publications}

\begin{itemize}
\item
  \textbf{Blassel, Luc}, Paul Medvedev and Rayan Chikhi. 2022. \textbf{``Mapping-friendly sequence reductions: going beyond homopolymer compression''}. \emph{iScience}\\
  \href{https://google.com}{DOI goes here} \emph{(Adapted as Chapter} \ref{HPC-paper}\emph{)}
\item
  \textbf{Blassel, Luc}\footnote{Co-first authors: Luc Blassel and Anna Zhukova}, Anna Zhukova\textsuperscript{1}, Christian J Villabona-Arenas, Katherine E Atkins, Stéphane Hué, and Olivier Gascuel. 2021. \textbf{``Drug Resistance Mutations in HIV: New Bioinformatics Approaches and Challenges.''} \emph{Current Opinion in Virology} 51 (December): 56--64.\\
  \href{https://doi.org/10.1016/j.coviro.2021.09.009}{10.1016/j.coviro.2021.09.009} \emph{(Used as the basis for Section} \ref{finding-drms}\emph{)}
\item
  \textbf{Blassel, Luc}, Anna Tostevin, Christian Julian Villabona-Arenas, MartinePeeters, Stéphane Hué, and Olivier Gascuel. 2021. \textbf{``Using Machine Learning and Big Data to Explore the Drug Resistance Landscape in HIV.''} \emph{PLOS Computational Biology} 17 (8): e1008873.\\
  \href{https://doi.org/10.1371/journal.pcbi.1008873}{10.1371/journal.pcbi.1008873}. \emph{(Adapted as Chapter} \ref{HIV-paper}\emph{)}
\item
  Zhukova, Anna, \textbf{Luc Blassel}, Frédéric Lemoine, Marie Morel, JakubVoznica, and Olivier Gascuel. 2021. \textbf{``Origin, Evolution and Global Spread of SARS-CoV-2.''} \emph{Comptes Rendus. Biologies} 344 (1): 57--75.\\
  \href{https://doi.org/10.5802/crbiol.29}{10.5802/crbiol.29}.
\item
  Lemoine, Frédéric, \textbf{Luc Blassel}, Jakub Voznica, and Olivier Gascuel.2020. \textbf{``COVID-Align: accurate online alignment of hCoV-19 genomes using a profile HMM''} \emph{Bioinformatics}, 37 (12): 1761-1762.\\
  \href{https://doi.org/10.1093/bioinformatics/btaa871}{10.1093/bioinformatics/btaa871}.
\end{itemize}

\hypertarget{presentations-and-posters}{%
\subsection*{Presentations and posters}\label{presentations-and-posters}}
\addcontentsline{toc}{subsection}{Presentations and posters}

\begin{itemize}
\item
  \textbf{``Mapping-friendly sequence reductions: going beyond homopolymer compression''} proceedings talk, \href{https://recomb2022.net/recomb-seq/}{RECOMB-SEQ 2022}. San Diego, USA \emph{(May 21\textsuperscript{st} 2022)}
\item
  \textbf{``Can we improve analyses be transforming DNA?''} Joint RECOMB-SEQ RECOMB-CCB scientific \href{https://recomb2022.net/recomb-ccb-seq-scientific-communication/}{communication session}\footnote{2nd place prize awarded}. San Diego, USA (\emph{May 21\textsuperscript{st}} \emph{2022).}
\item
  ``\textbf{Machine learning approaches to reveal resistance mutations in HIV''} Poster at \href{https://www.lirmm.fr/mceb2019/}{MCEB 2019}. Porquerolles, France \emph{(May 29\textsuperscript{th} 2019)}
\end{itemize}

\hypertarget{what-is-sequence-data}{%
\chapter{What is Sequence data ?}\label{what-is-sequence-data}}

\hypertarget{biological-sequences-a-primer}{%
\section{Biological sequences, a primer}\label{biological-sequences-a-primer}}

To fully understand the work that was done during this thesis, as well as the choices that were made, some basic knowledge of molecular biology and genetics is needed. If you are already familiar with biological sequences, feel free to skip ahead to section \ref{obtaining-sequence-data}.

\hypertarget{what-is-dna}{%
\subsection{What is DNA ?}\label{what-is-dna}}

\textbf{D}esoxyribo\textbf{N}ucleic \textbf{A}cid (DNA) is one of the most important molecules there is, without it complex life as we know it is impossible. It contains all the genetic information of a given organism, that is to say all the information necessary for the organism to: 1) function as a living being and 2) make a perfect copy of itself. This is the case for the overwhelming majority of living organisms on planet earth, from elephants to potatoes, to micro-organisms like bacteria.

DNA is a polymer, composed of monomeric units called nucleotides. Each nucleotide is composed of ribose (a five carbon sugar) on which are attached a phosphate group as well as one of four nucleobases: Adenine (A), Cytosine (C), Guanine (G) of Thymine (T). These four types of nucleotide monomers link up with one-another, through phosphate-sugar bonds, creating a single strand of DNA. The ordered sequence of these four types of nucleotides in strand encodes all the genetic information necessary for the organism to function. Nucleotides in a strand form strong complementary bonds with nucleotides from another strand, A with T and C with G. These bonds allow two strands of DNA to form the double-helix structure of DNA \autocite{watson1953} shown in Figure \ref{fig:figDNA}. The specificity of nucleotide bonds ensure that the two strands of the double helix are complementary and that the information contained in one strand can be recovered from the other. This ensures a certain structural stability to the DNA molecule and a way to recover the important information that could be lost due to a damaged strand.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{./figures/Sequence-Intro/DNA.pdf}
\extcaption{Double-helix structure of DNA}{Each strand of DNA has a phosphate-sugar backbone on which are attached nucleobases. The two strands are linked by complementary bonds between the nucleobases of different strands (A bonding with T and C bonding with G).}
\label{fig:figDNA}
\end{figure}

The amount of DNA necessary to encode the information varies greatly from organism to organism: 5400 base pairs (5.4kBp) for the \(\varphi X174\) phage \autocite{sangerNucleotideSequenceBacteriophage1977}, 4.9MBp for \emph{Escherichia coli} \autocite{archer2011}, 3.1GBp for \emph{Homo sapiens} \autocite{nurk2022} all the way up to almost 150GBp for \emph{Paris japonica,} a Japanese mountain flowering plant \autocite{pellicer2010}. While very small genome size tend to occur in smaller, simpler organisms, genome size does not correlate with organism complexity \autocite{macgregor2001}.

\hypertarget{from-information-to-action}{%
\subsection{From Information to action}\label{from-information-to-action}}

\hypertarget{proteins-their-structure-and-their-role}{%
\subsubsection{Proteins, their structure and their role}\label{proteins-their-structure-and-their-role}}

The double stranded DNA molecules present in the cells of a living organism contain information only; in order for the organism to live this information must be read and translated into actions. Most of the actions necessary for ``life'' are taken by large molecules called proteins, they have a very wide range of functions from catalyzing reactions in the cell to giving it its structure \autocite{alberts2002}.

Proteins are macromolecules, that are made up of one or several chains of amino acids. These chains then link together and fold up in a specific three dimensional structure, giving the protein the shape it needs to fulfill its goal. This structure is determined by the sequence of amino acids, and a given protein can be identified by this amino acid sequence \autocite{alberts2002}.

This sequence is directly dependent on the information contained in the DNA. First the DNA is transcripted in a similar, but single stranded, molecule called RNA (Ribonucleic Acid) which encodes the same sequence. This RNA molecule is then translated into a protein by the following process \autocite{crick1961}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Nucleotides in the RNA sequence are read in groups of three called codons.
\item
  These codons are read sequentially along the RNA molecule.
\item
  Each codon corresponds to an amino acid, according to the genetic code.
\item
  The sequence of codons in RNA \emph{(and by extension DNA)} determines the sequence of amino acids.
\item
  The translation process is stopped when a specific type of codon \emph{(a ``Stop'' codon)} is read.
\end{enumerate}

With four types of nucleotides and codons grouping three nucleotides there are \(4^3=64\) possible codons. However, as stated above, proteins are only made up of 20 different amino acids, meaning that several different codons correspond to the same amino acid. This gives the translation process a certain robustness to errors that can occur when the DNA is copied to create a new cell, or when it is transformed into RNA prior to protein translation.

The portion of DNA that is read to create the protein is said to be ``coding'', and is called a gene. There are several thousands of genes in the human genome \autocite{internationalhumangenomesequencingconsortiumFinishingEuchromaticSequence2004} resulting in proteins executing thousands of different functions in a cell. In human beings, coding DNA represents only 1\% to 2\% of the total genome \autocite{elkonCharacterizationNoncodingRegulatory2017,omennReflectionsHUPOHuman2021}. The large majority of the DNA in a human being is not translated into proteins, a portion of it has a regulatory role, controlling transcription and translation, but the role remains unknown for the rest of the human genome \autocite{shabalinaMammalianTranscriptomeFunction2004,IntegratedEncyclopediaDNA2012}.

\hypertarget{making-mistakes}{%
\subsubsection{Making mistakes}\label{making-mistakes}}

Going from DNA sequence to protein is quite a complicated process involving several steps, it is therefore possible for a mistake to happen. There are several mechanisms to avoid mistakes and alteration of the genetic information: the complementary nature of the two strands of DNA, the redundant nature of the genetic code as well as error correction mechanisms in the molecules \emph{(called ``polymerases'')} that read and write DNA and RNA. However, despite all that, some errors in the nucleic acid (DNA and RNA) or protein sequences still make it through, these are called mutations.

\hypertarget{where-can-mistakes-happen}{%
\paragraph{Where can mistakes happen ?}\label{where-can-mistakes-happen}}

There are several sources of error that can alter the genetic information \autocite{chatterjee2017}:

\begin{itemize}
\item
  \textbf{DNA replication:} When a cell divides, or when an organism reproduces, the DNA molecule must be copied in order to preserve and transmit genetic information. This process has a very low rate of errors, with as low as one error for every billion to every hundred billion of replicated base pairs \autocite{fijalkowska2012}. This is due to the fact that the DNA polymerase (the protein responsible for copying DNA molecules), has a relatively low error rate to start with, but mostly to the error correcting mechanisms that are present in certain cells and bacteria \autocite{pray2008dna}.
\item
  \textbf{RNA transcription:} Since errors in RNA transcripts are less important than in replicated DNA, RNA polymerases have a much higher error rate than their DNA counterparts. This error rate has been estimated to be between four errors for each million \autocite{gout2013} to two errors for each hundred thousand \autocite{gout2017} transcribed bases.
\item
  \textbf{Protein translation:} The process of translating RNA to a protein is done by proteins called ribosomes. This is a very error prone process with a mistranslation rate estimated to be of the order of one error for every 10,000 codons translated \autocite{shcherbakovRibosomalMistranslationLeads2019}
\item
  \textbf{Other mutagenic events:} Many external events and factors have been shown to provoke mutations in exposed DNA such as Ionizing radiation \autocite{desouky2015}, UV rays \autocite{kiefer2007}, Toxins \autocite{bennett2003}, heat Stress \autocite{kantidze2016}, cold stress \autocite{gregory1994} or oxidative stress \autocite{gafter-gvili2013}.
\end{itemize}

\hypertarget{what-kind-of-errors-are-possible}{%
\paragraph{What kind of errors are possible?}\label{what-kind-of-errors-are-possible}}

In biological sequences (nucleic acids and proteins), mutations can result from one of three error modes:

\begin{itemize}
\tightlist
\item
  \textbf{Substitutions}, where the original base units (nucleotide or amino acid) is mistakenly replaced by another one, for instance inserting an A instead of a G during RNA transcription.
\item
  \textbf{Insertions},where a new base unit not present in the original sequence is added to the newly synthesized biological sequence.
\item
  \textbf{deletions}, where a base unit from the original sequence is skipped and not taken into account when synthesizing the new sequence.
\end{itemize}

While these three types of errors occur both in nucleic acids and proteins there are some things to consider about the consequences of nucleic acid mutations on protein synthesis. Due to the redundant nature of the genetic code mentioned in Section \ref{proteins-their-structure-and-their-role}, some mutations in the nucleic acid sequence will result in the same protein sequence and therefore not have altered protein activity. Some mutations however will result in a substitution at the amino acid level which could potentially lead to a physicochemically altered or even non-functional protein. Finally, insertion and deletion errors (collectively called indels) can have big consequences on resulting proteins. Inserting or deleting nucleotides in multiples of three will result in the insertion/deletion of amino acids in the resulting protein, any other length of indel will result in what is called a frameshift mutation \autocite{rothFrameshiftMutations1974}. These mutations causes changes in the codons, potentially resulting in a completely different amino-acid sequence, including premature stop codon apparition as shown in Figure \ref{fig:frameshift}.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{./figures/Sequence-Intro/Frameshift.pdf}
\extcaption{Effect of frameshift mutations}{The deletion of a single C (highlighted in red) in the original DNA sequence leads to a change in the codons read during translation. The original codons (shown in grey, with corresponding amino acids, above the sequence) translate to the functional protein. The new codons caused by the deletion (shown in blue, with corresponding amino acids, below the sequence), induce a premature STOP codon leading to a non-functional protein. }
\label{fig:frameshift}
\end{figure}

\hypertarget{what-effect-can-mutations-have}{%
\paragraph{What effect can mutations have ?}\label{what-effect-can-mutations-have}}

As we stated above, some mutations in DNA may have no repercussions, some others can lead to non-functional proteins. In some cases mutations can be associated with a trait in the mutated individual. For example a single mutation in a gene linked with coagulation can lead to pathological Leiden thrombophilia \autocite{kujovichFactorLeidenThrombophilia2011}, a single amino acid deletion in the CFTR protein leads to \emph{(the very deadly)} cystic fibrosis \autocite{cuttingCysticFibrosisGenetics2015}, and many mutations have been linked to complex diseases like type 2 diabetes \autocite{fuchsbergerGeneticArchitectureType2016,morrisLargescaleAssociationAnalysis2012}. All mutational effects are not necessarily bad for the organism though, and mutations are essential for bacteria \autocite{woodfordEmergenceAntibioticResistance2007} or viruses like HIV \autocite{rheeHumanImmunodeficiencyVirus2003} to develop resistance to treatment \emph{(more on that in Chapters} \ref{viruses-hiv-and-drug-resistance} \emph{and} \ref{HIV-paper}\emph{).}

While some mutations, their mechanisms and consequences have been thoroughly studied, in many cases mutations are simply linked to a trait. Since it is easier to show correlation than causation, and that the former does not necessarily imply the latter, it is important to further study mutations of notice to understand their potential consequences.

\hypertarget{obtaining-sequence-data}{%
\section{Obtaining sequence data}\label{obtaining-sequence-data}}

In many fields, especially in computational biology, we need to know what genetic information the studied organism has, that is to say, what is the exact sequence of nucleotides that make up its DNA. The process of figuring out this sequence is, perhaps unsurprisingly, called sequencing. And a sequence that is inferred from this process is called a \emph{sequencing read} or, more commonly, just a \emph{read}.

\hypertarget{sanger-sequencing-a-breakthrough}{%
\subsection{Sanger sequencing, a breakthrough}\label{sanger-sequencing-a-breakthrough}}

The first sequencing method was developed in 1977 \autocite{sangerDNASequencingChainterminating1977}. Sanger \emph{et al.} devised a simple method to read the sequence of nucleotides that make up a DNA sequence known as ``chain termination sequencing'' or more commonly ``Sanger sequencing'' \emph{(represented in Figure} \ref{fig:sanger}\emph{)}. Although this method is now mostly obsolete, it established some key concepts in sequencing, some of which are in action in the most modern sequencers.

To understand Sanger sequencing, one must first understand how to synthesize DNA. As we stated in Section \ref{what-is-dna}, DNA is built up from building blocks that we called nucleotides, more specifically deoxynucleotidetriphosphates or dNTPs. these dNTPs are made up of a sugar (deoxyribose) a nucleobase (A, T, G or C) and 3 phosphate groups. By succesively adding these dNTPs at the end of an existing DNA molecule, we extend it, linking one of the phospates of the dNTP to an oxygen atom on the last nucleotide of the DNA molecule. Let us now consider a dideoxynucleotidestriphosphate (ddNTP), which is identical to a dNTP except we remove a specific oxygen atom. This ddNTP can be added to the growing molecule of DNA like regular dNTPS, but since it is missing that one oxygen atom no more dNTPs or ddNTPs can be added to the DNA molecule after this one. The elongation is terminated and we call these ddNTPs chain-terminators. This combination of DNA synthesis followed by termination are at the heart of Sanger sequencing.

It is important to note that while dNTPs and ddNTPs refer to nucleotides with any nucleobase, we can refer to specific dNTPs by replacing the ``N'' with the base of choice. For example, dATP refers to the dNTP that has adenine as a base. Similarly we have dCTP, dGTP and dTTP (as well as ddATP, ddCTP, ddGTP and ddTTP).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first step of Sanger sequencing (and most sequencing methods) is to amplify the DNA molecule we wish to sequence, \emph{i.e.} make plenty of copies of it (usually through a process called PCR). These clones of the sequence are then separated into their two complementary brands one of which will be used as a template for the sequencing steps.
\item
  The second step is to prepare 4 different sequencing environments \emph{(think of it as 4 test tubes)}. In each environment we introduce an equal mix of the 4 dNTPs, that will be used to elongate new DNA molecules from the amplified templates, and a single type of ddNTP. So in the first test tube we will have only ddATP, ddCTP in the second, \emph{et caetera.} In addition, these ddNTP are marked, at first with radioactive isotopes, and later on with dyes. This marking means that we can observe the location of these ddNTP later on.
\item
  Then an equal portion of the template is introduced in each environment with DNA polymerases (that will add the nucleotides to elongate a sequence that is complementary to the template), and short specific DNA molecules called primers that are necessary for the polymerases to start synthesizing new DNA.
\item
  During synthesis the chain will be elongated with dNTPs by the polymerase and the reaction will be stopped once a ddNTP is incorporated. At the end of this process we have plenty of fragments of DNA in each test tube, and we know that these fragments end with a specific base in a given environment. For example, in the test tube where we added ddATP, we know that all the fragments end with an A, and that we have all the possible fragments that start at the beginning of the template and end with an A. If the template is AACTA, then the fragments we would get in the ddATP test tube would be A, AA, and AACTA.
\item
  Then, a sample from each environment is taken and deposited in a gel, each in its own lane. A process called electrophoresis is then used to separate the fragments according to their weight. By applying an electrical current to the gel, the fragments of DNA will migrate away from where they were deposited along their lane in the gel. Lighter, shorted DNA fragments will travel further than heavier ones. We then get clusters of fragments ordered by weight (and therefore by length) called bands. With the marked ddNTP we can reveal these bands in the gel.
\item
  We know that: 1) bands are ordered by weight; 2) consecutive bands correspond to the addition of a single nucleotide; 3) in a specific lane fragments corresponding to a band end with a specific base. This knowledge is enough to deduce the sequence of the template we sequenced. An example gel is shown in Figure \ref{fig:sanger}.
\end{enumerate}

This process allowed Sanger \emph{et al.} to sequence the first genome, of a \(\varphi X174\) bacteriophage, in 1977 \autocite{sangerNucleotideSequenceBacteriophage1977}. Although revolutionnary, this method was costly, time consuming and labor intensive. Adjustments to this method were made in order to make it faster and less expensive. An important step was to change the way ddNTPs were marked. By using fluorescent markers, each base having a distinct ``color'', we can eliminate the need to have 4 different environments and lanes in the gel \autocite{smithSynthesisOligonucleotidesContaining1985,smithFluorescenceDetectionAutomated1986}. This also paved the way for automating sequencing, each fluorescently marked band can be excited with a laser, and the resulting specific wavelength can be recorded by optical systems and the corresponding base automatically deduced \autocite{ansorgeAutomatedDNASequencing1987} (Also see Figure \ref{fig:sanger}). Other improvements were made such as using capillary electrophoresis instead of gel electrophoresis.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{./figures/Sequence-Intro/sanger-sequencing.pdf}
\extcaption{Caption}{(WIP)}
\label{fig:sanger}
\end{figure}

These gradual improvements to the Sanger sequencing protocol, allowed to sequence longer and more accurate reads, with the latest technologies resulting in reads reaching 1 ,000 base pairs with an accuracy of 99.999\% \autocite{shendureNextgenerationDNASequencing2008}. These improvements also resulted in a lower cost for sequencing, which was greatly decreased from around \$1000 per base-pair \autocite{collinsHumanGenomeProject2003} to only \$0.5 per kilobase \autocite{shendureNextgenerationDNASequencing2008}. FInally these technological improvements also increased the throughput of sequencing machines from around 1 kilobase per day \autocite{collinsHumanGenomeProject2003} to 120 kilobases per hour \autocite{liuComparisonNextGenerationSequencing2012}.

Despite these improvements, for ambitious endeavours such as the human genome project, sequencing was a massive undertaking: the first human genome is estimated to have cost between 500 million and 1 billion US dollars to sequence \autocite{CostSequencingHuman}.

\hypertarget{next-generation-sequencing}{%
\subsection{Next-generation sequencing}\label{next-generation-sequencing}}

Through these large sequencing projects and the genomics field in general, the richness and usefulness of sequence data was made ever more apparent. This growing need of sequence data ushered in a new era of sequencing with the development of many new sequencing methods designed to have a higher throughput and a lower cost than Sanger sequencing. This second generation of sequencing technologies, also referred to as Next-Generation Sequencing (NGS) or Massively parallel sequencing. While there were different technologies, there are a few common key points \autocite{metzkerSequencingTechnologiesNext2010}:

\begin{itemize}
\item
  As with Sanger sequencing, we first need to amplify and clone the DNA template, however since these technologies result in shorted reads than Sanger sequencing, the DNA we want to sequence must first be randomly broken up into small template fragments before being amplified.
\item
  The amplified template fragments are attached to some sort of solid support, resulting in a physical support with billions of template fragments attached to it.
\item
  As in Sanger sequencing, DNA molecules, complementary to the template fragments, are elongated. This happens for billions of fragments at the same time (hence the ``massively parallel'' epithet).
\item
  The addition of specific nucleotides to a chain are detected in real time, and there is no chain termination. There is no need for the long step of electrophoresis. These detections are simultaneous for all the molecules being elongated at once.
\end{itemize}

The result of these steps is a very large number of short reads. With data analysis these short reads can be used to deduce longer sequences and eventually a fragemented approximation of the orginal whole genome sequence through a process called \emph{assembly}.

The main NGS method is called ``sequencing by synthesis'', developed by a company: Illumina. It is commonly referred to as \emph{llumina sequencing}. This method is based on \emph{reversible chain terminators}, developed at the Institut Pasteur in the 90's \autocite{canardDNAPolymeraseFluorescent1994}. These are marked dNTPs that can be used to elongate DNA molecules, but that have an additional molecular group that makes them terminators by default. However this terminating group can be removed once the NTP is included in a DNA molecule allowing the elongation process to continue. These dNTPs are fluorescently marked and when excited with a laser they emit light with a distinctive color. During Illumina sequencing, these reversible chain terminators are included to millions of fragments at the same time, stopping elongation. At this point all the fragments are excited with a laser and an optical system takes a picture of the emitted colors for all the fragments as once. In this image, a pixel loosely corresponds to a sequenced fragment, and its color to the most recently added dNTP. The terminating groups are then cleaved and the process can start over by incorporating a new batch of reversible terminators. By observing the successive images we can deduce the sequence of added nucleotides for each sequenced fragment and obtain all of our reads.

Another NGS method is called pyrosequencing, commercialized by 454 Life Sciences. Contrary to Illumina sequencing, this method does not use reversible chain terminators. Instead it uses a special enzyme called luciferase that emits light as specific dNTPs are added. This process is repeated for the 4 dNTPs (similarly to Sanger sequencing) and from the light emissions we can deduce the sequence of nucleotides \autocite{nyrenSolidPhaseDNA1993}.

These technologies yield reads around 150 nucleotides for Illumina and 400nt for pyrosequencing \autocite{mardisDecadePerspectiveDNA2011}, this is much shorter than the 1kB reads obtainable from the latest Sanger sequencing technologies. However the throughputs are much higher \autocite{liuComparisonNextGenerationSequencing2012}: 2.5 to 12.5 Gigabases per hour for Illumina and 30 Megabases per hour for pyrosequencing. Costs are also quite low: \$0.07 and \$10 per Megabase for Illumina and pyrosequencing respectively. The per-base sequencing accuracies are also quite high, up to 99.9\% for both Illumina \autocite{stolerSequencingErrorProfiles2021} and pyrosequencing \autocite{liuComparisonNextGenerationSequencing2012}. A summary of the key characteristics for various sequencing technologies can be found in Table \ref{tab:sequencing}. The lower cost and higher throughput has made the Illumina sequencing technology the dominant one, the company estimating that 90\% of the world's sequencing data was generated with Illumina machines in 2015 \autocite{SequencingTechnologySequencing}.

\hypertarget{long-read-sequencing}{%
\subsection{Long read sequencing}\label{long-read-sequencing}}

Although NGS technologies revolutionized the sequencing world, recent efforts have been made to get longer reads. These third-generation methods generate reads of tens of kilobases and are commonly called \emph{long-read sequencing} method. Long reads have a host of applications \autocite{pollardLongReadsTheir2018} for which short NGS reads might not be well suited: \emph{De novo} assembly of large complex genomes, studying complex repetitive regions such as centromeres or telomeres or detection of structural variants. They have recently been use to assemble the first truly complete human genome, including telomeric and centromeric regions \autocite{nurk2022}.

The two available long read technologies are: Single Molecule Real Time sequencing (SMRT), commercialized by Pacific Biosciences (PacBio) and Nanopore sequencing, commercialized by Oxford Nanopore Technologies (ONT). While both technologies are quite different they result in much longer reads than even Sanger sequencing in real time, without the need for chain terminators or separate sequencing reactions, with a high throughput and at a reasonably low cost.

SMRT sequencing was first developed in 2009 \autocite{eidRealTimeDNASequencing2009}, before being commercialized and furthered by PacBio. The basic principle is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fragment and amplify DNA to obtain a very large number of DNA templates.
\item
  Link both strands of each DNA template together with known sequences called \emph{bell adapters}. Denature the DNA to create a single stranded, circular DNA molecule.
\item
  Primers and polymerases are attached to the circular molecule specifically on one of the bell adapters.
\item
  Add the circular DNA template, primer, polymerases complexes to a SMRT chip. This chip is essentially a large aluminium surface with hundreds of thousands of microscopic wells called \emph{Zero-Mode Waveguides} (ZMWs) only 100nm in diameter \autocite{leveneZeroModeWaveguidesSingleMolecule2003}. The polymerases are chemically bonded to the bottom of each of these ZMWs so we effectively get a single DNA template and polymerase per well.
\item
  Fluorescently marked dNTPs are incorporated progressively in each of the wells. When a marked dNTP is incorporated in the newly synthesized DNA brand, light of a specific wavelength is emitted.
\item
  The size of these ZMWs make the detection of the fluoresence possible with an optical system. Incorporation of dNTPs in each ZMW can be detected simultaneously in a parallel fashion and the resulting sequences deduced.
\end{enumerate}

Nanopore sequencing, though of in the eighties, further developed along the years \autocite{clarkeContinuousBaseIdentification2009} and first commercialized by ONT in 2014 \autocite{deamerThreeDecadesNanopore2016}, is completely different from all the sequencing technologies previously mentioned. Where all the other ones are based on synthesizing a complementary DNA strand and detecting specific dNTP incorporation in some way or another, there is no synthesis in nanopore sequencing. The principle relies on feeding a single strand of a DNA template through a small hole in a membrane, a nanopore, at a controlled speed. As the nucleotides go through the nanopore, the electric current is formed between both sides of the membrane. This current can be measured and is specific to the succession of 5 to 6 nucleotides inside the nanopore channel at any given time. By looking at the evolution of the electric current as the DNA strand goes through the nanopore, we can deduce the sequence of nucleotides through a process called \emph{base calling.} Base calling is usually done with machine learning methods, mainly artificial neural networks \autocite{wickPerformanceNeuralNetwork2019}. In the flow cells used in ONT sequencers, there are hundreds of thousands of nanopores, spread out over a synthetic membrane, allowing for massively parallel sequencing as well. Theoretically, since this method is not based on synthesis, the upper limit for read length is only limited by the length of the template, and in practice ONT sequencing produces the longest reads.

Both technologies yield long reads, the median and highest read lengths being 10 kilobases and 60 kilobases respectively for PacBio sequencing \autocite{rhoadsPacBioSequencingIts2015}. For nanopore the median read lengths of 10 to 12 kilobases \autocite{ipMinIONAnalysisReference2015,logsdonLongreadHumanGenome2020} are similar to PacBio, but in it can also yield ultra-long reads of 1 up to 2.3 megabases long \autocite{jainNanoporeSequencingAssembly2018,TharSheBlows,payneBulkVisGraphicalViewer2019}. The length of the reads and parallel nature of these two technologies allow these sequencers to have truly massive throughputs. PacBio sequencers can sequence between 2 and 11 gigabases per hour and ONT from 12.5 gigabases per hour, up to a staggering 260 gigabases per hour for the latest ONT PromethION machines \autocite{logsdonLongreadHumanGenome2020}. The cost of sequencing with these machines, while higher than for Illumina sequencers, remains reasonably affordable at \$0.32 and \$0.13 per megabase for PacBio and ONT respectively \autocite{murigneuxComparisonLongreadMethods2020}. These characteristics are summarized in Table \ref{tab:sequencing} along with other sequencing technologies.

The length, throughput and sequencing cost of both these technologies paint a pretty picture, and indeed they have proved useful in many settings, sequencing accuracy however is the main problem with these technologies. The per-base sequencing accuracy has been estimated to be between 85\% and 92\% for PacBio sequencers and 87\% to 98\% for ONT machines \autocite{chaissonResolvingComplexityHuman2015,logsdonLongreadHumanGenome2020,jainOxfordNanoporeMinION2016}. This accuracy is much lower than either Sanger sequencing or Illumina reads. Characterizing, correcting and accounting for these errors is widely studied and it will be discussed in more detail in Sections \ref{sequencing-errors-how-to-account-for-them} and \ref{the-special-case-of-homopolymers}.

\begin{table}
    \centering
    \begin{tabular}{lllll}
        \toprule
        technology & read length (nt) & throughput (nt/hour) & cost (\$/Mb) & accuracy \\ \midrule
        Sanger & \numprint{1000} & 120 $10^3$ & \$500 & 99.999\% \\
        Illumina & 150 & 2.5-12.5 $10^9$ & \$0.07 & 99.9\% \\
        Pyrosequencing & 400 & 30 $10^6$ & \$10 & 99.9\% \\
        PacBio SMRT & \numprint{10000} (up to \numprint{60000}) & 2-11 $10^9$ & \$0.32 & 85-92\% \\
        Nanopore & \numprint{12000} (up to 2.5 $10^6$) & 12.5-260 $10^9$ & \$0.13 & 87-98\% \\
    \bottomrule
    \end{tabular}
    \extcaption{Comparison of sequencing technology characteristics.}{Characteristics for the latest sequencers were used for the Sanger sequencing entry. The length is given in nucleotides, throughputs in sequences nuctleotides per hour and cost in US dollars per megabase.}
    \label{tab:sequencing}
\end{table}

While most of the mentioned technologies can also be adapted and used to sequence RNA instead of DNA \autocite{hongRNASequencingNew2020,ozsolakRNASequencingAdvances2011}, directly sequencing proteins remains a challenge. The sequence of amino acids making up a protein is usually deduced from the codons in sequenced DNA or RNA after detection of potentially coding regions called open reading frames (ORFs). Development of methods to directly sequence protein molecules using mass spectrometry was started not very long after Sanger sequencing \autocite{huntProteinSequencingTandem1986} and improved \autocite{smithProteinSequencingProtocols2002}. New methods are still being developed \autocite{restrepo-perezPavingWaySinglemolecule2018} but protein sequencing is still a challenge.

\hypertarget{sequencing-errors-how-to-account-for-them}{%
\section{Sequencing errors, how to account for them ?}\label{sequencing-errors-how-to-account-for-them}}

Sequencing technologies are not perfect, they make errors as we can see from the various accuracy rates reported in Section \ref{obtaining-sequence-data}. For technologies based on nucleic acid synthesis (i.e.~everything except ONT), since they use polymerases it stands to reason that the same three types of errors, described in Section \ref{making-mistakes}, occur: substitutions, insertions and deletions. For long read technologies though, most of the errors do not come from the polymerase, but from signal processing used to deduce the sequence. Since both technologies execute single molecule sequencing, the signal to noise ratio is low \autocite{weirather2017,wangNanoporeSequencingTechnology2021} making base calling more complicated.

This explains the discrepancy in error rates between short and long read sequencing technologies: the former getting as low as 10\textsuperscript{-4} or 10\textsuperscript{-5} after computational processing \autocite{maAnalysisErrorProfiles2019} where the latter are between 10\% and 15\%. This high error rate long reads is bothersome and many efforts have been made lower this error rate, computationally or technologically.

\hypertarget{error-correction-methods}{%
\subsection{Error correction methods}\label{error-correction-methods}}

The long read error-correction literature and toolset is rich and active \autocite{limaComparativeAssessmentLongread2020,fuComparativeEvaluationHybrid2019,zhangComprehensiveEvaluationLong2020}. There are two main ways to correct errors: 1) hybrid methods where higher-accuracy shorter reads are used to correct errors and 2) non-hybrid methods where only the long-reads are used.

In Non-hybrid methods \autocite{limaComparativeAssessmentLongread2020,amarasingheOpportunitiesChallengesLongread2020}, by finding regions that overlap fairly well between reads and taking the consensus (i.e.~the majority nucleotide at each position) of the overlapped regions some errors can be eliminated. In many analyses and sequencing data processing pipelines, the first step is to break up the reads into all possible overlapping subsequences of length \(k\) called k-mers (e.g the 3-mers of the ATTGC are ATT, TTG and TGC). Rare k-mers in the read dataset, i.e.~k-mers that appear only a handful of times in all the reads, are likely the result of an error and filtering them out can improve analysis. One or both of these procedures are implemented in several pieces of commonly used software such as assembler like wtdbg2 \autocite{ruanFastAccurateLongread2020}, and canu \autocite{korenCanuScalableAccurate2017} or standalone long-read correctors like daccord \autocite{tischlerNonHybridLong2017}. In some cases, errors are corrected not on the raw reads but after having assembled the long reads into long continuous sequences (contigs), this process is called polishing. The ntEdit polisher \autocite{warrenNtEditScalableGenome2019} also filters out rare kmers to correct errors. The Arrow \autocite{heplerImprovedCircularConsensus2016} and Nanopolish \autocite{simpsonDetectingDNACytosine2017} polishers correct the assembly using the raw PacBio and ONT long reads respectively.

Hybrid methods, as their name suggest, makes use of short reads to correct errors in long reads. By finding similar regions between the short and long reads we can use the higher accuracy of short reads to correct the long ones. This is implemented in many pieces of software proovread \autocite{hacklProovreadLargescaleHighaccuracy2014}, Jabba \autocite{miclotteJabbaHybridError2016}, PBcR \autocite{korenHybridErrorCorrection2012} or LoRDEC \autocite{salmelaLoRDECAccurateEfficient2014}. Short reads can also be used to polish long read assemblies with tools like Pilon \autocite{walkerPilonIntegratedTool2014} or Racon \autocite{vaserFastAccurateNovo2017}. The first complete human genome was assembled and polished using many different sequencing technologies including PacBIo, ONT and Illumina technologies \autocite{nurk2022}.

\hypertarget{more-accurate-sequencing-methods}{%
\subsection{More accurate sequencing methods}\label{more-accurate-sequencing-methods}}

While a lot of effort is being put into error correction, another angle of attack to lower the error rate of long reads is to improve the sequencing technology.

In 2019, PacBio introduced HiFi reads, based on a circular consensus (CCS) technique \autocite{wenger2019}. During SRMT sequencing the 2 strands are linked together by ball adapters to form a circular DNA template (c.f. Section \ref{long-read-sequencing}), the central idea of CCS is to sequence this molecule multiple time by going over the circle multiple times. In the resulting long sequence the known bell adapter sequences can be removed, and a consensus sequence can be built from the multiple passes over the same DNA template. This results in long-read accuracies of 99.8\% to 99.9\% \autocite{wenger2019,logsdonLongreadHumanGenome2020}. This works because PacBio sequencing errors are mostly randomly distributed along the sequenced template (more on that in Section \ref{homopolymers-and-long-reads}), therefore it is unlikely that the same error will appear in multiple passes over the same template portion.

For ONT sequencing, most improvement efforts have been focused on base-callers. These tools were originally based on Hidden Markov Models \autocite{timpDNABaseCallingNanopore2012} (HMMs), but gradually they have been shifting over to neural network based deep learning methods \autocite{peresiniNanoporeBaseCalling2021,bozaDeepNanoDeepRecurrent2017,wickPerformanceNeuralNetwork2019,amarasingheOpportunitiesChallengesLongread2020} with faster inference times and better performance.

Similarly to PacBio HiFi reads, ONT developed 2D, and 1D\textsuperscript{2} sequencing. In 2D sequencing both strands of the DNA molecule to sequence are linked with a hairpin adapter to form one long sequenced. Each strand is sequenced once and a consensus is built from these 2 passes \autocite{tylerEvaluationOxfordNanopore2018}. 1D\textsuperscript{2} sequencing operates in a similar fashion but without the need for a hairpin adapter \autocite{linNanoporeTechnologyIts2021}. 2D sequencing produces reads with 97\% accuracy albeit much shorter than 1D sequencing \autocite{tylerEvaluationOxfordNanopore2018}. Recently, Oxford Nanopore Technologies announced the release of a new chemistry they call duplex. Using new a chemistry, new basecaller and sequencing of both strands (similarly to 2D and 1D\textsuperscript{2}) they announce raw read accuracies of 99.3\% \autocite{OxfordNanoporeTech}. Pre-printed research seems to confirm these numbers with one experiment yielding duplex reads with a 99.9\% accuracy \autocite{sandersonComparisonR9Kit102022}.

A technologically agnostic method using unique molecular identifiers added during the template preparation phase, and consensus sequencing has been shown, in specific contexts, to improve the accuracies of both ONT and PacBio CCS long reads to 99.59\% and 99.93\% respectively \autocite{karstHighaccuracyLongreadAmplicon2021}.

Finally, new sequencing technologies are being developed, like built in error-correction short-read technologies yielding error-free reads of up to 200 nucleotides \autocite{chenHighlyAccurateFluorogenic2017}. Illumina also recently announced its own high-throughput, high-accuracy long-read sequencing technology in 2022 \autocite{HighPerformanceLong}, although details about the performance and technology are scarce.

\hypertarget{the-special-case-of-homopolymers}{%
\section{The special case of homopolymers}\label{the-special-case-of-homopolymers}}

Despite improvement in error correction methods and sequencing technologies, certain genetic patterns are particularly difficult to process, homopolymers are one such pattern.

\hypertarget{homopolymers-and-the-human-genome}{%
\subsection{Homopolymers and the human genome}\label{homopolymers-and-the-human-genome}}

\emph{Homopolymers} consist of a stretch of repeated nucleotides (i.e.~\(\geq 2\)) occurring at some point in the genome. For example the sequence AAAA is a length 4 adenine homopolymer. In the complete human genome assembly (CHM13 v1.1 from the T2T consortium \autocite{nurk2022}), 50\% of its three gigabases are in homopolymers of size 2 or more, and 10\% are in homopolymers of size 4 or more. As can be seen in Figure \ref{fig:HPpercent}, short and medium length homopolymers make up a significant part of the genome. In a previous GRCh38 human genome assembly, more than 1.9 megabases are in homopolymers of length 8 or higher \autocite{booeshaghiPseudoalignmentFacilitatesAssignment2022}, representing about 1‰ of that assembly. The longest homopolymer run in the CHM13 v1.1 assembly is 86 \emph{(90 in GRCh38 \autocite{booeshaghiPseudoalignmentFacilitatesAssignment2022})}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{_main_files/figure-latex/HPpercent-1} 

}

\caption[Homopolymer fraction of the whole human genome by homopolymer length.]{\textbf{Homopolymer fraction of the whole human genome by homopolymer length.}\\The homopolymer counts were calculated from the T2T consortium full human genome assembly CHM13 v1.1. This figure was inspired by Figure 3b of reference \autocite{booeshaghiPseudoalignmentFacilitatesAssignment2022}.}\label{fig:HPpercent}
\end{figure}

In the human genome, homopolymers tend to occur more often in adenine and thymine runs than guanine and cytosine. There are are approximately twice as more nucleotides within A or T homopolymers (481 Mb and 484 Mb) than G or C (278 Mb and 279 Mb). This discrepancy is even more pronounced when looking homopolymers longer than four nucleotides (Figure \ref{fig:HPdistrib}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{_main_files/figure-latex/HPdistrib-1} 

}

\caption[Distribution of homopolymer lengths per base in the human genome, for homopolymers of length $\geq$ 4.]{\textbf{Distribution of homopolymer lengths per base in the human genome, for homopolymers of length $\geq$ 4.}\\The homopolymer counts were calculated from the T2T consortium full human genome assembly CHM13 v1.1.}\label{fig:HPdistrib}
\end{figure}

\hypertarget{homopolymers-and-long-reads}{%
\subsection{Homopolymers and long reads}\label{homopolymers-and-long-reads}}

Unfortunately homopolymers are a source of errors in sequencing, particularly for long-read technologies: while substitutions seem to be randomly distributed along the reads for PacBio and ONT, the main error mode seems to be indels in homopolymeric sections, \emph{i.e.} reading the same nucleotide several times or skipping over one of the repeated nucleotides. Many study show that homopolymeric indels is the main type of error for PacBIO SMRT and ONT long-read sequencing \autocite{delahaye2021,goodwin2015,dohmBenchmarkingLongreadCorrection2020,weirather2017}. This is even the case for PacBio HiFi reads, while the circular concensus approach eliminates the randomly distributed substitutions homopolymer indels remain \autocite{wenger2019}. It seems that ONT reads are more prone to this type of error than PacBIo \autocite{logsdonLongreadHumanGenome2020}. The rate of these errors is independent of the length of the homopolymer for ONT, but it rises with homopolymer length for short-read and PacBio technologies \autocite{fooxPerformanceAssessmentDNA2021}.

\hypertarget{accounting-for-homopolymers}{%
\subsection{Accounting for homopolymers}\label{accounting-for-homopolymers}}

The fact that they make up a significant part of the human genome, and that they are a source of errors for long read technologies means that they warrant special attention and care. Methods have been devised and implemented, specifically to counter homopolymer-linked errors.

\hypertarget{specific-error-correction}{%
\subsubsection{Specific error correction}\label{specific-error-correction}}

Homopolymer errors are taken under special consideration during assembly polishing when using certain tools like HomoPolish \autocite{huangHomopolishMethodRemoval2021} or NanoPolish \autocite{simpsonDetectingDNACytosine2017}. Methods to improve base calling of homopolymer stretches have been developed for nanopore sequencing \autocite{rangSquiggleBasepairComputational2018,sarkozyCallingHomopolymerStretches2018}, and implemented in state of the art base-callers such as guppy or scrappie \autocite{wickPerformanceNeuralNetwork2019}.

Steps before sequencing can also be taken in order to reduce the effect of these errors, like avoiding homopolymers in barcode sequences \autocite{hawkinsIndelcorrectingDNABarcodes2018,srivathsanMinIONBasedPipeline2018}. or during the development of DNA based storage systems \autocite{wangConstructionBioConstrainedCode2019}.

Improving the sequencing technologies can also be a solution by reducing the number of homopolymer errors straight from the source. The latest ONT chemistry R.10 reportedly improves accuracy in homopolymer rich regions \autocite{R10NewestNanopore,amarasingheOpportunitiesChallengesLongread2020}, non-biological solid-state nanopores also reduces errors in homopolymers \autocite{zhouDetectionDNAHomopolymer2019,gotoIdentificationFourSinglestranded2018}.

\hypertarget{hpc-trick}{%
\subsubsection{Homopolymer compression, a nifty trick}\label{hpc-trick}}

In many cases, reads cannot be re-sequenced with newer technologies, or base-called with better base callers. Only the read sequences, potentially containing homopolymer errors, are available for usage. In order to account for this sort of error, a simple pre-processing trick was developed: \emph{homopolymer compression} (HPC).

The idea is very simple, for any sequence replace a repeated run of any nucleotide (i.e.~homopolymers) by a single occurrence of that nucleotide. This means that after going through HPC the sequence AAACTGGG will yield the sequence ACTG. This simple pre-processing step, applied to all the reads and sequences to analyze removes all indels in homopolymers, and can resolve some ambiguities (c.f. Figure \ref{fig:hpcSchema}). It can also remove legitimate information contained in homopolymers, however the trade-off with the reduced error rate has been deemed advantageous.

HPC has been implemented in many sequence bioinformatics software tools. The HiCanu \autocite{nurkHiCanuAccurateAssembly2020}, MDBG \autocite{ekimMinimizerspaceBruijnGraphs2021}, wtdbg2 \autocite{ruanFastAccurateLongread2020}, shasta \autocite{shafinNanoporeSequencingShasta2020} assemblers all use HPC under the hood to provide better assemblies, and it was used to assemble the complete human genome sequence \autocite{nurk2022}. The first published usage of HPC, was actually in the CABOG assembler \autocite{millerAggressiveAssemblyPyrosequencing2008} developed for pyrosequencing reads. HPC has also been implemented for other tasks, like clustering \autocite{sahlinNovoClusteringLongRead2020}, long read error correction with LSC \autocite{auImprovingPacBioLong2012} and LSCPlus \autocite{huLSCplusFastSolution2016}, alignment with minimap2 \autocite{liMinimap2PairwiseAlignment2018} and winnowmap2 \autocite{jainWeightedMinimizerSampling2020}, or specific analysis pipelines for satellite tandem repeats \autocite{vannesteForensicSTRAnalysis2012}.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{./figures/Sequence-Intro/hpc-helps.pdf}
\extcaption{Homopolymer compression can help resolve ambiguities due to sequencing errors}{
A read with homopolymer related sequencing errors can be homologous to two different regions of the reference genome, with one discrepancy for each region. After applying HPC, this ambiguity is properly accounted for and the read is homologous to only one region. This figure, however, only shows one way homopolymers can be detrimental to mapping and others are possible\footnotemark.
}
\label{fig:hpcSchema}
\end{figure}
\footnotetext{Homopolymer indels can be harmful in opposite circumstances as well.Let us consider, for example, a read that should map to several repetitions of a conserved motif. Homopolymer indels can artificially resolve an ambiguity by making the read unique and prefer a specific repetition of the motif or entirely misplace the read.}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

I hope, after reading this chapter, you will agree with me that sequencing is fundamental for furthering our knowledge of biological processes, organisms and Life in general. And as such, the sequencing field is still very active with new technologies being developed, to improve the current technologies in various aspects: Illumina promising high accuracy long reads with Infinity \autocite{HighPerformanceLong}, PacBio developing its own short read sequencing technology, moving away from sequencing by synthesis \autocite{ShortreadSequencingBinding,cetinPlasmonicSensorCould2018}. Finally efforts are also being made to make sequencing more affordable and available in a greater number settings with Ultima genomics promising accurate short reads for as low as \$1 per gigabase \autocite{almogyCostefficientWholeGenomesequencing2022}.

With all these technological improvements we are approaching an era where sequencing is easy and quick, opening the door for massive projects like Tara Oceans \autocite{sunagawaTaraOceansGlobal2020} or the BioGenome project \autocite{lewinEarthBioGenomeProject2018} to better understand biodiversity. Routine whole-genome sequencing could usher in an era personalized medicine \autocite{lightbodyReviewApplicationsHighthroughput2019}.

Despite all these advancements, sequencing errors remain an obstacle to certain analyses. This is particularly true for the ever more used and useful long reads, and the important fraction of genomes made up of homopolymers. Detecting, removing or accounting for these errors in some way is a crucial step to improve any analysis based on sequencing data, and to make sure that no theory or conclusion are built upon erroneous sequence data.

Finally, it is important to note (at least for the remainder of this thesis) that, from a computational standpoint, a biological sequence is simply a succession of letters and a set of reads is simply a text file. Therefore many analysis and data processing methods are inspired or directly transposed from the field of text algorithmics.

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

\hypertarget{aligning-sequence-data}{%
\chapter{Aligning sequence data}\label{aligning-sequence-data}}

\hypertarget{what-is-an-alignment}{%
\section{What is an alignment ?}\label{what-is-an-alignment}}

In biology, comparison is at the heart of many studies: between individuals, between species, between sequencing runs, \emph{etc}\ldots{} In order to do this at a fine grained level and extract knowledge from it we need to compare what is comparable, this is where sequence alignment steps in. In broad terms, during sequence alignment, we aim to find regions similar to each other in two or more sequences and group them together. When this process is done with only two sequences it is called a \emph{pairwise alignment}, when three or more sequences are used it is called \emph{multiple alignment}. We will first focus on pairwise alignment as it was used as the basis for the more complex multiple alignment.

\hypertarget{why-align}{%
\subsection{Why align ?}\label{why-align}}

The first question we might ask ourselves is why align at all, if we want to compare two sequences there are plenty of distances and metrics out there to use. Something like the Hamming distance \autocite{hammingCodingInformationTheory1980} is very quick and easy to compute by comparing characters two by two. It is however ill-suited to our needs in biology: first the two sequences to compare must be of the same length, second the differences between the sequences arise from biological processes. These processes (c.f. Section \ref{what-kind-of-errors-are-possible}) can be substitutions which the Hamming distance can handle, however insertions and deletions shifts one of the sequences and introduce many character-to-character differences that could be explained by a single indel.

For example, if we have the two following sequences \texttt{ATGTGCAGTA} and \texttt{AGTGCAGTAC}. if we count the differences character by character, except the first pair of A, all the characters are different (c.f. below). However if we consider that the first T was deleted and a C was inserted at the end of the second sequence then we can see that none of the characters are actually different. In order to represent insertions and deletions \emph{gaps} are inserted in the sequences as seen below:

\textbf{\texttt{ATGTGCAGTA-}}\strut \\
\textbf{\texttt{A-GTGCAGTAC}}

This problem of comparing two sequences with insertions or deletions is a fairly studied one in text algorithmics: the string-edit problem \autocite{gusfieldAlgorithmsStringsTrees1997}. Some metrics like the Levenshtein distance \autocite{levenshteinBinaryCodesCapable1966} and the more edit distance \autocite{gusfieldAlgorithmsStringsTrees1997} exist and are closely related to the pairwise sequence alignment problem, finding the minimal number of substitution, insertion of deletion operations to go from one sequence to the other.

Sequence alignments have many downstream use-cases. They are the basis of comparative genomics \autocite{hardisonComparativeGenomics2003} and are used to infer evolutionary relationships and phylogenetic tree reconstruction methods usually take as input multiple alignments \autocite{felsensteinEvolutionaryTreesDNA1981,kumarMEGAMolecularEvolutionary1994,kozlovRAxMLNGFastScalable2019,guindonNewAlgorithmsMethods2010,priceFastTreeApproximatelyMaximumLikelihood2010}. Sequence alignments have been used to study protein structure \autocites[ ]{jumperHighlyAccurateProtein2021,karplusPredictingProteinStructure1999} and function \autocite{watsonPredictingProteinFunction2005,leePredictingProteinFunction2007}. They can be used to correct sequencing errors \autocite{hacklProovreadLargescaleHighaccuracy2014,korenHybridErrorCorrection2012,salmelaCorrectingErrorsShort2011} or detect structural variations in genomes \autocite{medvedevComputationalMethodsDiscovering2009,mahmoudStructuralVariantCalling2019}. All this to say that they are absolutely fundamental to the field of computational biology and errors in alignments can lead to errors somewhere down the line.

\hypertarget{how-to-align-two-sequences}{%
\subsection{How to align two sequences ?}\label{how-to-align-two-sequences}}

There are two approaches for for pairwise alignment \autocite{sungAlgorithmsBioinformaticsPractical2011}: \emph{global alignment} where the entirety of both sequences is used when aligning them, and \emph{local alignment} where we only seek to find regions in each sequence that are most similar to each other. Global alignment is used when the two sequences are expected to be quite similar (e.g.~comparing two related proteins), whereas local alignment is mostly used when we expect the sequences to be fairly different but with highly similar regions, like genomes of two distantly related species that share a highly conserved region.

The seminal method for global pairwise alignment was the Needleman-Wünsch algorithm \autocite{needlemanGeneralMethodApplicable1970} based on a dynamic programming method. A decade later, the Smith-Waterman algorithm \autocite{smithIdentificationCommonMolecular1981} was developed with similar ideas to perform local alignment. Both are still used today for pairwise alignment.

Dynamic programming is often used to solve complex problems by breaking it into smaller sub-problems and solving each one optimally and separately \autocite{bradleyAppliedMathematicalProgramming1977,bellmanTheoryDynamicProgramming1954}, it is particularly useful when we wish to have a precise alignment between 2 sequences.

\hypertarget{global-alignment}{%
\subsubsection{Global alignment}\label{global-alignment}}

The fundamental algorithm for globally aligning two sequences was the Needleman-Wünsch (NW) algorithm \autocite{needlemanGeneralMethodApplicable1970}, this can be conceptualized in one of two equivalent ways: finding the alignment with 1) the lowest edit-distance or 2) the highest alignment score. These two are equivalent so in this section we will maximize the alignment score.

The first thing that is needed is, given an alignment, how to compute an alignment score. To do this we need to assign costs to each operation. Usually matches (i.e.~aligning two identical characters) are given a positive cost and mismatches or indels a negative cost. If we assign a cost of +1 to a match and a cost of -1 to mismatches and indels then the alignment presented above in Section \ref{why-align} would have an alignment score of 9 - 2 = 7 \emph{(9 matches and two indels)}.

The NW algorithm is based on a simple recurrence relation: the optimal alignment score of two sequences \(S_1\) and \(S_2\) of lengths \(n\) and \(m\) respectively is the maximum of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The optimal alignment score of \(S_1[1,n-1]\)\footnote{Here I am using an index starting at 1 and inclusive, so \(S_1[1,n-1]\) represents the first \(n-1\) characters. If \(S_1 = ABCD\) then \(S_1[1;3]=ABC\)} and \(S_2[1,m-1]\) plus the cost of a match or mismatch between the \(n^{th}\) character of \(S_1\) and the \(m^{th}\) character of \(S_2\)
\item
  The optimal alignment score of \(S_1\) and \(S_2[1,m-1]\) plus the cost of an indel
\item
  The optimal alignment score of \(S_1[1,n-1]\) and \(S_2\) plus the cost of an indel
\end{enumerate}

This simple relation can be used to compute optimal global alignment score for two sequences, however if it is implemented naively it can be very inefficient as the number of scores to compute grows exponentially with sequence lengths, and many intermediary alignment scores need to be computed many times. This is where dynamic programming comes in: these intermediary costs are pre-computed in an efficient manner and one can then deduce the optimal alignment from these. This pre-computing step is usually represented as filling out a matrix whose rows and columns represent the characters in each sequence to be aligned.

If \(S_1\) represents the rows of the matrix, and \(S_2\) the columns, the value \(C(i,j)\) of a cell \((i,j)\) of this matrix represents the optimal alignment score between \(S_1[1,i]\) and \(S_2[1,j]\). In the recurrence relation described above the alignment score as dependant on the optimal alignment scores of subsequences, when filling out the dynamic programming matrix we proceed in the inverse fashion by using the scores of short subsequences to build up the scores of progressively longer sequences.

We will go here through a short example showing how the NW algorithm is used to align two short sequences: \(S_1=\)\texttt{ACCTGA} and \(S_2=\)\texttt{ACGGA}. The first step is to represent the dynamic programming matrix, prefix each sequence with an empty character and label the rows of the matrix with one of the sequences and the columns with the other \emph{(this extra row and column at the beginning of each sequence are indexed as column and row 0)}. In this matrix, due to the recurrence relation stated above, the score of a particular cell, \(C(i,j)\) is the maximum of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The score in the diagonally adjacent cell \(C(i-1,j-1)\) plus the cost of a match or mismatch between \(S_1[i]\) and \(S_2[j]\).
\item
  The score of the cell to the left \(C(i,j-1)\) plus the cost of an indel
\item
  The score of the cell on top \(C(i-1,j)\) plus the cost of an indel
\end{enumerate}

Therefore in order to compute \(C(i,j)\) we need to know the three values of \(C(i-1,j-1)\), \(C(i-1,j)\) and \(C(i,j-1)\). This is the reason why we start with an extra column and row at the beginning of each sequence that we can fill out with the increasing costs of indels. In our case since the cost of an indel is -1, this row and column are filled out with decreasing relative integers, as can be seen in Figure \ref{fig:nwAlign}A.

From this starting point we can fill out the Dynamic programming matrix with all the alignment scores. To compute \(C(1,1)\) we have three possible values:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(C(0,0)\) plus the cost of a match between \(S_1[1]=A\) and \(S_2[1]=A\): \(0+1=1\)
\item
  \(C(0,1)\) plus the cost of an indel: \(-1 -1 = -2\)
\item
  \(C(0,1)\) plus the cost of an indel: \(-1-1=-2\)
\end{enumerate}

By taling the maximum out of these three values we can fill out the matrix with \(C(1,1)=1\). By continuing this process until we fill out the whole we obtain the scores visible below. This is enough if we only want to compute the optimal global alignment score between \(S_1\) and \(S_2\), however if we want to deduce the operations leading to alignment, and therefore the alignment itself, we need to keep track of which operation we made to get a specific score. The easiest way to do that is to also consider this matrix as a graph where each cell is a vertex. When we compute the score of cell \((i,j)\) we add an edge from this cell to the previous cell that was used to compute \(C(i,j)\). In our example above, we obtained \(C(1,1)\) from a match and \(C(0,0)\), so we can add an edge in our graph going from cell \((1,1)\) to cell \((0,0)\). The filled out matrix with the graph edges represented as arrows can be seen in Figure \ref{fig:nwAlign}B.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{./figures/Align-Intro/NW-total.png}
\extcaption{Example global alignment with the Needleman-Wunsch algorithm}{This figure represents three different steps in the NW algorithm, with a match cost of +1, a mismatch cost of -1 and an indel cost of -1. \textbf{A)} the matrix is initialized with $S_1$ as the columns and $S_2$ as the rows. Column and row 0 are filled out. \textbf{B)} The dynamic programming matrix is filled out, and the alignment graph is constructed. \textbf{C)} The alignment graph is traversed from the vertex in the bottom right cell to the vertex in the top left cell. Each of the three possible paths corresponds to an optimal global alignment, represented on the right.}
\label{fig:nwAlign}
\end{figure}

Once this matrix \emph{(and corresponding graph)} is filled out, we can deduce the alignment by following a path through the graph starting at cell \((n,m)\) to cell \((0,0)\). A diagonal edge starting at \((i,j)\) indicates a match or mismatch between \(S_1[i]\) and \(S_2[j]\), a vertical edge indicates a gap in \(S_2\) and a horizontal edge a gap in \(S_1\). This can lead to several optimal alignments if there are several such paths in the graph. In our case this algorithm yields three equally optimal global alignments shown in Figure \ref{fig:nwAlign}C.

This algorithm although guaranteed to result in an optimal alignment, has a time complexity of \(O(nm)\) where \(n\) and \(m\) are the lengths of the sequences to align \autocite{sungAlgorithmsBioinformaticsPractical2011}. Some methods have been proposed to speed up \autocite{masekFasterAlgorithmComputing1980}, however the complexity is still \(O(nm/\log(n))\). Lower bounds have been studied and there is not much optimization to be done if optimal exact alignment are needed \autocite{vinhInformationTheoreticMeasures2010,ullmanBoundsComplexityLongest1976}. If we want to do better we have to rely on heuristics.

Another issue is space complexity since we need to store the matrix, the space complexity is also \(O(nm)\). If we wish to align 2 human genomes we would need to store \(\approx 10^{19}\) matrix cells, which would amount to 10 Exabytes of storage if we use 8bit integers \emph{(i.e.~the storage scale of a data-center)}. However, in practice, we can do much better than that, and construct an optimal alignment in linear space complexity \(O(n+m)\) \autocite{hirschbergLinearSpaceAlgorithm1975} meaning we would only need a couple gigabytes to store the matrix for 2 human genomes. This resulted in an improved global alignment algorithm, the Myers-Miller algorithm \autocite{myersOptimalAlignmentsLinear1988}, implemented in the EMBOSS stretcher alignment software \autocite{riceEMBOSSEuropeanMolecular2000}.

\hypertarget{local-alignment}{%
\subsubsection{Local alignment}\label{local-alignment}}

In global alignment two full sequences are aligned to each other. In local alignment the goal is to find the optimal alignment of two subsequences from these parent sequences. The main algorithm for locally aligning is the Smith-Waterman (SW) algorithm \autocite{smithIdentificationCommonMolecular1981} developed a decade later than NW.

The two algorithms are very similar, SW also relies on first building the dynamic programming matrix with the same parametrizable costs for matches, mismatches and indels as NW. One key difference is that the optimal scores in the matrix are bound by 0 so they cannot become negative, we only store edges in the alignment graph is the starting cell has an alignment score \textgreater{} 0. If we use the SW algorithm to locally align the two example sequences \(S_1\) and \(S_2\) and the same costs as used above, we obtain the dynamic programming matrix and graph shown in Figure \ref{fig:swAlign}.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{./figures/Align-Intro/SW-total.png}
\extcaption{Example local alignment with the Smith-Waterman algorithm}{Two sequences $S_1$ and $S_2$ (the same as in Figure \ref{fig:nwAlign}) are locally aligned. A match has a cost of +1, a mismatch a cost of -1 and indels a cost of -1. \textbf{A)} The dynamic programming matrix is filled out and the alignment graph constructed. Alignment scores are constrained to be non-negative. \textbf{B)} We find paths in the graph between the vertex with the maximal score and a score of 0. Here there are two such paths resulting in two optimal local alignments represented on the right. }
\label{fig:swAlign}
\end{figure}

The traceback part to determine the optimal alignment is very similar to NW, however instead of starting at cell \((n,m)\), we start at the cell in with the maximal alignment score and follow the path back until we arrive at a cell with an alignment score of 0. In the example shown in Figure \ref{fig:swAlign}, two cells contain the maximal alignment score of 2. Tracing back from these cells gives two optimal local alignments between \(S_1\) and \(S_2\): \texttt{AC} to \texttt{AC} and \texttt{GA} to \texttt{GA}.

Since the SW algorithm is so similar to NW it has the same quadratic time and space complexity, however the same optimization can be used to bring it down to a linear space complexity \autocite{sungAlgorithmsBioinformaticsPractical2011}. Optimizations were resulted in the Huang and Miller algorithm \autocite{huangTimeefficientLinearspaceLocal1991} which is implemented in the EMBOSS Lalign tool \autocite{riceEMBOSSEuropeanMolecular2000}, and the Waterman Eggert algorithm \autocite{watermanNewAlgorithmBest1987}.

Both the NW and the SW algorithms are implemented in many different software tools and are used widely to perform pairwise alignments of short sequences \autocite{stajichBioperlToolkitPerl2002,gentlemanBioconductorOpenSoftware2004,riceEMBOSSEuropeanMolecular2000}. Some versions even benefit from hardware acceleration with version implemented for specific CPU instruction sets \autocite{dailyParasailSIMDLibrary2016} or GPUs \autocite{frohmbergGPASImprovedVersion2012} to substantially speed up alignment.

\hypertarget{scoring-and-substitution-models}{%
\subsection{Scoring and substitution models}\label{scoring-and-substitution-models}}

In the examples used above to present the NW and SW algorithms, we used a very simple cost function: a match has a cost of +1 while mismatches and indels have a cost of -1. This is really the simplest cost function we can use but also the crudest. In many cases it may be interesting to infuse this cost function with biological knowledge. For example some substitutions might occur more rarely than others in nature so it would stand to reason to penalize those more than other, more common, substitutions.

These biology-aware cost functions usually take the form of a matrix, called scoring or substitution matrix corresponding to an underlying evolutionary model. When using these matrices, matches and mismatches between specific characters are given. For example the cost of aligning an A and a T might be lower than aligning that same A with a G. A lot of different substitution matrices have been developed especially for protein alignments \autocite{altschulSubstitutionMatrices2013}, developed with different techniques and underlying models and with different use-cases.

The earliest and simplest substitution matrices are match/mismatch matrices, they are effectively what we used above where all matches are given a fixed positive score and all mismatches a fixed negative score. In our examples above the corresponding substitution matrix would be a four by four matrix with ones on the diagonal indicating matches and -1 everywhere else. These are simple and useful however when dealing with proteins they have a severe limitation as they ignore the biology of amino acids and.

In order to reflect this biological reality of proteins, new substitution matrices were developed using Log-odds models, based on the fact that substitutions in amino acids are not equiprobable, and some mutations between related amino acids \emph{(e.g.~I and L)} are much more common than others. Two of the most widely used substitution matrices, PAM and BLOSUM matrices, were built this way. The score for aligning residue \(i\) with residue \(j\) is given by the matrix entry \(s_{i,j}\) by looking at the background frequencies \emph{(i.e.~how often one expects to see a particular residue in a sequence)} of \(i\) and \(j\) denoted \(p_i\) and \(p_j\) respectively and the frequency \(q_{i,j}\) with which \(i\) and \(j\) are aligned in accurate biological alignments. from this we can compute the substitution score \(s_{i,j}\) as a Log-odds \autocite{altschulSubstitutionMatrices2013}:

\[
S_{i,j}=\log\bigg(\frac{q_{i,j}}{p_ip_j}\bigg)
\]

Both \(p_i\) and \(p_j\) are easy to compute from available biological sequence data, the real work in developing a Log-odds based substitution matrix is to estimate \(q_{i,j}\) values. To do this accurate biological alignments are needed, and how you obtain or choose these is determining of the substitution model. Finally this Log-odds formulation yields values with nice properties for local alignment, namely negative scores for unlikely substitutions and positive ones for likely substitutions and matches.

The PAM matrix (Point Accepted Mutation), developed in 1978 \autocite{dayhoffModelEvolutionaryChange1978}, is one such matrix. Using families of closely related proteins, and phylogenetic analysis, Dayhoff and colleagues estimated the probability of one amino acid replacing another over time to build an evolutionary model. Using this model they could estimate values for \(q_{i,j}\) and obtain a substitution matrix: the PAM matrix. Using this model, several PAM matrices were actually built, optimized for aligning sequences of different similarities: the PAM\textsubscript{250} matrix is suited to alignments where we expect 20\% of identical residues, the PAM\textsubscript{180} matrix however is suited to more closely related sequences where we expect 27\% of identical residues \autocite{altschulSubstitutionMatrices2013}. With the growing amount of sequence data and curated alignments, further refinements have been made leading to the development of PAM-like matrices based on the same principles \autocite{mullerModelingAminoAcid2000} but some improvements to the underlying mathematical model.

The other main type of substitution matrix are the BLOSUM matrices (Block Substitution matrix), developed in 1992 \autocite{henikoffAminoAcidSubstitution1992}. Instead of using whole, closely-related protein sequences like the PAM matrices, the values of \(q_{i,j}\) were estimated on highly conserved segments across whole protein families. Furthermore, to derive the BLOSUM scores, Henikoff and Henikoff did not estimate an underlying evolutionary model but only counted substitutions to estimate \(q_{i,j}\) values. Similarly to PAM matrices there are several BLOSUM matrices adapted to different levels of similarity between the sequences to be aligned.

PAM and BLOSUM matrices have fairly broad use-cases and are widely used in alignment, however there exist many other protein substitution models. Instead of using log-odds, some substitution models were developed by estimating scores with maximum-likelihood approaches \autocite{whelanGeneralEmpiricalModel2001,leImprovedGeneralAmino2008}. Some matrices were developed with very specific usage conditions in mind. Some are tailored to specific types of proteins like Transmembrane \autocite{mullerNonsymmetricScoreMatrices2001,ngPHATTransmembranespecificSubstitution2000}, disordered \autocite{trivediAminoAcidSubstitution2019} or polar/non-polar \autocite{goonesekereContextspecificAminoAcid2008} proteins. Some matrices were developed to align sequences from specific organisms like \emph{P. falciparum} \autocite{pailaGenomeBiasInfluences2008} \emph{(responsible for malaria)} or HIV \autocite{nickleHIVSpecificProbabilisticModels2007}. A substitution matrix was even developed in 2005 specifically for global rather than local alignment \autocite{sardiuScoreStatisticsGlobal2005}.

This wealth of protein substitution matrices reflects the biological and evolutionary diversity of proteins, however substitution matrices for aligning DNA sequences are much less frequent. Some work has been done to derive matrices similar to PAM matrices from DNA alignments \autocite{chiaromonteScoringPairwiseGenomic2001}. Codon substitution matrices \autocite{schneiderEmpiricalCodonSubstitution2005,doron-faigenboimCombinedEmpiricalMechanistic2007} have been developed as well, although they are used in DNA sequence alignment, ultimately they use knowledge derived from protein alignments.

\hypertarget{dealing-with-gaps}{%
\subsection{Dealing with gaps}\label{dealing-with-gaps}}

In the NW and SW examples of Section \ref{how-to-align-two-sequences}, as with the simplistic match/mismatch costs, we used a very simple cost of insertions and deletions: any indel has a cost of -1. However this, as was the case with substitutions, does not reflect the biological reality very well.

In biology, when insertions or deletions occur it is more likely that the indel will span several nucleotides rather than just one \autocite{cartwrightProblemsSolutionsEstimating2009}. This means that when inserting gaps into longer gap stretches are more likely than many individual gaps. For example the two alignments below have the same number of matches, mismatches and gaps. The second one is more likely since it is the result of a single insertion (or deletion) of \texttt{AGGT} rather than multiple independent indels.

\textbf{\texttt{AGGAGGTTCG}} ~~~~~~ \textbf{\texttt{AGGAGGTTCG}}\\
\textbf{\texttt{A-G-G-T-CC}} ~~~~~~ \textbf{\texttt{AGG-\/-\/-\/-TCC}}

The first approach was to try and optimize the gaps more generally \autocite{fitchOptimalSequenceAlignments1983} over the whole aligned sequence, however even with dynamic programming this has at best a time complexity of \(o(n^2m)\) \autocite{watermanBiologicalSequenceMetrics1976}. In 1982, Gotoh proposed affine gap costs \autocite{gotohImprovedAlgorithmMatching1982}, with this model there are two separate costs associated to indels: 1) the gap open cost and 2) the gap extend cost. Usually the costs are set up so that opening a new gap is more costly than extending it, meaning that longer gap stretches are favored over many short indels. The other major advantage is that with Gotoh's algorithm time complexity is back down to \(o(nm)\). The algorithm was further refined by Altschul \emph{et al.} \autocite{altschulOptimalSequenceAlignment1986}.

Over the years different types of gap costs were developed and tested like the logarithmic gap costs proposed by Waterman \autocite{watermanEfficientSequenceAlignment1984} and improved by Miller and Myers \autocite{millerSequenceComparisonConcave1988} turned out to be less accurate than affine gap costs \autocite{cartwrightLogarithmicGapCosts2006}). A bi-linear gap cost was also proposed to replace the affine cost\autocite{goonesekereFrequencyGapsObserved2004}, with a breakpoint at gaps of length three, the size of a codon. As more and more sequence data became available, similarly to what happened with substitution matrices, empirical profile-based models derived from this data were developed \autocite{bennerEmpiricalStructuralModels1993}. Some of these penalties leverage structural information and context for proteins \autocite{wrablGapsStructurallySimilar2004,zhangSP5ImprovingProtein2008}. A context dependent gap penalty depending on the hydrophobicity of aligned residues is inplemented in Clustal X \autocite{jeanmouginMultipleSequenceAlignment1998} one of the most widely used sequence aligners. Although quite complex and empirically derived, these profile-based penalties show limited improvement over the affine and bi-linear penalties \autocite{wangComparisonLinearGap2011}.

\hypertarget{how-do-we-speed-up-pairwise-alignment}{%
\section{How do we speed up pairwise alignment ?}\label{how-do-we-speed-up-pairwise-alignment}}

The NW and SW algorithms, as well as their improvements, are proven to be optimal \autocite{pearson27DynamicProgramming1992}. However when dealing with large sequences, which are more and more common, or when having to do many pairwise alignments they become limiting due to their time and space complexity. In many cases, to get around these limitations, optimality was left aside in favor of heuristics and approximate methods speeding up alignment.

\hypertarget{changing-the-method}{%
\subsection{Changing the method}\label{changing-the-method}}

One of the early approaches to speed up alignment was to focus on speeding up the dynamic programming which is the time and space consuming step of the NW and SW algorithms. Bounded dynamic programming \autocite{spougeSpeedingDynamicProgramming1989a} is one such approach. By making the assumption that the majority of alignment operations are matches and mismatches instead of indels we can make the hypothesis about the alignment graph. Most probably, the path in the graph corresponding to the optimal alignment will be around the diagonal of the dynamic programming matrix, and scores far away from the diagonal are probably not needed. By making these assumptions a lot of the scores of the matrix do not need to be computed, speeding up the execution and leading to a sparse dynamic programming matrix (shown in Figure \ref{fig:boundedDP}). This approach was used to speed up alignment early on in 1984 \autocite{fickettFastOptimalAlignment1984}. The advantage of this method is that the optimal alignment can be found very efficiently, however if there are many indels in the optimal alignment, this algorithm is not guaranteed to run faster than NW.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{./figures/Align-Intro/boundedDP.png}
\extcaption{Bounded dynamic programming to speed up alignment.}{CAPTION TODO... Adapted from \autocite{chaoDevelopmentsAlgorithmsSequence2022} \textbf{(Original figure here as a placeholder, I will adapt it)}
}
\label{fig:boundedDP}
\end{figure}

Another direction in order to speed up alignment was to leave dynamic programming to the side in favor of other methods and algorithms. One of these methods, fairly well studied in general, and in the context of alignment, are hidden markov models (HMMs). In certain circumstances PaiHMMs, HMMs used for pairwise alignment, can be mathematically equivalent to NW \autocite{durbinBiologicalSequenceAnalysis1998}. HMMs have been used for sequence alignment in many software tools like HHsearch \autocite{sodingProteinHomologyDetection2005}, HMMer \autocite{finnHMMERWebServer2011} or MCALIGN2 \autocite{wangMCALIGN2FasterAccurate2006} which is used to efficiently search for alignments in large databases of sequences.

More ``exotic'' methods have also been used successfully for sequence alignment. Fast Fourier Transform (FFT) are used in the MAFFT aligner \autocite{katohMAFFTNovelMethod2002} in order to quickly find homologous segments between two sequences. These homologous regions can be used as the basis for alignment. MAFFT, primarily a multiple sequence aligner \emph{(c.f. Section} \ref{multiple-sequence-alignment} \emph{below)}, can also be used for pairwise alignment.

Finally, a new exact alignment algorithm has been developed recently: the WaveFront algorithm (WFA) for pairwise alignment \autocite{marco-solaFastGapaffinePairwise2020}. WFA leverages homologous regions of both sequences to speed up the alignment. The complexity is now \(o(ns)\), so proportional to the sequence length and optimal alignment score instead of quadratic w.r.t. \(n\). This algorithm is also easily vectorizable and can take advantage of hardware acceleration, making its implementation run between 10 to 300 times faster than alternative methods depending on the testing context \autocite{marco-solaFastGapaffinePairwise2020}.

\hypertarget{seed-and-extend-with-data-structures}{%
\subsection{Seed and extend with data structures}\label{seed-and-extend-with-data-structures}}

In parallel to the development of new alignment algorithms, another way of substantially speeding up pairwise alignment is the so-called ``seed and extend'' method. This is based on the observation that a pairwise alignment most likely has several short subsequences that are almost identical in both sequences to align. These homologous subsequences, the seeds, can be used to initialize an alignment that can be extended in both directions with dynamic programming until we have a suitable alignment.

This method can be used for 1) local alignment, where seeds indicate possible local matches which can be extended in local alignments; or 2) for global alignment where the seeds anchor the dynamic programming matrix, limiting the number of cells to fill out as shown in Figure \ref{fig:anchor}. In both cases this approach follows the divide and conquer philosophy and extending seeds or filling out the matrix between anchors can be done independently and in parallel.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{./figures/Align-Intro/anchors.png}
\extcaption{Divide and conquer to speed up alignment.}{CAPTION TODO... Adapted from \autocite{chaoDevelopmentsAlgorithmsSequence2022} \textbf{(Original figure here as a placeholder, I will adapt it)}
}
\label{fig:anchor}
\end{figure}

This type of approach can also be used for many-to-one local alignments: either trying to find homologies between a query sequence and a database of sequences, or find several local alignments in a large reference sequence like in read-mapping (see Section \ref{what-is-read-mapping}). In these many-to-one scenarios it is useful to index seeds in data structures that allow rapid querying and compact storage. This general framework has proven to be quite flexible with many different ways to pick seeds \autocite{sunChoosingBestHeuristic2006} and many different data structures to index them \autocite{liSurveySequenceAlignment2010}.

\hypertarget{blast-and-hash-tables}{%
\subsubsection{BLAST and hash tables}\label{blast-and-hash-tables}}

One of the early methods for very quick heuristic alignment is the Basic Local Alignment Search Tool, BLAST \autocite{altschulBasicLocalAlignment1990}. It is widely used to this day to find homologous sequences in large databases and as such is one of the most cited papers of all time with over 100,000 citations, and is available as a web service hosted by the NCBI (\url{https://blast.ncbi.nlm.nih.gov/Blast.cgi}). Over the year many different versions for different use cases have been developed like BLASTP for protein sequences or BLASTN and MEGABLAST for nucleic acid sequences.

In our description of the BLAST algorithm we will have a target sequence and a query sequence that we wish to align.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For each sequence we build a hash table that uses subsequences of length \(k\), called \(k\)\emph{-mers}, as keys and their position in the whole sequence as values.
\item
  The hash tables are then scanned to check for exact matches between \(k\)-mers in the target and query sequences, called \emph{hits.}
\item
  The positions of the hits in the target and query sequences are used to seed a candidate local alignment
\item
  The candidate local alignment are extended in both directions from the seed with the SW algorithm. If the alignment score reaches a value under a specified threshold, the alignment stop and the candidate is discarded
\end{enumerate}

By selecting the right size \(k\) of the seeds (by default 11 when aligning nucleotides, 3 when aligning amino acids) as well as the alignment score threshold, one can adjust the sensitivity of the method at the cost of runtime. In purpose

It might not seem very useful to precompute the target hash-table for a single target. However, in practice BLAST is used to find local alignments between a query sequence and a very large number of target sequences; databases hosted by NCBI have hundreds of millions of target sequences (\url{https://ftp.ncbi.nlm.nih.gov/blast/db/}), at these scales pre-computing the target database saves an enormous amount of time.

Over time, several improvements have been developed for BLAST, PSI-BLAST \autocite{altschulGappedBLASTPSIBLAST1997} iteratively refines the alignments, Gapped BLAST \autocite{altschulGappedBLASTPSIBLAST1997}and BLASTZ \autocite{schwartzHumanMouseAlignments2003} use spaced seeds, introduced in the PatternHunter method \autocite{maPatternHunterFasterMore2002}, corresponding to seeds where not all characters match, increasing sensitivity. By sorting the target sequences it is possible to stop earlier and gain some speed as well \autocite{edgarSearchClusteringOrders2010}.

FASTA \autocite{pearsonImprovedToolsBiological1988}, an improvement on FASTP \autocite{lipmanRapidSensitiveProtein1985}, is another method for local alignment. Similarly to BLAST, \(k\)-mers for the target and query sequence are indexed in a hash table and hits are found between the two sequences. The \(k\)-mers used in the FASTA tool are usually shorter than for BLAST, so instead of initializing an alignment at a single hit, FASTA identifies regions in both sequences that have a high density of hits, keeping the best 10. These regions are then scored using matrices discussed in Section \ref{scoring-and-substitution-models} and high scoring regions are combined to build an approximate alignment. An optimal version of this alignment is then computed using the SW algorithm and banded dynamic programming.

Both FASTA and BLAST are very fast, it takes only a couple of seconds to find approximate local alignments between 100 query sequences \autocite{saripellaBenchmarkingNextGeneration2016} in a database of over 80 million target sequences \autocite{finnPfamProteinFamilies2016}. Trying this task with standard SW or NW algorithms would be much slower \autocite{essoussiComparisonFourPairwise2007} but would yield more sensitive, optimal alignments \autocite{shpaerSensitivitySelectivityProtein1996}.

\begin{itemize}
\item
  Minimap2 \autocite{liMinimap2PairwiseAlignment2018} minimizer (Figure \ref{fig:minimizers}) hash table
\item
  Diamond \autocite{buchfinkFastSensitiveProtein2015,buchfinkSensitiveProteinAlignments2021} which indexes both the reference and query at the same time.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{./figures/Align-Intro/minimizers.pdf}
\extcaption{Example of kmers sharing a minimizer}{}
\label{fig:minimizers}
\end{figure}

\hypertarget{suffix-trees-suffix-arrays}{%
\subsubsection{Suffix trees / suffix arrays}\label{suffix-trees-suffix-arrays}}

\begin{itemize}
\item
  Used in many pattern matching problems \autocite{gusfieldAlgorithmsStringsTrees1997}.
\item
  Software: AVID uses suffix trees to find anchors \autocite{brayAVIDGlobalAlignment2003}, MUMmer finds largest identical subsequences between 2 sequences to anchor alignments using suffix trees \autocite{delcherFastAlgorithmsLargescale2002}
\end{itemize}

\hypertarget{fm-index}{%
\subsubsection{FM Index}\label{fm-index}}

\begin{itemize}
\item
  FM index \autocite{ferraginaOpportunisticDataStructures2000}:
\item
  In some cases when using suffix trees take too much memory we can use an FM index which is based on the burrows wheeler transform \autocite{burrowsBlockSortingLosslessData1994}.
\item
  used for in exact and approximate string matching \autocite{sungAlgorithmsBioinformaticsPractical2011}
\item
  Software: BWT-SW uses an FM-index to speed up local alignment \autocite{lamCompressedIndexingLocal2008}, Bowtie 2 uses an FM index to find seeds \autocite{langmeadFastGappedreadAlignment2012}, BWA and BWA-SW use a similar idea \autocite{liFastAccurateShort2009,liFastAccurateLongread2010}, BWA-MEM \autocite{liAligningSequenceReads2013} and CUSHAW \autocite{liuLongReadAlignment2012} also uses FM indices to find exact matches to seed a local alignment .
\end{itemize}

Some newer alignment algorithms like WFA were also used in seed and extend approached like the recent genome to genome aligner AnchorWave \autocite{songAnchorWaveSensitiveAlignment2022}.

\hypertarget{multiple-sequence-alignment}{%
\section{Multiple sequence alignment}\label{multiple-sequence-alignment}}

When we need to compare a lot of individuals together we can do MSA, essential task in many bioinformatics analyses \autocite{russellMultipleSequenceAlignment2014}.

NP-hard \autocite{wangComplexityMultipleSequence1994,justComputationalComplexityMultiple2001} problem if you do it with DP so we need heuristics or tricks

Even if we align all sequences pairwise we need to then combine all gaps and stuff -\textgreater{} complicated.

\hypertarget{progressive-alignment}{%
\subsection{Progressive alignment}\label{progressive-alignment}}

guide tree, clustering of sequences then refine alignment. Good heuristic but with larger datasets, becomes harder. \autocite{fengProgressiveSequenceAlignment1987} Main MSA method.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute pairwise distance matrix for sequence set:

  \begin{itemize}
  \item
    either by doing N(N-1)/2 pairwise alignments
  \item
    Or alignment free methods to speed things up \autocite{jonesRapidGenerationMutation1992,blaisdellMeasureSimilaritySets1986}
  \end{itemize}
\item
  Build guide tree from distances (neighbor joining, UPGMA, \ldots)
\item
  Align sequences one by one according to the tree, from the leafs (i.e sequences) to the root (full MSA).
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{./figures/Align-Intro/progressive-aln.pdf}
\extcaption{Overview of the progressive alignment process.}{A) sequences to align B) guide tree constructed from distances C) Alignment steps along the guide tree and resulting MSA. Adapted from \autocite{sungAlgorithmsBioinformaticsPractical2011}}
\label{fig:progAlign}
\end{figure}

Problems -\textgreater{} keeps gaps and if bad alignment at first steps error propagates (``once a gap, always a gap'' \autocite{fengProgressiveSequenceAlignment1987}).

In order to curtail this problem we have iterative refinement \autocite{russellMultipleSequenceAlignment2014}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  create MSA (e.g.~progressive)
\item
  divide MSA into 2 groups + remove columns with only gaps
\item
  realign with profile alignment
\item
  Redo 2+3 until no improvement is made, according to some scoring function (Weighted sum-of-pair \autocite{altschulWeightsDataRelated1989}, or others like log-odds, correlation \autocite{edgarComparisonScoringFunctions2004} or consistency \autocite{notredameCOFFEEObjectiveFunction1998})
\end{enumerate}

Some of the most used MSA software uses these methods of progressive/iterative refinement:

\begin{itemize}
\item
  CLUSTAL-W \autocite{thompsonCLUSTALImprovingSensitivity1994} and CLUSTAL-X \autocite{thompsonCLUSTALWindowsInterface1997}
\item
  T-Coffee \autocite{notredameTcoffeeNovelMethod2000}
\item
  MUSCLE \autocite{edgarMUSCLEMultipleSequence2004a,edgarMUSCLEMultipleSequence2004}
\item
  MAFFT \autocite{katohMAFFTNovelMethod2002}, which uses Fast Fourier transforms to make the guide tree
\item
  ProbCons \autocite{doProbConsProbabilisticConsistencybased2005}
\end{itemize}

Different strenghts/weaknesses so many reviews and benchmarks to make a choice \autocite{notredameRecentEvolutionsMultiple2007,notredameRecentProgressMultiple2002,edgarMultipleSequenceAlignment2006,paisAssessingEfficiencyMultiple2014,thompsonBAliBASEBenchmarkAlignment1999}

\hypertarget{hmms-in-profile-profile-or-seq-profile-alignments}{%
\subsection{HMMs in profile-profile or seq-profile alignments}\label{hmms-in-profile-profile-or-seq-profile-alignments}}

HMM method similar performance to clustal-w \autocite{eddyMultipleAlignmentUsing}, HHMer \autocite{finnHMMERWebServer2011}, MSAProbs \autocite{liuMSAProbsMultipleSequence2010} and COVID-align \autocite{lemoineCOVIDAlignAccurateOnline2020}

Example of COVID where homology is high so we can get away with aligning pairwise to ancestral-sequence NextClade/NextAlign \autocite{aksamentovNextcladeCladeAssignment2021}

\hypertarget{other-methods-short}{%
\subsection{Other methods (short)}\label{other-methods-short}}

\begin{itemize}
\item
  Simulated annealing to speed up DP \autocite{kimMultipleSequenceAlignment1994,ishikawaMultipleSequenceAlignment1993,huoSimulatedAnnealingAlgorithm2007}
\item
  Genetic algorithm, review \autocite{chowdhuryReviewMultipleSequence2017}, can speed things up \autocite{zhangGeneticAlgorithmMultiple1997}

  \begin{itemize}
  \tightlist
  \item
    SAGA \autocite{notredameSAGASequenceAlignment1996}, VGDA \autocite{nazninVerticalDecompositionGenetic2011}, GAPAM \autocite{nazninProgressiveAlignmentMethod2012}
  \end{itemize}
\item
  recently: regressive method \autocite{garrigaLargeMultipleSequence2019} root to leaf allows to align a very large number of sequences: 1.4 million!
\end{itemize}

\hypertarget{the-specificities-of-read-mapping}{%
\section{The specificities of read-mapping}\label{the-specificities-of-read-mapping}}

Huge review of mapping in \autocite{alserTechnologyDictatesAlgorithms2021}

\hypertarget{what-is-read-mapping}{%
\subsection{What is read-mapping ?}\label{what-is-read-mapping}}

Task: locally/semi-globally align independent sequencing reads to a reference genome.

Plenty of aligners both for short reads (reviews \autocite{schbathMappingReadsGenomic2012,hatemBenchmarkingShortSequence2013,canzarShortReadMapping2017}) and long-reads (\autocite{liMinimap2PairwiseAlignment2018}. Active field and there are benchmarks \autocite{brindaRNFGeneralFramework2016} and reviews \autocite{alserTechnologyDictatesAlgorithms2021,ruffaloComparativeAnalysisAlgorithms2011}.

\begin{itemize}
\tightlist
\item
  Mapping quality:

  \begin{itemize}
  \item
    Intro and definition, quite a loosely defined term.

    \begin{itemize}
    \tightlist
    \item
      Different definitions/implementations ?
    \end{itemize}
  \item
    Mapping quality from tandem simulation \autocite{langmeadTandemSimulationFramework2017}
  \end{itemize}
\item
  Technical aspects to speed up, since we usually have a couple refs and and many many query reads, the index strategy described earlier is particularly well suited:

  \begin{itemize}
  \item
    hash tables: \autocite{liMinimap2PairwiseAlignment2018}
  \item
    FM index / BWT: BWT-SW \autocite{lamCompressedIndexingLocal2008}, Bowtie 2 \autocite{langmeadFastGappedreadAlignment2012}, BWA, BWA-SW \autocite{liFastAccurateShort2009,liFastAccurateLongread2010}, BWA-MEM \autocite{liAligningSequenceReads2013} and CUSHAW \autocite{liuLongReadAlignment2012}.
  \end{itemize}
\item
  Divide and conquer as well: \autocite{linKartDivideandconquerAlgorithm2017}
\item
  These heuristics also become more and more needed as sequencing technologies yield longer and longer reads.
\item
  Also hardware acceleration for short \autocite{olsonHardwareAccelerationShort2012} and long \autocite{chenAcceleratingNextGeneration2014,suzukiIntroducingDifferenceRecurrence2018,zeniLOGANHighPerformanceGPUBased2020} reads.
\end{itemize}

\hypertarget{challenges-of-read-mapping}{%
\subsection{Challenges of read-mapping}\label{challenges-of-read-mapping}}

\begin{itemize}
\tightlist
\item
  low homology / sequencing errors\ldots{} make mapping and other tasks hard \autocite{gusfieldAlgorithmsStringsTrees1997}

  \begin{itemize}
  \item
    Particularly with long reads since sequencing error rate is higher: specific long read mappers that take this into account:

    \begin{itemize}
    \item
      PB: BLASR \autocite{chaissonMappingSingleMolecule2012}, lordFAST \autocite{haghshenasLordFASTSensitiveFast2019}
    \item
      ONT: GraphMap \autocite{sovicFastSensitiveMapping2016}
    \item
      Both: NGMLR \autocite{sedlazeckAccurateDetectionComplex2018}, MashMap \autocite{jainFastApproximateAlgorithm2018}, DuploMap \autocite{prodanovSensitiveAlignmentUsing2020}
    \end{itemize}
  \end{itemize}
\item
  Repetitive regions (centromeres, telomeres) make it hard to map \autocite{alserTechnologyDictatesAlgorithms2021}. Specific tools have been developed for this:

  \begin{itemize}
  \item
    winnowmap \autocite{jainWeightedMinimizerSampling2020}, winnowmap2 \autocite{jainLongreadMappingRepetitive2022}
  \item
    tandemtools \autocite{mikheenkoTandemToolsMappingLong2020} for tandem repeats.
  \end{itemize}
\item
  homopolymers can also be a problem, so HPC is implemented in mappers \emph{(c.f.} \ref{hpc-trick}\emph{)}
\end{itemize}

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

\hypertarget{HPC-paper}{%
\chapter{Contribution 1: Improving read alignment by exploring a sequence transformation space}\label{HPC-paper}}

Recall that:

\begin{itemize}
\item
  Homopolymers are a problem (c.f. \ref{homopolymers-and-long-reads})
\item
  Mapping is hard (c.f. \ref{the-specificities-of-read-mapping})
\item
  HPC has been used successfully used to improve mapping (c.f. \ref{hpc-trick})
\end{itemize}

This chapter was written as an article titled:\\
\textbf{``Mapping-friendly sequence reductions: going beyond homopolymer compression''}\\
it was published in \textbf{\emph{DATE HERE}}, in the iScience proceedings of the RECOMB-SEQ 2022 conference (\href{https://doi.org/10.1371/journal.pcbi.1008873}{\emph{doi:10.1371/journal.pcbi.1008873}}).\\
The author list, complete with affiliations is given below:\\
\strut \\

Luc Blassel\textsuperscript{\textbf{1},\textbf{2}*}, Paul Medvedev\textsuperscript{\textbf{3},\textbf{4},\textbf{5}}, Rayan Chikhi\textsuperscript{\textbf{1}}

\textbf{1} Sequence Bioinformatics, Department of Computational Biology, Institut Pasteur, Paris, France\\
\textbf{2} Sorbonne Université, Collège doctoral, Paris, France\\
\textbf{3} Department of Computer Science and Engineering, Pennsylvania State University, University Park, Pennsylvania, United States of America\\
\textbf{4} Department of Biochemistry and Molecular Biology, Pennsylvania State University, University Park, Pennsylvania, United States of America\\
\textbf{5} Center for Computational Biology and Bioinformatics, Pennsylvania State University, University Park, Pennsylvania, United States of America\\
\strut \\

\newcommand{\stirling}[2]{\bigg\{%
\begin{matrix}
    #1 \\
    #2
\end{matrix}
\bigg\} }
\newcommand{\comb}[2]{\bigg(
\begin{matrix}
    #1 \\
    #2
\end{matrix}
\bigg) }

\newcommand{\lmer}{$\ell$-mer\xspace}
\newcommand{\lmers}{$\ell$-mers\xspace}
\newcommand{\iscomp}{\textsc{IsComp}\xspace}
\newcommand{\rccore}{RC-core-insensitive\xspace}
\newcommand{\minimap}{\texttt{minimap2}\xspace}
\newcommand{\winnowmap}{\texttt{winnowmap2}\xspace}
\newcommand{\msr}[1]{MSR$_{\text{#1}}$}

\hypertarget{highlights}{%
\section*{Highlights}\label{highlights}}
\addcontentsline{toc}{section}{Highlights}

\begin{itemize}
\item
  Mapping-friendly sequence reductions (MSRs) are functions that transform DNA sequences.
\item
  They are a generalization of the concept of homopolymer compression.
\item
  We show that some well-chosen MSRs enable more accurate long read mapping.
\end{itemize}

\hypertarget{graphical-abstract}{%
\section*{Graphical Abstract}\label{graphical-abstract}}
\addcontentsline{toc}{section}{Graphical Abstract}

\includegraphics[width=0.6\textwidth]{figures/HPC-MSRs/abstract.pdf}

\hypertarget{hpc-abstract}{%
\section*{Abstract}\label{hpc-abstract}}
\addcontentsline{toc}{section}{Abstract}

Sequencing errors continue to pose algorithmic challenges to methods working with sequencing data. One of the simplest and most prevalent techniques for ameliorating the detrimental effects of homopolymer expansion/contraction errors present in long read data is homopolymer compression. It collapses runs of repeated nucleotides, with the intuitive goal of removing some of the sequencing errors and often improving mapping sensitivity. Though our intuitive understanding justifies why homopolymer compression works, it in no way implies that it is the best transformation that can be done. In this paper, we explore if there are transformations that can be applied in the same pre-processing manner as homopolymer compression that would achieve better alignment sensitivity. We introduce a more general framework than homopolymer compression, called mapping-friendly sequence reductions. We transform the reference and the reads using these reductions and then apply an alignment algorithm. We demonstrate that some mapping-friendly sequence reductions lead to improved mapping accuracy, outperforming homopolymer compression.

\hypertarget{hpc-introduction}{%
\section{Introduction}\label{hpc-introduction}}

Sequencing errors continue to pose algorithmic challenges to methods working with read data. In short-read technologies, these tend to be substitution errors, but in long reads, these tend to be short insertions and deletions; most common are expansions or contractions of homopolymers (i.e.~reporting 3 As instead of 4)~\autocite{dohmBenchmarkingLongreadCorrection2020}. Many algorithmic problems, such as alignment, become trivial if not for sequencing errors~\autocite{gusfieldAlgorithmsStringsTrees1997}. Error correction can often decrease the error rate but does not eliminate all errors. Most tools therefore incorporate the uncertainty caused by errors into their underlying algorithms. The higher the error rate, the more detrimental its effect on algorithm speed, memory, and accuracy. While the sequencing error rate of any given technology tends to decrease over time, new technologies entering the market typically have high error rates (e.g.~Oxford Nanopore Technologies). Finding better ways to cope with sequencing error therefore remains a top priority in bioinformatics.

One of the simplest and most prevalent techniques for ameliorating the detrimental effects of homopolymer expansion/contraction errors is \emph{homopolymer compression} (abbreviated HPC). HPC simply transforms runs of the same nucleotide within a sequence into a single occurrence of that nucleotide. For example, HPC applied to the sequence AAAGGTTA yields the sequence AGTA. To use HPC in an alignment algorithm, one first compresses the reads and the reference, then aligns each compressed read to the compressed reference, and finally reports all alignment locations, converted into the coordinate system of the uncompressed reference. HPC effectively removes homopolymer expansion/contraction errors from the downstream algorithm. Though there is a trade-off with specificity of the alignment (e.g.~some of the compressed alignments may not correspond to true alignments) the improvement in mapping sensitivity usually outweighs it~\autocite{liMinimap2PairwiseAlignment2018}.

The first use of HPC that we are aware of was in 2008 as a pre-processing step for 454 pyrosequencing data in the Celera assembler~\autocite{millerAggressiveAssemblyPyrosequencing2008}. It is used by a wide range of error-correction algorithms, e.g.~for 454 data~\autocite{braggFastAccurateErrorcorrection2012}, PacBio data~\autocite{auImprovingPacBioLong2012}, and Oxford Nanopore data~\autocite{sahlinErrorCorrectionEnables2021}. HPC is used in alignment, e.g.~by the widely used minimap2 aligner~\autocite{liMinimap2PairwiseAlignment2018}. HPC is also used in long-read assembly, e.g.~HiCanu~\autocite{nurkHiCanuAccurateAssembly2020}, SMARTdenovo~\autocite{liuSMARTdenovoNovoAssembler2021}, or mdBG~\autocite{ekimMinimizerspaceBruijnGraphs2021}. HPC is also used for clustering transcriptome reads according to gene family of origin~\autocite{sahlinNovoClusteringLongRead2020}. Overall, HPC has been widely used, with demonstrated benefits.

Though our intuitive understanding justifies why HPC works, it in no way implies that it is the best transformation that can be done. Are there transformations that can be applied in the same pre-processing way as HPC that would achieve better alignment sensitivity? In this work, we define a more general notion which we call \emph{mapping-friendly sequence reductions}. In order to efficiently explore the performance of all reductions, we identify two heuristics to reduce the search space of reductions. We then identify a number of mapping-friendly sequence reductions which are likely to yield better mapping performance than HPC. We evaluate them using two mappers (\texttt{minimap2} and \texttt{winnowmap2}) on three simulated datasets (whole human genome, human centromere, and whole \emph{Drosophila} genome). We show that some of these functions provide vastly superior performance in terms of correctly placing high mapping quality reads, compared to either HPC or using raw reads. For example, one function decreased the mapping error rate of \texttt{minimap2} by an order of magnitude over the entire human genome, keeping an identical fraction of reads mapped.

We also evaluate whether HPC sensitivity gains continue to outweigh the specificity cost with the advent of telomere-to-telomere assemblies~\autocite{nurk2022}. These contain many more low-complexity and/or repeated regions such as centromeres and telomeres. HPC may increase mapping ambiguity in these regions by removing small, distinguishing, differences between repeat instances. Indeed, we find that neither HPC nor our mapping-friendly sequence reductions perform better than mapping raw reads on centromeres, hinting at the importance of preserving all sequence information in repeated regions.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{sec:msr-def}{%
\subsection{Streaming sequence reductions}\label{sec:msr-def}}

We wish to extend the notion of homopolymer compression to a more general function while maintaining its simplicity. What makes HPC simple is that it can be done in a streaming fashion over the sequence while maintaining only a local context. The algorithm can be viewed simply as scanning a string from left to right and, at each new character, outputting that character if and only if it is different from the previous character. In order to prepare for generalizing this algorithm, let us define a function \(g^\text{HPC} : \Sigma^2\rightarrow\Sigma\cup\{\varepsilon\}\) where \(\Sigma\) is the DNA alphabet, \(\varepsilon\) is the empty character, and

\begin{align*}
    & g^\text{HPC}(x_1\cdot x_2) =
    \begin{cases}
        x_2 & \text{if } x_1 \neq x_2 \\
        \varepsilon & \text{if } x_1 = x_2
    \end{cases}
\end{align*}

Now, we can view HPC as sliding a window of size 2 over the sequence and at each new window, applying \(g^\text{HPC}\) to the window and concatenating the output to the growing compressed string. Formally, let \(x\) be a string, which we index starting from 1. Then, the HPC transformation is defined as

\begin{equation}
  f(x) = x[1,\ell-1]\cdot g(x[1,\ell]) \cdot g(x[2, \ell+1])\cdots g(x[|x|-\ell+1,|x|]) 
  \label{eq:MSR}
\end{equation}

where \(\ell = 2\) and \(g=g^\text{HPC}\). In other words, \(f\) is the concatenation of the first \(\ell-1\) characters of \(x\) and the sequence of outputs of \(g\) applied to a sliding window of length \(\ell\) over \(x\). The core of the transformation is given by \(g\) and the size of the context \(\ell\), and \(f\) is simply the wrapper for \(g\) so that the transformation can be applied to arbitrary length strings.

With this view in mind, we can generalize HPC while keeping its simplicity by 1) considering different functions \(g\) that can be plugged into~Equation \eqref{eq:MSR} increasing the context that \(g\) uses (i.e.~setting \(\ell>2\)). Formally, for a given alphabet \(\Sigma\) and a context size \(\ell\), a function \(T\) mapping strings to strings is said to be an \emph{order-}\(\ell\) Streaming sequence reduction (abbreviated \emph{SSR}) if there exists some \(g : \Sigma^\ell\rightarrow\Sigma\cup\{\varepsilon\}\) such that \(T=f\).

Figure \ref{fig:countingMSRs}A shows how an SSR can be visualized as a directed graph. Observe that an order-\(\ell\) SSR is defined by a mapping between \(|\Sigma|^\ell\) inputs and \(|\Sigma| + 1\) outputs. For example, for \(\ell=2\), there are \(n=16\) inputs and \(k=5\) outputs. Figure \ref{fig:countingMSRs}B visualizes HPC in this way.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./figures/HPC-MSRs/panels_inline.pdf}
    \extcaption{Representing and counting Streaming sequence reductions.}{
        \textbf{A}: General representation of an order-2 Streaming sequence reduction as a mapping of 16 input dinucleotides, to the 4 nucleotide outputs and the empty character $\varepsilon$.
        \textbf{B}: Homopolymer compression is an order-2 SSR. All dinucleotides except those that contain the same nucleotide twice map to the second nucleotide of the pair. The 4 dinucleotides that are the two same nucleotides map to the empty character $\varepsilon$.
        \textbf{C}: Our RC-core-insensitive order-2 SSRs are mappings of the 6 representative dinucleotide inputs to the 4 nucleotide outputs and the empty character $\varepsilon$. The 4 dinucleotides that are their own reverse complement are always mapped to $\varepsilon$. The remaining 6 dinucleotides are mapped to the complement of the mapped output of the reverse complement dinucleotide input. For example, if AA is mapped to C, then TT (the reverse complement of AA) will be mapped to G (the complement of C).
        \textbf{D}: Number of possible SSR mappings under the different restrictions presented in the main text. All mappings from 16 dinucleotide inputs to 5 outputs (as in panel A) are represented by the outermost circle. All RC-core-insensitive mappings (as in panel C) are represented by the medium circle. All RC-core-insensitive mappings with only one representative of each equivalence class are represented by the innermost circle.
    }
    \label{fig:countingMSRs}
\end{figure}

Since we aim to use SSRs in the context of sequencing data, we need to place additional restrictions on how they handle reverse complements. For example, given two strings \(x\) (e.g.~a read) and \(y\) (e.g.~a substring of the reference), a mapper might check if \(x = RC(y)\). When strings are pre-processed using an SSR \(f\), it will end up checking if \(f(x) = RC(f(y))\). However, \(x = RC(y)\) only implies that \(f(x) = f(RC(y))\). In order to have it also imply that \(f(x) = RC(f(y))\), we need \(f\) to be commutative with RC, i.e.~applying SSR then RC needs to be equivalent to applying RC then SSR. We say that \(f\) is \emph{RC-insensitive} if for all \(x\), \(f(RC(x))= RC(f(x))\). Observe that HPC is RC-insensitive.

\hypertarget{sec:enum}{%
\subsection{Restricting the space of streaming sequence reductions}\label{sec:enum}}

To discover SSRs that improve mapping performance, our strategy is to put them all to the test by evaluating the results of an actual mapping software over a simulated test dataset reduced by each SSR. However, even with only \(16\) inputs and \(5\) outputs, the number of possible \(g\) mappings for order-2 SSRs is \(5^{16}\approx 1.5\cdot10^{11}\), which is prohibitive to enumerate. In this section, we describe two ideas for reducing the space of SSRs that we will test. In subsection~\ref{sec:rc-insensitive}, we show how the restriction to RC-insensitive mappings can be used to reduce the search space. In subsection~\ref{sec:equiv}, we exploit the natural symmetry that arises due to Watson-Crick complements to further restrict the search space.

These restrictions reduce the number of order-2 SSRs to only , making it feasible to test all of them. Figure \ref{fig:countingMSRs}D shows an overview of our restriction process.

\hypertarget{sec:rc-insensitive}{%
\subsubsection{Reverse complement-core-insensitive streaming sequence reductions}\label{sec:rc-insensitive}}

Consider an SSR defined by a function \(g\), as in Equation \eqref{eq:MSR}. Throughout this paper we will consider SSRs that have a related but weaker property than RC-insensitive. We say that an SSR is \emph{RC-core-insensitive} if the function \(g\) that defines it has the property that for every \(\ell\)-mer\(x\) and its reverse complement \(y\), we have that either \(g(x)\) is the reverse complement of \(g(y)\) or \(g(x) = g(y) = \varepsilon\). We will restrict our SSR search space to RC-core-insensitive reductions in order to reduce the number of SSRs we will need to test.

Let us consider what this means for the case of \(\ell=2\), which will be the focal point of our experimental analysis. There are 16 \(\ell\)-mers(i.e.~dinucleotides) in total. Four of them are their own reverse complement: AT, TA, GC, CG. The RC-core-insensitive restriction forces \(g\) to map each of these to \(\varepsilon\), since a single nucleotide output cannot be its own reverse complement. This leaves 12 \(\ell\)-mers, which can be broken down into 6 pairs of reverse complements. For each pair, we can order them in lexicographical order and write them as \((AA,TT), (AC,GT), (AG,CT), (CA,TG), (CC,GG),\) and \((GA,TC)\). Defining \(g\) can then be done by assigning an output nucleotide to the first \(\ell\)-mer in each of these pairs (Figure \ref{fig:countingMSRs}C). For example, we can define an SSR by assigning \(g(AA) = C\), \(g(AC) = C\), \(g(AG) = A\), \(g(CA) = A\), \(g(CC) = T\), and \(g(GA) = G\). As an example, let us apply the corresponding SSR to an example read \(r\):

\begin{align*}
    r & = \text{TAAGTTGA}    & f(RC(r)) &=\color{red}{\text{T}}\color{green}{\text{CACCTG}} \\
    f(r) & =\text{TCAGGTG}   & RC(f(r)) &=\;\;\;\color{green}{\text{CACCTG}}\color{red}{\text{A}} \\
    RC(r) & =\text{TCAACTTA} & &
\end{align*}

Observe that the first \(\ell-1\) nucleotides of \(r\) (shown in red) are copied as-is, since we do not apply \(g\) on them (as per Equation \eqref{eq:MSR}). As we see in this example, this implies that \(f(RC(r))\) is not necessarily equal to \(RC(f(r))\); thus an RC-core-insensitive SSR is not necessarily an RC-insensitive SSR. However, an RC-core-insensitive SSR has the property that for all strings \(r\), we have \(f(RC(r))[\ell, |r|]) = RC(f(r))[1, |r| - \ell + 1]\). In other words, if we drop the \(\ell - 1\) prefix of \(f(RC(r))\) and the \(\ell - 1\) suffix of \(RC(f(r))\), then the two strings are equal. Though we no longer have the strict RC-insensitive property, this new property suffices for the purpose of mapping long reads. Since the length of the read sequences will be much greater than \(\ell\) (in our results we will only use \(\ell=2\)), having a mismatch in the first or last nucleotide will be practically inconsequential.

It is important to note though that there may be other RC-insensitive functions not generated by this construction. For instance, HPC cannot be derived using this method (as it does not map the di-nucleotides AT,TA,GC and CG to \(\varepsilon\)), and yet it is RC-insensitive.

We can count the number of RC-core-insensitive SSRs. Let us define \(i(\ell)\) the number of input assignments necessary to fully determine the RC-core-insensitive SSR; one can think of this as the degrees-of-freedom in choosing \(g\). As we showed, for \(\ell=2\), we have \(i(\ell)=6\). The number of RC-core-insensitive SSRs is then \(5^{i(\ell)}\). Therefore, for \(\ell=2\), instead of \(5^{16}\) possible mappings we have at most \(5^{6}\approx1.5\cdot10^{4}\) RC-core-insensitive mappings (Figure \ref{fig:countingMSRs}D). For an odd \(\ell>2\), there are no \(\ell\)-mers that are their own reverse complements, hence \(i(\ell)=4^\ell/2\). If \(\ell\) is even then there are \(4^{\ell/2}\) inputs that are their own reverse complements (i.e.~we take all possible sequences of length \(\ell/2\) and reconstruct the other half with reverse complements). Thus, \(i(\ell)=(4^\ell- 4^{\ell/2})/2\).

\hypertarget{sec:equiv}{%
\subsubsection{Equivalence classes of SSRs}\label{sec:equiv}}

Non mapping-related preliminary tests led us to hypothesize that swapping \(A\leftrightarrow T\) and/or \(C\leftrightarrow G\), as well as swapping the whole \(A/T\) pair with the \(C/G\) pair in the SSR outputs would have a negligible effect on performance. In other words, we could exchange the letters of the output in a way that preserves the Watson-Crick complementary relation. Intuitively, this can be due to the symmetry induced by reverse complements in nucleic acid strands, though we do not have a more rigorous explanation for this effect. In this section, we will formalize this observation by defining the notion of SSR equivalence. This will reduce the space of SSRs that we will need to consider by allowing us to evaluate only one SSR from each equivalence class.

Consider an RC-core-insensitive SSR defined by a function \(g\), as in Equation \eqref{eq:MSR}. An \(\ell\)-mer is canonical if it is the not lexicographically larger than its reverse complement. Let \(I\) be the set of all \(\ell\)-mers that are canonical. Such an SSR's \emph{dimension} \(k\) is the number of distinct nucleotides that can be output by \(g\) on inputs from \(I\) (not counting \(\varepsilon\)). The dimension can range from \(1\) to \(4\). Next, observe that \(g\) maps all elements of \(I\) to one of \(k+ 1\) values (i.e.~\(\Sigma \cup \varepsilon\)). The output of \(g\) on \(\ell\)-mers not in \(I\) is determined by its output on \(\ell\)-mers in \(I\), since we assume the SSR is RC-core-insensitive. We can therefore view it as a partition of \(I\) into \(k+1\) sets \(S_0\), \ldots, \(S_k\), and then having a function \(t\) that is an injection from \(\{1, \ldots, k\}\) to \(\Sigma\) that assigns an output letter to each partition. Further, we permanently assign the output letter for \(S_0\) to be \(\varepsilon\). Note that while \(S_0\) could be empty, \(S_1, \ldots, S_k\) cannot be empty by definition of dimension. For example, the SSR used in Section \ref{sec:rc-insensitive} has dimension four and corresponds to the partition \(S_0 = \{\}, S_1=\{AG,CA\}\), \(S_2=\{CC\}\), \(S_3=\{AA,AC\}\), \(S_4=\{GA\}\), and to the injection \(t(1) = A\), \(t(2) =T\), \(t(3) = C\), and \(t(4) = G\).

Let \(\textsc{IsComp}(x,y)\) be a function that returns true if two nucleotides \(x, y \in \Sigma \cup \{\varepsilon\}\) are Watson-Crick complements, and false otherwise. Consider two SSRs of dimension \(k\) defined by \(S_0, \ldots, S_k, t\) and \(S'_0, , S'_k, t'\), respectively. We say that they are equivalent iff all the following conditions are met:

\begin{itemize}
\item
  \(S_0 = S'_0\),
\item
  there exists a permutation \(\pi\) of \(\{1,\ldots, k\}\) such that for all \(1 \leq i \leq k\), we have \(S_i = S'_{\pi(i)}\),
\item
  for all \(1 \leq i < j \leq k\), we have \(\textsc{IsComp}(t(i), t(j)) = \textsc{IsComp}(t'(\pi(i)), t'(\pi(j)))\).
\end{itemize}

One can verify that this definition is indeed an equivalence relation, i.e.~it is reflexive, symmetric, and transitive. Therefore, we can partition the set of all SSRs into equivalence classes based on this equivalence relation. One caveat is that a single SSR defined by a function \(g\) may correspond to multiple SSRs of the form \(S_0,\ldots,S_k,t\). However, these multiple SSRs are equivalent, hence the resulting equivalence classes are not affected. Furthermore, we can assume that there is some rule to pick one representative SSR for its equivalence class; the rule itself does not matter in our case.

Figure \ref{fig:figMSRConfigs} shows the equivalence classes for \(\ell=2\), for a fixed partition. An equivalence class can be defined by which pair of classes \(S_i\) and \(S_j\) have complementary outputs under \(t\) and \(t'\). Let us define \(o(k)\) as the number of equivalence classes for a given partition and a given \(k\). Then Figure \ref{fig:figMSRConfigs} shows that \(o(1)=1\), \(o(2)=2\) and \(o(3) = o(4) = 3\). There are thus only 9 equivalence classes for a given partition.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/HPC-MSRs/equivalence_classes_simpler.pdf}
    \extcaption{SSR equivalence classes for a fixed partition of the inputs.}{
    $S_0$ is always assigned $\varepsilon$, so it is represented by a gray node. 
    A blue link between $S_i$ and an $S_j$ denotes that $\textsc{IsComp}(t(i), t(j))=\text{true}$.
    The equivalence classes are determined by the Watson-Crick complementary relationships between the rest of the parts, i.e. by all the possible ways to draw the blue links.
    }
    \label{fig:figMSRConfigs}
\end{figure}

\hypertarget{counting-the-number-of-restricted-ssrs}{%
\subsubsection{Counting the number of restricted SSRs}\label{counting-the-number-of-restricted-ssrs}}

In this section, we derive a formula for the number of restricted SSRs, i.e.~SSRs that are RC-core-insensitive and that are representative for their equivalence class. Consider the class of RC-core-insensitive SSRs with dimension \(k\). In subsection~\ref{sec:rc-insensitive}, we derived that the degrees-of-freedom in assigning \(\ell\)-mers to an output is \(i(\ell) = 4^\ell/2\) if \(\ell\) is odd and \(i(\ell) = (4^\ell - 4^{\ell / 2})/2\) if \(\ell\) is even. Let \(C(\ell,k)\) be the number of ways that \(i(\ell)\) \(\ell\)-mers can be partitioned into \(k+1\) sets \(S_0, \ldots, S_k\), with \(S_1, \ldots, S_k\) required to be non-empty. Then, in subsection~\ref{sec:equiv}, we have derived \(o(k)\), the number of SSR equivalence classes for each such partition. The number of restricted SSRs can then be written as

\begin{equation}
N(\ell) = \sum_{k=1}^{4} C(\ell, k) \cdot o(k)
\label{eq:N}
\end{equation}

To derive the formula for \(C(\ell, k)\), we first recall that the number of ways to partition \(n\) elements into \(k\) non-empty sets is known as the Stirling number of the second kind and is denoted by \(\tiny\bigg\{%
\begin{matrix}
    n \\
    k
\end{matrix}
\bigg\} \) \autocite[p.265]{grahamConcreteMathematicsFoundation1994}. It can be computed using the formula

\begin{equation*}
    \bigg\{%
\begin{matrix}
    n \\
    k
\end{matrix}
\bigg\}  = \frac{1}{k!}\sum_{i=0}^k(-1)^i\bigg(
\begin{matrix}
    k \\
    i
\end{matrix}
\bigg) (k-i)^n
\end{equation*}

Let \(j\) be the number of the \(i(\ell)\) \(\ell\)-mers that are assigned to \(S_0\). Note this does not include the \(\ell\)-mers that are self-complementary that are forced to be in \(S_0\). Let \(C(\ell,k,j)\) be the number of ways that \(i(\ell)\) \(\ell\)-mers can be partitioned into \(k+1\) sets \(S_0, \ldots, S_k\), such that \(j\) of the \(\ell\)-mers go into \(|S_0|\) and \(S_1, \ldots, S_k\) to are non-empty. We need to consider several cases depending on the value of \(j\):

\begin{itemize}
\item
  In the case that \(j = 0\), we are partitioning the \(i(\ell)\) inputs among non-empty sets \(S_1, \ldots, S_k\). Then \(C(\ell, k,j) = \tiny{\bigg\{%
  \begin{matrix}
      i(\ell) \\
      k
  \end{matrix}
  \bigg\} }\).
\item
  In the case that \(1 \leq j \leq i(\ell) - k\), there are \(\tiny{\bigg(
  \begin{matrix}
      i(\ell) \\
      j
  \end{matrix}
  \bigg) }\) ways to choose which \(j\) \(\ell\)-mers are in \(S_0\), and \(\tiny{\bigg\{%
  \begin{matrix}
      i(\ell) - j \\
      k
  \end{matrix}
  \bigg\} }\) ways to partition the remaining \(\ell\)-mers into \(S_1, \ldots, S_k\). Hence, \(C(\ell, k,j) = \tiny{\bigg(
  \begin{matrix}
      i(\ell) \\
      j
  \end{matrix}
  \bigg) }\tiny{\bigg\{%
  \begin{matrix}
      i(\ell) - j \\
      k
  \end{matrix}
  \bigg\} }\).
\item
  In the case that \(j > i(\ell) - k\), it is impossible to partition the remaining \(k\) (or fewer) \(\ell\)-mers into \(S_1, \ldots, S_k\) such that the sets are non-empty. Recall that as we assume the dimension is \(k\), each set must contain at least one element. Hence, \(C(\ell, k,j) = 0\).
\end{itemize}

Putting this together into Equation~\eqref{eq:N}, we get

\begin{equation*}
    N(\ell) = \sum_{k=1}^4 o(k) \bigg( \bigg\{%
\begin{matrix}
    i(\ell) \\
    k
\end{matrix}
\bigg\}  + \sum_{j=1}^{i(\ell) - k}\bigg(
\begin{matrix}
    i(\ell) \\
    j
\end{matrix}
\bigg) \bigg\{%
\begin{matrix}
    i(\ell)-j \\
    k
\end{matrix}
\bigg\}   \bigg)
\end{equation*}

For \(\ell=2\), we have \(N(2)=2,135\) restricted SSRs, which is several orders of magnitude smaller than the initial \(5^{16}\) possible SSRs and allows us to test the performance of all of them. For order-3 SSRs we get \(N(3)=2.9\cdot10^{21}\) which much smaller than the full search space of \(5^{4^3}\approx5.4\cdot10^{44}\), for order-4 SSRs we get a similar reduction in search space with \(N(4)=9.4\cdot10^{84}\) as opposed to the full search space of \(5^{4^4}\approx8.6\cdot10^{178}\). For these higher order SSRs, although the restricted search space is much smaller than the full original one, it is still too large to exhaustively search.

\hypertarget{datasets-and-pipelines}{%
\section{Datasets and Pipelines}\label{datasets-and-pipelines}}

\hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

The following three reference sequences were used for evaluation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Whole human genome:} This reference sequence is a whole genome assembly of the CHM13hTERT human cell line by the Telomere-to-Telomere consortium \autocite{nurk2022}. We used the 1.1 assembly release (Genbank Assembly ID \href{https://www.ncbi.nlm.nih.gov/assembly/GCA_009914755.3/}{GCA\_009914755.3}).
\item
  \textbf{Whole \emph{Drosophila} genome:} This reference sequence is a whole genome assembly of a \emph{Drosophila melanogaster}, release 6.35 (Genbank Assembly ID \href{https://www.ncbi.nlm.nih.gov/assembly/GCF_000001215.4/}{GCA\_000001215.4}) \autocite{adamsGenomeSequenceDrosophila2000}.
\item
  \textbf{Synthetic centromeric sequence:} This sequence was obtained from the \texttt{TandemTools} mapper test data \autocite{mikheenkoTandemToolsMappingLong2020}. It is a simulated centromeric sequence that is inherently difficult to map reads to. Appendix S1 describes how it was constructed, and it is downloadable from \url{https://github.com/lucblassel/TandemTools/blob/master/test_data/simulated_del.fasta}
\end{enumerate}

\hypertarget{simulation-pipeline}{%
\subsection{Simulation pipeline}\label{simulation-pipeline}}

Given a reference sequence, simulated reads were obtained using \texttt{nanosim} \autocite{yangNanoSimNanoporeSequence2017} with the \texttt{human\_NA12878\_DNA\_FAB49712\_guppy\_flipflop} pre-trained model, mimicking sequencing with an Oxford Nanopore instrument. The number of simulated reads was chosen to obtain a theoretical coverage of whole genomes around 1.5x, this resulted in simulating \(\approx 6.6\cdot10^5\) reads for the whole human genome and \(\approx 2.6\cdot10^4\) reads for the whole Drosophila genome. Since the centromeric sequence is very short, we aimed for a theoretical coverage of 100x which resulted in \(\approx 1.3\cdot10^4\) simulated reads.

For each evaluated SSR, the reads as well as the reference sequence were reduced by applying the SSR to them. The reduced reads were then mapped to the reduced reference using \texttt{minimap2}\autocite{liMinimap2PairwiseAlignment2018} with the \texttt{map-ont} preset and the \texttt{-c} flag to generate precise alignments. Although HPC is an option in \texttt{minimap2} we do not use it and we evaluate HPC as any of the other SSRs by transforming the reference and reads prior to mapping. The starting coordinates of the reduced reads on the reduced reference were translated to reflect deletions incurred by the reduction process. The mapping results with translated coordinates were filtered to keep only the primary alignments. This process was done for each of our SSRs as well as with HPC and the original untransformed reads (denoted as \emph{raw}).

\hypertarget{evaluation-pipeline}{%
\subsection{Evaluation pipeline}\label{evaluation-pipeline}}

We use two metrics to evaluate the quality of a mapping of a simulated read set. The first is the \emph{fraction of reads mapped}, i.e.~that have at least one alignment. The second is the \emph{mapping error rate}, which is the fraction of mapped reads that have an incorrect location as determined by \texttt{paftools\ mapeval} \autocite{liMinimap2PairwiseAlignment2018}. This tool considers a read as correctly mapped if the intersection between its true interval of origin, and the interval where it has been mapped to, is at least 10\% of the union of both intervals.

Furthermore, we measure the mapping error rate as a function of a given \emph{mapping quality threshold}. Mapping quality (abbreviated mapq) is a metric reported by the aligner that indicates its confidence in read placement; the highest value (60) indicates that the mapping location is likely correct and unique with high probability, and a low value (e.g.~0) indicates that the read has multiple equally likely candidate mappings and that the reported location cannot be trusted. The mapping error rate at a mapq threshold \(t\) is then defined as the mapping error rate of reads whose mapping quality is \(t\) or above. For example, the mapping error rate at \(t=0\) is the mapping error rate of the whole read set, while the mapping error rate at \(t=60\) is the mapping error rate of only the most confident read mappings. Observe that the mapping error rate decreases as \(t\) increases.

\hypertarget{hpc-results}{%
\section{Results}\label{hpc-results}}

\hypertarget{selection-of-mapping-friendly-sequence-reductions}{%
\subsection{Selection of mapping-friendly sequence reductions}\label{selection-of-mapping-friendly-sequence-reductions}}

We selected a set of ``promising'' SSRs starting from all of the SSRs enumerated in Section~\ref{sec:enum}, that we call \emph{mapping-friendly sequence reductions} (abbreviated \emph{MSR}). The selection was performed by considering an independent read set of lower (0.5x) coverage, simulated from the whole human genome reference. This dataset is separate from the ones used for evaluation. Note that overfitting SSRs to a particular genome is acceptable in applications where a custom SSR can be used for each genome. Yet in this work, the same set of selected SSR will be used across all genomes.

\begin{figure}[t]
    \centering
    % \includegraphics[width=0.4\textwidth]{figures/main_text/hpc.png}
    \includegraphics[width=0.4\textwidth]{figures/HPC-MSRs/threshold_selection-v2.pdf}
    \extcaption{Illustration of how a respective mapq threshold is chosen for each of our evaluated MSRs.}{ 
        The orange dot shows the error rate and fraction of reads mapped for HPC at mapq threshold 60. 
        Anything below and to the right of this point is strictly better than HPC 60, i.e.
        it has both a lower error rate and higher fraction of reads mapped. 
        If an evaluated MSR does not pass through this region, then it is discarded from further consideration. 
        In the figure, the blue MSR does pass through this region, indicating that it is better than HPC 60. 
        We identify the leftmost point (marked as a blue dot) and use the mapq threshold at that point as the respective threshold.
        }
    \label{fig:thresholdChoice}
\end{figure}

For each evaluated SSR, we selected, if it exists, the highest mapq threshold for which the mapped read fraction is higher and the mapping error rate is lower than HPC at mapq 60 (\(0.93\) and \(2.1\cdot10^{-3}\) respectively), Figure \ref{fig:thresholdChoice} illustrates the idea. Then we identified the 20 SSRs that have the highest fraction of reads mapped at their respective thresholds. Similarly we identified the 20 SSRs with the lowest mapping error rate. Finally we select the 20 SSRs that have the highest percentage of thresholds ``better'' than HPC at mapq 60; i.e.~the number of mapq thresholds for which the SSR has both a higher fraction of reads mapped and lower mapping error rate than HPC at a mapq threshold of 60, divided by the total number of thresholds (=60).

The union of these 3 sets of 20 SSRs resulted in a set of 58 ``promising'' MSRs. Furthermore, we will highlight three MSRs that are ``best in their category'', i.e.

\begin{itemize}
\item
  \textbf{MSR}\textsubscript{F}: The MSR with the highest fraction of mapped reads at a mapq threshold of 0.
\item
  \textbf{MSR}\textsubscript{E}: The MSR with the lowest mapping error rate at its respective mapq threshold.
\item
  \textbf{MSR}\textsubscript{P}: The MSR with the highest percentage of mapq thresholds for which it is ``better'' than HPC at mapq 60.
\end{itemize}

Figure \ref{fig:topMSRs} shows the actual functions MSR\textsubscript{F}, MSR\textsubscript{E}, MSR\textsubscript{P}. An intriguing property is that they output predominantly As and Ts, with MSR\textsubscript{P} assigning only 2 input pairs to the G/C output whereas MSR\textsubscript{E} and MSR\textsubscript{F} assign only one. Additionally, MSR\textsubscript{E} and MSR\textsubscript{P} both assign the \{CC,GG\} input pair to the deletion output \(\varepsilon\) removing any information corresponding to repetitions of either G or C from the reduced sequence. Overall this means the reduced sequences are much more AT-rich than their raw counterparts, but somehow information pertinent to mapping is retained.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/HPC-MSRs/Top_MSRs.pdf}
    \extcaption{Graph representations of our highlighted MSRs: MSR$_{\text{E}}$,  MSR$_{\text{F}}$,  and MSR$_{\text{P}}$.}{
        MSR$_{\text{E}}$ has the lowest error rate of among MSRs at the highest mapq threshold for which it performs better than HPC at mapq 60, MSR$_{\text{F}}$ has the highest fraction of reads mapped at mapq 60 and MSR$_{\text{P}}$ has the highest percentage of mapq thresholds for which it outperforms HPC at mapq 60.
        The grayed out nodes represent the reverse complement of input dinucleotides and outputs, as in \autoref{fig:countingMSRs}C. For example for MSR$_{\text{E}}$, AA is mapped to T, so TT is mapped to A.}
    \label{fig:topMSRs}
\end{figure}

\hypertarget{mapping-friendly-sequence-reductions-lead-to-lower-mapping-errors-on-whole-genomes}{%
\subsection{Mapping-friendly sequence reductions lead to lower mapping errors on whole genomes}\label{mapping-friendly-sequence-reductions-lead-to-lower-mapping-errors-on-whole-genomes}}

Across the entire human genome, at high mapping quality thresholds (above 50), our selected 58 MSRs generally have lower mapping error rate than HPC and raw Figure \ref{fig:mapeval}A and Table \ref{tab:table-subset}. This is not surprising, as we selected those MSRs partly on the criteria of outperforming HPC at mapq 60; however, it does demonstrate that we did not overfit to the simulated reads used to select the MSRs.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/HPC-MSRs/MSR_mapeval_new_selection.reordered.pdf}
    \extcaption{Performance of our 58 selected mapping-friendly sequence reductions across genomes on reads simulated by \texttt{nanosim}}{
        Panel \textbf{A)} shows the whole human genome assembly, \textbf{B)} the subset of mapped reads from panel B that originate from repetitive regions, and \textbf{C)} the ``TandemTools'' synthetic centromeric reference sequence.
        We highlighted the best-performing mapping-friendly sequence reductions as MSR E, F and P, respectively in terms of cumulative \texttt{mapeval} error rate, fraction of reads mapped, and percentage of better thresholds than HPC.
        Each point on a line represents, from left to right, the mapping quality thresholds 60, 50, 40, 30, 20, 10 and 0. For the first point of each line, only reads of mapping quality 60 are considered, and the y value represents the rate of these reads that are not correctly mapped, the x value represents the fraction of simulated reads that are mapped at this threshold. The next point is computed for all reads of mapping quality $\geq50$, etc. The rightmost point on any curve represents the mapping error rate and the fraction of mapped reads for all primary alignments. The x-axes are clipped for lower mapped read fractions to better differentiate HPC, raw and MSRs E, F and P.
    }
    \label{fig:mapeval}
\end{figure}

Mapping quality is only an indication from the aligner to estimate whether a read mapping is correct, and according to Figure \ref{fig:mapeval}A the mapping error rate of most MSRs is low even for mapping qualities lower than 60. Therefore, we choose to compare MSR-mapped reads with lower mapping qualities against raw or HPC-mapped reads with the highest (60) mapping quality (which is the mapping quality thresholds most practitioners would use by default).

Table @ref(tab:table\_subset) shows that the three selected MSRs outperform both HPC and raw in terms of mapping error rate, with similar fractions of mapped reads overall. For example on the human genome, at mapq\(\geq 50\), MSR\textsubscript{F}, MSR\textsubscript{P} and MSR\textsubscript{E} all map more reads than either HPC or raw at mapq=60, and MSR\textsubscript{P} and MSR\textsubscript{E} also have mapping error rates an order of magnitude lower than either HPC or raw.

To evaluate the robustness of MSRs E, F and P we investigated the impact of mapping to a different organism or using another mapper. To this effect we repeated the evaluation pipeline in these different settings:

\begin{itemize}
\item
  Using the \emph{Drosophila melanogaster} whole genome assembly as reference and mapping with \texttt{minimap2}
\item
  Using the whole human genome assembly as reference but mapping with \texttt{winnowmap2}(version 2.02) \autocite{jainWeightedMinimizerSampling2020}. The same options as \texttt{minimap2} were used, and k-mers were counted using \texttt{meryl} \autocite{rhieMerquryReferencefreeQuality2020}, considering the top \(0.02\%\) as repetitive (as suggested by the \texttt{winnowmap2} usage guide).
\end{itemize}

MSRs E, F and P behave very similarly in both of these contexts compared to HPC/raw: by selecting mapped reads with mapq\(\geq\) 50 for the three MSRs we obtain a similar fraction of mapped reads with much lower error rates (Table \ref{tab:table-subset}). A noticeable exception is the \texttt{winnowmap2} experiment, where a larger fraction of raw reads are mapped than any other MSR and even HPC. A more detailed results table can be found in Table \ref{tab:mapperComparison}, and a graph of MSR performance on the whole Drosophila genome in Figure \ref{fig:drosophila-results}. As Figure \ref{fig:drosophila-results} shows, we also evaluated these MSRs on a whole \emph{Escherichia coli} (Genbank ID \href{https://www.ncbi.nlm.nih.gov/nuccore/U00096.2}{U00096.2}) genome, where we observed similar results, albeit the best MSRs do not seem to be one of our three candidates. This might mean that specific MSRs are more suited to particular types of genomes.

\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rlr@{}lr@{}lr@{}lr@{}lr@{}lr@{}l@{}}
\toprule
\multicolumn{2}{l}{}     & \multicolumn{4}{c}{Whole human genome}                                                  & \multicolumn{4}{c}{Whole human genome}                                                  & \multicolumn{4}{c}{Whole Drosophila genome}                                          \\
\multicolumn{2}{l}{}     & \multicolumn{4}{c}{\texttt{minimap2}}                                                            & \multicolumn{4}{c}{\texttt{winnowmap2}}                                                          & \multicolumn{4}{c}{\texttt{minimap2}}                                                         \\
\multicolumn{2}{r}{mapq} & \multicolumn{2}{l}{fraction}             & \multicolumn{2}{l}{error}                    & \multicolumn{2}{l}{fraction}             & \multicolumn{2}{l}{error}                    & \multicolumn{2}{l}{fraction}             & \multicolumn{2}{l}{error}                 \\ \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(l){11-14}
HPC         & 60         & 0.935          & {\:\footnotesize$+$0\%} & 1.85e-03          & {\:\footnotesize$+$ 0\%} & 0.894          & {\:\footnotesize$+$0\%} & 1.43e-03          & {\:\footnotesize$+$ 0\%} & 0.957          & {\:\footnotesize$+$0\%} & 2.27e-03      & {\:\footnotesize$+$  0\%} \\
raw         & 60         & 0.921          & {\:\footnotesize$-$1\%} & 1.86e-03          & {\:\footnotesize$+$ 0\%} & \textbf{0.932} & \textbf{\:\small$+$4\%} & 1.75e-03          & {\:\footnotesize$+$23\%} & 0.958          & {\:\footnotesize$+$0\%} & 2.27e-03      & {\:\footnotesize$-$  0\%} \\
MSR$_{\text{F}}$     & 50         & \textbf{0.938} & \textbf{\:\small$+$0\%} & 1.29e-03          & {\:\footnotesize$-$30\%} & 0.886          & {\:\footnotesize$-$1\%} & 3.82e-04          & {\:\footnotesize$-$73\%} & \textbf{0.960} & \textbf{\:\small$+$0\%} & 1.37e-03      & {\:\footnotesize$-$ 39\%} \\
MSR$_{\text{E}}$     & 50         & 0.936          & {\:\footnotesize$+$0\%} & \textbf{1.17e-04} & \textbf{\:\small$-$94\%} & 0.820          & {\:\footnotesize$-$8\%} & \textbf{8.93e-05} & \textbf{\:\small$-$94\%} & 0.954          & {\:\footnotesize$-$0\%} & \textbf{0.00} & \textbf{\:\small$-$100\%} \\ 
MSR$_{\text{P}}$     & 50         & \textbf{0.938} & \textbf{\:\small$+$0\%} & 4.15e-04          & {\:\footnotesize$-$78\%} & 0.845          & {\:\footnotesize$-$6\%} & 1.14e-04          & {\:\footnotesize$-$92\%} & 0.957          & {\:\footnotesize$+$0\%} & 8.11e-04      & {\:\footnotesize$-$ 64\%} \\ \bottomrule
\end{tabular}%
}
\extcaption{Performance of MSRs, HPC, and raw mappings across different mappers and reference sequences.}{
  For each reference sequence and mapper pair, we report the fraction of reads mapped (``fraction'' columns), the \texttt{paftools mapeval} mapping error rate (``error'' columns). The percentage differences are computed w.r.t to the respective HPC value. For HPC and the raw these metrics were obtained for alignments of mapping quality of 60. For MSRs E, F and P these metrics were obtained for alignments of mapping quality $\geq 50$. 
}
\label{tab:table-subset}
\end{table}

\hypertarget{mapping-friendly-sequence-reductions-increase-mapping-quality-on-repeated-regions-of-the-human-genome}{%
\subsection{Mapping-friendly sequence reductions increase mapping quality on repeated regions of the human genome}\label{mapping-friendly-sequence-reductions-increase-mapping-quality-on-repeated-regions-of-the-human-genome}}

To evaluate the performance of our MSRs specifically on repeats, we extracted the reads for which the generating region overlapped with the repeated region of the whole human genome by more than \(50\%\) of the read length. We then evaluated the MSRs on these reads only. Repeated regions were obtained from \url{https://t2t.gi.ucsc.edu/chm13/hub/t2t-chm13-v1.1/rmsk/rmsk.bigBed}.

We obtained similar results as on the whole human genome, with MSRs E, F and P performing better than HPC at mapq 50 (Figure \ref{fig:mapeval}B). At a mapq threshold of 50, the mapping error rate is \(53\%\), \(31\%\), and \(39\%\) lower than HPC at mapq 60 for MSRs E, F and P respectively, while the fraction of mapped reads remains slightly higher. At mapq=60, raw has an mapping error rate \(40\%\) lower than HPC but the mapped fraction is also \(17\%\) lower.

\hypertarget{raw-mapping-improves-upon-hpc-on-centromeric-regions}{%
\subsection{Raw mapping improves upon HPC on centromeric regions}\label{raw-mapping-improves-upon-hpc-on-centromeric-regions}}

On the ``TandemTools'' centromeric reference, HPC consistently maps a smaller fraction of reads than raw, across all mapping quality thresholds (Figure \ref{fig:mapeval}C). Additionally, the mapping error rate for raw is often inferior to that of HPC. The same is true for our selected MSRs: most of them have comparable performance to HPC, but none of them outperform raw mapping (Figure \ref{fig:mapeval}C).

We conjecture this is due to the highly repetitive nature of centromeres. HPC likely removes small unique repetitions in the reads and the reference that might allow mappers to better match reads to a particular occurrence a centromeric pattern. Mapping raw reads on the other hand preserves all bases in the read and better differentiates repeats. Therefore it seems inadvisable to use HPC when mapping reads to highly repetitive regions of a genome, such as a centromere.

\hypertarget{positions-of-incorrectly-mapped-reads-across-the-entire-human-genome}{%
\subsection{Positions of incorrectly mapped reads across the entire human genome}\label{positions-of-incorrectly-mapped-reads-across-the-entire-human-genome}}

To study how MSRs E, F, and P improve over HPC and raw mapping in terms of mapping error rate on the human genome, we selected all the primary alignments that \texttt{paftools\ mapeval} reported as incorrectly mapped. For HPC and raw, only alignments of mapping quality equal to 60 were considered. To report a comparable fraction of aligned reads, we selected alignments of mapping quality \(\geq50\) for MSRs. We then reported the origin of those incorrectly mapped reads on whole human genome reference, shown per-chromosome in Figure \ref{fig:errorHists}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/HPC-MSRs/ridgelines.new_selection.pdf}\\
    \extcaption{Histogram of the original simulated positions for the incorrectly mapped reads using \minimap at high mapping qualities across the whole human genome, for several transformation methods.}{For a given chromosome, each row represents the number of simulated reads starting at that particular region. The dark gray rectangle represents the position of the centromere for that chromosome, obtained from annotations provided by the T2T consortium (\href{http://t2t.gi.ucsc.edu/chm13/hub/t2t-chm13-v1.1/}{http://t2t.gi.ucsc.edu/chm13/hub/t2t-chm13-v1.1/}). Similarly for chromosomes 13, 14, 15, 21 and 22, a lighter gray rectangle represents the position of the ``stalk'' satellites also containing repetitive regions. For HPC and raw reads only alignments of mapping quality 60 were considered. To provide a fair comparison, alignments with mapping qualities $\geq 50$ were considered for MSRs E, F and P.}
    \label{fig:errorHists}
\end{figure}

We observe that erroneously mapped reads are not only those from centromeres, and instead originate from many other genomic regions. MSRs E and P have a markedly lower number of these incorrect mappings than either HPC or raw, with 1118 incorrect mappings for raw mappings and 1130 for HPC as opposed to 549, 970 and 361 for MSRs E, F and P respectively. This stays true even for difficult regions of the genome such as chromosome X, where raw and HPC have 70 incorrect mappings as opposed MSRs E and P that have 39, and 27 errors respectively.

We also investigated where all simulated reads were mapped on the whole human genome assembly, for raw, HPC and MSRs E,F and P in Figures \ref{fig:hist-raw} through \ref{fig:hist-msr-f}. The correctly mapped reads are, as expected, evenly distributed along each chromosome. The incorrectly mapped reads are however unevenly distributed. For most chromosomes there is a sharp peak in the distribution of incorrectly mapped reads, located at the position of the centromere. For the acrocentric chromosomes, there is a second peak corresponding to the ``stalk'' satellite region, with an enrichment of incorrectly mapped reads. This is expected since both centromeres and ``stalks'' are repetitive regions which are a challenge for mapping. For chromosomes 1, 9 and 16 however the majority of incorrectly mapped reads originate in repeated regions just after the centromere.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

We have introduced the concept of mapping-friendly sequence reduction and shown that it improves the accuracy of the popular mapping tool \texttt{minimap2} on simulated Oxford Nanopore long reads.

We focused on reads with high mapping quality (50-60), as it is a common practice to disregard reads with low mapping quality~\autocite{prodanovSensitiveAlignmentUsing2020,liNewStrategiesImprove2021,liSyntheticdiploidBenchmarkAccurate2018}. However across all mapped reads (mapq\(\geq 0\)), HPC and our MSRs have lower mapping accuracies than raw reads, consistent with the recommendation made in \texttt{minimap2} to not apply HPC to ONT data. Despite this, we newly show the benefit of using HPC (and our MSRs) with \texttt{minimap2} on ONT data when focusing on high mapping quality reads. Furthermore MSRs provide a higher fraction of high-mapq reads compared to both raw and HPC, as shown on the human and Drosophila genomes.

A natural future direction is to also test whether our MSRs perform well on mapping Pacific Biosciences long reads. Furthermore, it is important to highlight that our sampling of MSRs is incomplete. This is of course due to only looking at functions having \(l=2\), but also to the operational definition of RC-core-insensitive functions, and finally to taking representatives of equivalence classes. An interesting future direction would consist in exploring other families of MSRs, especially those that would include HPC and/or close variations of it.

Additionally, our analyses suggests to not perform HPC on centromeres and other repeated regions, hinting at applying sequence transformations to only some parts of the genomes. We leave this direction for future work.

\hypertarget{limitations-of-this-study}{%
\section{Limitations of this study}\label{limitations-of-this-study}}

Our proposed MSRs improve upon HPC at mapq 60, both in terms of fraction of reads mapped and mapping error rate on whole human, \emph{Drosophila melanogaster}, and \emph{Escherichia coli} genomes. We chose these sequences because they were from organisms that we deemed different enough, however it would be interesting to verify if our proposed MSRs are still advantageous on even more organisms, e.g.~more bacterial or viral genomes. This would allow us to assess the generalizability of our proposed MSRs.

We made the choice of using simulated data to be able to compute a mapping error rate. Some metrics, such as fraction of reads mapped might still be informative with regards to the mapping performance benefits of MSRs, even on real data. Evaluating the MSRs on real data might be more challenging but would offer insight into real-world usage of such pre-processing transformations.

The hypothesis we made in subsection \ref{sec:equiv} was derived from non mapping-related tests, it helped us reduce the search space and find MSRs. Testing if this hypothesis holds true on mapping tasks would help us make sure we are not missing some potentially well-performing SSRs by discarding them at this stage.

Finally, the restrictions we imposed to define RC-core-insensitive MSRs though intuitively understandable are somewhat arbitrary, so exploring a larger search space might be beneficial. Alternatively for higher order MSRs, even with our restrictions, the search spaces remain much too large to be explored exhaustively. To mitigate this problem, either further restrictions need to be found, or an alternative, optimization-based exploration method should be implemented.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

The authors thank Kristoffer Sahlin for feedback on the manuscript.

R.C was supported by ANR Transipedia, SeqDigger, Inception and PRAIRIE grants (ANR-18-CE45-0020, ANR-19-CE45-0008, PIA/ANR16-CONV-0005, ANR-19-P3IA-0001). This method is part of projects that have received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreements No 956229 and 872539. L.B was also supported by the ANR PRAIRIE grant (ANR-19-P3IA-0001). This material is based upon work supported by the National Science Foundation under Grant No.~1453527 and 1931531.

\hypertarget{author-contributions}{%
\section*{Author contributions}\label{author-contributions}}
\addcontentsline{toc}{section}{Author contributions}

Conceptualization, P.M. and R.C.; Methodology, L.B., P.M. and R.C.; Software, L.B.; Validation, L.B. and R.C.; Formal Analysis, L.B.; Investigation, L.B.; Resources, R.C.; Writing -- Original Draft, L.B., P.M. and R.C.; Writing -- Review \& Editing, L.B., P.M. and R.C.; Visualization, L.B.; Supervision, R.C.; Project Administration, R.C.; Funding Acquisition, R.C.;

\hypertarget{declaration-of-interests}{%
\section*{Declaration of interests}\label{declaration-of-interests}}
\addcontentsline{toc}{section}{Declaration of interests}

The authors declare no competing interests.

\hypertarget{star-methods}{%
\section*{STAR Methods}\label{star-methods}}
\addcontentsline{toc}{section}{STAR Methods}

\hypertarget{lead-contact}{%
\subsection*{Lead contact}\label{lead-contact}}
\addcontentsline{toc}{subsection}{Lead contact}

Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Rayan Chikhi (\href{mailto:rayan.chikhi@pasteur.fr}{\nolinkurl{rayan.chikhi@pasteur.fr}})

\hypertarget{materials-availability}{%
\subsection*{Materials availability}\label{materials-availability}}
\addcontentsline{toc}{subsection}{Materials availability}

This study did not generate new unique reagents.

\hypertarget{data-and-code-availability}{%
\subsection*{Data and code availability}\label{data-and-code-availability}}
\addcontentsline{toc}{subsection}{Data and code availability}

This paper analyzes existing, publicly available data. These accession numbers for the datasets are listed in the key resources table

All original code has been deposited at a github backed zenodo repository and is publicly available as of the date of publication. DOIs are listed in the key resources table, and the backing github repository is available at \href{https://github.com/lucblassel/MSR_discovery}{github.com/lucblassel/MSR\_discovery}.

Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request.

\hypertarget{method-details}{%
\subsection*{Method details}\label{method-details}}
\addcontentsline{toc}{subsection}{Method details}

All experiments performed for this article are implemented and documented as nextflow workflows available in this project's repository (\href{https://github.com/lucblassel/MSR_discovery}{github.com/lucblassel/MSR\_discovery}). These workflows may be used to rerun experiments and reproduce results. The repository also contains a Rmarkdown notebook to generate all figures and tables in the main text and supplemental information from the pipeline outputs.

\hypertarget{supplementary-information}{%
\section*{Supplementary information}\label{supplementary-information}}
\addcontentsline{toc}{section}{Supplementary information}

Supporting Information can be found in Appendix \ref{HPC-appendix}

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

\hypertarget{learning-from-alignments}{%
\chapter{Learning from alignments}\label{learning-from-alignments}}

\hypertarget{alignments-are-a-rich-source-of-information}{%
\section{Alignments are a rich source of information}\label{alignments-are-a-rich-source-of-information}}

\hypertarget{pairwise-alns}{%
\subsection{Pairwise alns}\label{pairwise-alns}}

we can compare sequences and say if an organism, or in the case of mapping get an idea of where on the genome we are sequencing

\hypertarget{msa}{%
\subsection{MSA}\label{msa}}

Here we have richer

\hypertarget{clustering}{%
\subsubsection{Clustering}\label{clustering}}

\begin{itemize}
\item
  Phylogenetic trees
\item
  Evolutionary inference
\item
  Protein structure prediction
\end{itemize}

\hypertarget{ml}{%
\subsubsection{ML}\label{ml}}

\begin{itemize}
\tightlist
\item
  Alphafold
\item
  Predict location / function
\item
  predict characteristics i.e.~resistance, \ldots{}
\end{itemize}

\hypertarget{preprocessing-the-alignment-for-machine-learning}{%
\section{Preprocessing the alignment for machine learning}\label{preprocessing-the-alignment-for-machine-learning}}

In order to do some learning we need to have the data in digestible form

\hypertarget{embedding-the-alignment}{%
\subsection{Embedding the alignment}\label{embedding-the-alignment}}

We need a way to represent, the position and the character in a sequence

\hypertarget{physico-chemical-embeddings}{%
\subsubsection{Physico-chemical embeddings}\label{physico-chemical-embeddings}}

AAIndex, or other embeddings, we add extra info, but we also make a string choice when deciding what features to add

\hypertarget{generalistic-categorical-embeddings}{%
\subsubsection{Generalistic categorical embeddings}\label{generalistic-categorical-embeddings}}

One-Hot, etc\ldots, easily interpretable\ldots{}

\hypertarget{learned-embeddings}{%
\subsubsection{Learned embeddings}\label{learned-embeddings}}

language models, transformers, etc\ldots{}

Powerful but hard to interpret what the model actually learns. i.e.~``black box''

\hypertarget{choosing-a-learning-target}{%
\subsection{Choosing a learning target}\label{choosing-a-learning-target}}

Of course one we have the data in digestible form we need an objective, a goal and once again a multitude

\hypertarget{regression}{%
\subsubsection{Regression}\label{regression}}

Either resistance level, IC50, \ldots{}

\hypertarget{classification}{%
\subsubsection{Classification}\label{classification}}

Resistant or not, compartments in the cell, \ldots{}

\hypertarget{task-based}{%
\subsubsection{Task-based\ldots{}}\label{task-based}}

end-to-end training like aligning sequences, this is harder because it requires developing a custom differentiable scoring function based on the task.

\hypertarget{how-to-learn-from-alns}{%
\section{How to learn from ALNs}\label{how-to-learn-from-alns}}

\hypertarget{tests-and-statistical-learning}{%
\subsection{Tests and statistical learning}\label{tests-and-statistical-learning}}

\begin{itemize}
\tightlist
\item
  correlation
\item
  Fisher
\item
  Multiple testing ?
\end{itemize}

\hypertarget{taking-interactions-into-account}{%
\subsection{Taking interactions into account}\label{taking-interactions-into-account}}

\begin{itemize}
\tightlist
\item
  Regressions w/ regularization
\item
  RF
\item
  \ldots{}
\end{itemize}

\hypertarget{deep-learning}{%
\subsection{Deep Learning}\label{deep-learning}}

\begin{itemize}
\tightlist
\item
  Steiner et al\ldots{}
\item
  plenty of other refs (DRMs + ML section from our minireview in Current opinions in virology 2021)
\end{itemize}

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

\hypertarget{viruses-hiv-and-drug-resistance}{%
\chapter{Viruses, HIV and drug resistance}\label{viruses-hiv-and-drug-resistance}}

\hypertarget{what-are-viruses}{%
\section{What are viruses ?}\label{what-are-viruses}}

Viruses occupy a strange place in the tree of life, with many debating if they are actually alive or not. André Lwoff gave what is probably the most fitting definition: \emph{``viruses are viruses''} \autocite{lwoffConceptVirus1957}. Despite this ambiguity, viruses share some common characteristics which allow us to define them as intracellular parasites \autocite{minorViruses2014}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Viruses have some type of genetic information, contained in either DNA or RNA
\item
  This genetic information is protected by some form of envelope
\item
  They use the cellular machinery of host cells to make copies of themselves.
\end{enumerate}

While we all know that viruses can be pathogenic and dangerous (the recent example of SARS-CoV2 springs to mind), that is not necessarily the case and some viruses like GBV-C \autocite{stapletonGBVirusesReview2011} and certain strains of H5N1 \emph{Influenza} \autocite{yamamotoCharacterizationNonpathogenicH5N12011} are non pathogenic and essentially harmless.

Viruses have been discovered for all three domains of life: Eukaryota, Bacteria and Archea. In Eukaryota many viruses have been discovered for animals (both vertebrate \autocite{shiEvolutionaryHistoryVertebrate2018} and invertebrate \autocite{adamsAtlasInvertebrateViruses2017}), plants \autocite{lefeuvreEvolutionEcologyPlant2019}, protozoa \autocite{wangVirusesParasiticProtozoa1991}, chromista \autocite{ferminVirusesProkaryotesProtozoa2018} and even fungi \autocite{sutelaVirusesFungiOomycetes2019}. Bacterial viruses known as phages have been known to exists since the beginning of the 20th century \autocite{twortINVESTIGATIONNATUREULTRAMICROSCOPIC1915,delbrockBacterialVirusesBacteriophages1946}. These bacteriophages are being considered as a therapeutic alternative to antibiotics \autocite{clarkBacterialVirusesHuman2004,vankan-davelaarUsingVirusesNanomedicines2014} which could help with multi-drug-resistant bacterial pathogens. Archea are also known to have their own viral infections \autocite{prangishviliVirusesArchaea2016,prangishviliVirusesArchaeaUnifying2006}.

Strangely even viruses of viruses seem to exist such as the plant satellite virus \autocite{franckiPlantVirusSatellites,xuPlantVirusSatellites2011} or hepatitis delta virus \autocite{laiMolecularBiologyHepatitis1995,hughesHepatitisDeltaVirus2011}. These ``viroids'' do not infect viral hosts \emph{per se} but they cannot replicate on their own. Replication must happen during co-infection with a larger virus. More recently, true viruses of viruses called virophages have been discovered. These virophages like sputnik \autocite{desnuesChapterSputnikVirophage2012} or zamilon \autocite{gaiaZamilonNovelVirophage2014} specifically infect giant viruses.

There is a huge diversity of viruses affecting all types of life, and new viruses are being discovered all the time \autocite{edgarPetabasescaleSequenceAlignment2022}. This diversity hints at a rich and long evolutionary history. When and where viruses originated is still under study \autocite{nasirInvestigatingConceptOrigin2020,forterreOriginViruses2009} and we might never know how they emerged, it is however believed that they may have played an important role in the emergence of eukaryotic cells \autocite{forterreOriginVirusesTheir2006}. This co-evolution between virus and host cell shows a strong link between the two organisms and some parts of the human genome are likely of ancient viral origin \autocite{boekeRetrotransposonsEndogenousRetroviruses1997,kojimaViruslikeInsertionsSequence2021}. It has been estimated that 1\% to 8\% of the human genome are endogenous retroviral sequences \autocite{lowerVirusesAllUs1996,griffithsEndogenousRetrovirusesHuman2001}.

The rich diversity of viruses is reflected in the variety of genetic information support, replication strategy, physical and genomic size, as well as shape. The differences in genetic information support and replication strategy form the basis of the Baltimore virus classification system \autocite{baltimoreExpressionAnimalVirus1971} , still used today \autocite{kooninBaltimoreClassificationViruses2021} to classify virus lineages.

As stated above all viruses have some genetic information, this information is stored either as DNA or as RNA, which is the molecule of choice for 70\% of human pathogenic viruses \autocite{domingoRNAVirusGenomes2018} \emph{(HIV and SARS-CoV 2 are RNA viruses)}.

For DNA viruses, the molecule can be double-stranded as for \emph{Herpesvirus} \autocite{mcgeochTopicsHerpesvirusGenomics2006,boehmerHerpesVirusReplication2003}, single-stranded like in the case of \emph{Papillomavirus} \autocite{brentjensHumanPapillomavirusReview2002} or even circular in the case of the Hepatitis B virus \autocite{kayHepatitisVirusGenetic2007}. This molecular diversity is also present in RNA viruses where the RNA molecule can be double-stranded like for \emph{Rotavirus} \autocite{parasharRotavirus1998}, or single-stranded. Furthermore, for single-stranded RNA viruses the strand can either be positive \emph{(i.e.~can be directly translated into a protein)} like Hepatitis C virus \autocite{simmondsVariabilityHepatitisVirus1995} or \emph{Poliovirus} \autocite{wimmerGeneticsPoliovirus1993,racanielloOneHundredYears2006}; conversely there are negative-strand RNA viruses, for which the complementary strand of RNA must be synthesized before translation into a protein, such as the Influenza or Measles viruses \autocite{paleseNegativestrandRNAViruses1996}.

This diversity in genetic information support implies a necessary diversity in replication strategy. The main replication strategies are as follows \autocite{domingoVirusEvolution2014}:

\begin{itemize}
\item
  The RNA molecule is directly copied as RNA. This is the strategy followed by single-stranded RNA coronaviruses \autocite{vkovskiCoronavirusBiologyReplication2021}, Dengue viruses \autocite{backDengueVirusesOverview2013} or Hepatitis C virus \autocite{dustinHepatitisVirusLife2016}.
\item
  The DNA molecule is directly replicated as DNA. this can happen for both single-stranded DNA viruses like \emph{Papillomavirus} \autocite{kadajaPapillomavirusDNAReplication2009}and double-stranded DNA viruses like Herpes simplex virus \autocite{wellerHerpesSimplexViruses2012}.
\item
  The DNA molecule is replicated by going through an RNA intermediary like Hepatitis B virus \autocite{beckHepatitisVirusReplication2007}.
\item
  The RNA molecule is replicated by going through a DNA intermediary. This strategy is used by retroviruses that integrate this viral DNA intermediary into the host DNA, like HIV-1 (see Section \ref{the-replication-cycle-of-hiv}).
\end{itemize}

Finally, the genetic diversity of viruses is reflected in their physical characteristics: viruses come in all shapes and sizes. Physical size range from 17nm for plant satellite viruses \autocite{pyleChapter58Biology2017} to the giant 400nm \emph{Mimivirus} \autocite{raoult2megabaseGenomeSequence2004}. Genomic size is also quite variable, there is a stark contrast between the 860 bp \emph{Circovirus SFBeef} and the 2.5 Mbp \emph{Pandoravirus salinus} genomes \autocite{campillo-balderasViralGenomeSize2015}. Viruses come in a variety of shapes \autocite{cannVirusStructure2015}: icosahedral for HIV, helical for the tobacco mosaic virus or a distinctive head-tail shape for bacteriophages.

Although there are a large number of viruses, and many of them are of great importance for human health, we will now focus on one virus of particular importance: Human Immunodeficiency Virus otherwise known as HIV.

\hypertarget{getting-to-know-hiv}{%
\section{Getting to know HIV}\label{getting-to-know-hiv}}

\hypertarget{quick-presentation-of-hiv}{%
\subsection{Quick Presentation of HIV}\label{quick-presentation-of-hiv}}

HIV is a single-stranded RNA retrovirus that is responsible for the Acquired Immune Deficiency Syndrome (AIDS) pandemic that has been around for the last couple decades. This virus is transmitted through sexual contact or through blood. Sexual activity is the largest transmission factor followed by intravenous drug use \autocite{hladikSettingStageHost2008,shawHIVTransmission2012}.

The HIV infects cells of the host immune system, specifically CD4 T-cell lymphocytes and destroys them due to it's replication process \autocite{weissHowDoesHIV1993}. CD4 T-cells are an essential part of the immune system response helping fight against infection in humans. An HIV infection typically start with an asymptomatic phase that can last years, followed by a growth in viral replication leading to a decrease in CD4 cells which progresses into AIDS \autocite{melhuishNaturalHistoryHIV2018}. During AIDS, when the CD4 cell count is low enough opportunistic diseases such as pneumonia or tuberculosis \autocite{murrayPulmonaryComplicationsAcquired1984} can easily infect the host, leading to death when the immune system is weak enough.

The HIV/AIDS is one of the deadliest pandemics in history, estimated to have lead to the death of 36 million people \autocite{sampathPandemicsThroughoutHistory2021}. In 2010 \autocite{worldhealthorganizationGlobalReportUNAIDS2010} approximately 33 million people were infected with HIV, 2.6 million of which were due to new infections, and 1.8 million died of AIDS. Most of the new infections happened in economically developing regions of the world, 70\% of them coming from sub-Saharan Africa \autocite{worldhealthorganizationGlobalReportUNAIDS2010}. As of 2020, these numbers have decreased with ``only'' 1.5 million new infections and 680,000 AIDS deaths, which is encouraging from a public health perspective.

The HIV-1 virus was discovered simultaneously in 1983 by Françoise Barré-Sinoussi, Luc Montagnier \autocite{barre-sinoussiIsolationTlymphotropicRetrovirus1983} and Robert Gallo \autocite{galloIsolationHumanTcell1983}. There exists a second HIV-2 virus discovered shortly after HIV-1 \autocite{clavelIsolationNewHuman1986}, it is however less transmissible than HIV-1 which is largely responsible for the global HIV/AIDS pandemic \autocite{gilbert2003}. In Africa in 2006, HIV-1 infections were rising where HIV-2 were declining \autocite{vanderloeffSixteenYearsHIV2006}.

While both viruses are of zoonotic origin, from transmissions of Simian Immunodeficiency Virus (SIV) from primates to humans. HIV-1 most likely originates from an SIV present in chimpanzees \autocite{gaoOriginHIV1Chimpanzee1999,hamelTwentyYearsProspective2007,sharpOriginsHIVAIDS2011}, and HIV-2 from an SIV present in Sooty mangabeys \autocite{hirschAfricanPrimateLentivirus1989,gaoHumanInfectionGenetically1992,chenGeneticCharacterizationNew1996}.

Several independent such transmissions have resulted in 4 lineages of HIV-1 labeled groups M, N, O and P \autocite{hemelaarOriginDiversityHIV12012} (similarly HIV-2 is split into groups A to H also resulting from independent zoonotic transmissions). While Groups N and P have been identified in only a handful of individuals in Cameroon , and group O is estimated to a few thousand cases in western Africa, the majority of the pandemic is due to viruses from group M.

The most most recent common ancestor, \emph{i.e.~the putative virus that founded group M}, is estimated to have originated in what is now the Democratic Republic of Congo \autocite{worobeyDirectEvidenceExtensive2008,vidalUnprecedentedDegreeHuman2000,fariaEarlySpreadEpidemic2014} at some point between 1910 and 1931 \autocite{korberTimingAncestorHIV12000,worobeyDirectEvidenceExtensive2008,rambautCausesConsequencesHIV2004}.

Group M is further subdivided into 9 subtypes each with distinct genetic characteristics, labeled A to K \autocite{mccutchanGlobalEpidemiologyHIV2006,sharpOriginsHIVAIDS2011}. Like in many viruses \autocite{perez-losadaRecombinationVirusesMechanisms2015}, when 2 genetically different strains of HIV co-infect a single host there is a risk genetic recombination leading to a new strain \autocite{robertsonRecombinationAIDSViruses1995}. During recombination, a new genome is formed from parts of the original genomes. This can lead to new strains that can spread and form lineages of their own. HIV strains resulting from recombination are called Cirulating Recombinant Forms (CRFs). There are currently 118 identifed HIV-1 CRFs in the Los Alamos National Laboratory HIV sequence database \autocite{HIVCirculatingRecombinant} (1 for HIV-2). There also exist many unique recombinant forms (URFs). Recombination can be particulary bothersome, complicating evolutionary analyses \autocite{posadaRecombinationEvolutionaryGenomics2002}, or facilitating drug resistance and hindering vaccine development \autocite{taylorChallengeHIV1Subtype2008}.

While subtype C represented almost half of global infections from 2004 to 2007, subtype B is the majority subtype in richer countries of North America and Western Europe \autocite{hemelaarGlobalTrendsMolecular2011} where sequencing efforts are more common. This accounts for an over-representation of subtype B sequences in public databases such as the Los Alamos sequence database where 54\% of sequences are of the B subtype and only 15\% are C \autocite{DistributionAllHIV1}.

\hypertarget{the-replication-cycle-of-hiv}{%
\subsection{The replication cycle of HIV}\label{the-replication-cycle-of-hiv}}

The viruses replication cycle and its immune-cell host specificity are what makes it particularly dangerous. This replication cycle can broadly be categorized into 9 separate steps \autocite{freedHIV1Replication2001,fergusonHIV1ReplicationCycle2002} shown in Figure \ref{fig:hivCycle}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  An HIV virion binds itself to the CD4 host cell through membrane proteins.
\item
  The virion envelope and host cell membrane fuse together allowing the viral genetic material and proteins to enter the host cell.
\item
  The viral RNA is reverse-transcripted into viral DNA
\item
  The viral DNA is integrated into the host cell genome
\item
  The integrated viral DNA is transcribed by the host cell machinery into multiple copies of viral RNA
\item
  The viral RNA is translated into immature viral polyproteins
\item
  The viral polyproteins are cleaved to form individual viral proteins.
\item
  The newly synthesized viral RNA and viral proteins gather around the host-cell membrane which starts budding to create a new virion
\item
  Once the budding is complete, the virion is released from the host cell and matures before being able to infect other CD4 cells and replicate again.
\end{enumerate}

The successive infection of CD4 cells by HIV virions leads to cellular death due to inflammatory response and/or activation of apoptosis \autocite{gougeonDirectIndirectMechanisms1993,vidyavijayanPathophysiologyCD4TCell2017}. The gradual depletion of CD4 cells in the infected individual's body lead to the suppression of the immune system, eventually leading to AIDS.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{./figures/HIV-Intro/HIV-cycle.pdf}
\extcaption{Main steps of HIV-1 replication cycle}{The HIV virion contains viral RNA and three essential proteins: Reverse Transcriptase (RT) represented in red, Integrase (IN) represented in cyan and Protease (PR) represented in yellow.}
\label{fig:hivCycle}
\end{figure}

\hypertarget{genetics-of-hiv}{%
\subsection{Genetics of HIV}\label{genetics-of-hiv}}

The replication cycle described in Section \ref{the-replication-cycle-of-hiv} is made possible by the 15 proteins of HIV. These proteins are coded by 9 separate genes \autocite{frankelHIV1FifteenProteins1998}. An overview of the HIV proteins, their structure and localization within the viral particle can be seen in Figure \ref{fig:hivStructure}.

The HIV genome is made up of three main genes each coding for polyproteins and six genes coding for proteins with regulatory and accessory roles. The three polyproteins correspond to long chains of amino acids which are subsequently cleaved at specific positions to result in separate viral proteins.

The \emph{gag} (``group-specific-antigen'') gene codes for the Gag polyprotein which, once cleaved, results in four proteins with mainly structural roles:

\begin{itemize}
\item
  The Matrix protein (MA or p17) lines the internal surface if the virion membrane maintaining the shape and structural integrity of the virion.
\item
  The Capsid protein (CA or p24) forms an inner core (the capsid) inside the virion around the viral RNA. It helps protect the viral genetic information.
\item
  The Nucleocapsid protein (NC or p7) binds with the viral RNA inside the capsid, stabilizing the molecule and further protecting the genetic information.
\item
  The p6 protein is a small, largely unstructured protein \autocite{fossenSolutionStructureHuman2005} that is suspected of playing a role in virion budding and release from the host cell at the end of the replication cycle \autocite{gottlingerEffectMutationsAffecting1991,huangP6GagRequiredParticle1995}.
\end{itemize}

The \emph{pol} (``polymerase'') gene coding for the Pol polyprotein. After cleaving this results in the three essential viral enzymes at the heart of the replication cycle:

\begin{itemize}
\item
  The Protease (PR) is responsible for cleaving the Gag, Pol and Env polyproteins to get the individual viral proteins. Without it the individual viral proteins cannot come into being and therefore cannot function, stopping viral replication.
\item
  The Reverse Transcriptase (RT or p51/p66) is responsible for synthesizing viral DNA from the viral RNA template contained in the virion. This is the first step in hijacking the cellular machinery for replication. Without viral DNA, HIV replication is impossible.
\item
  The Integrase (IN) is responsible for integrating the viral DNA produced by RT in to the host cell DNA. Once the viral DNA is inside the host genome it can be transcribed and then translated (as described in Section \ref{biological-sequences-a-primer}) to produce new copies of the viral RNA and proteins. Without this integration step the viral genetic information cannot be expressed and the replication cycle is stopped.
\end{itemize}

These three proteins are of particular importance and we will go into more detail about them in Section \ref{drug-mechanisms}.

The \emph{env} (``envelope'') gene code for the Env, the third and last polyprotein. The two resulting proteins coat the membrane of the virion and are responsible for binding with the CD4 host cells.

\begin{itemize}
\item
  The Surface protein (SU or gp120) binds to receptors on the surface of CD4 cells and allow the virion to attach itself to the host cell \autocite{bourHumanImmunodeficiencyVirus1995}. It also enables membrane fusion, the essential first step in the viral replication cycle \autocite{hernandezViruscellCellcellFusion1996}.
\item
  The Transmembrane protein (TM or gp41) anchors SU into the virion membrane.
\end{itemize}

The 6 remaining genes all code for single proteins. Two of these have essential regulatory roles and the remaining four have accessory roles.

The \emph{tat} (``trans-activator of transcription'') gene codes for Tat, the first essential regulatory protein. Tat activates and promotes transcription leading to more numerous and longer copies of the viral RNA \autocite{jonesControlRnaInitiation1994}. The \emph{rev} (for ``regulator of virion'') gene codes for Rev, the second essential regulatory protein. Rev helps transcribed viral RNA exist the nucleus of the host cell in order to be translated to viral proteins or be packages in new, budding virions \autocite{hopeViralRNAExport1997}.

The remaining four accessory genes are as follows: \emph{nef} (``negative regulatory factor'') code for the Nef protein which prevents the production of the CD4 cellular defense proteins increasing infectivity \autocite{mangasarianMultifacetedRoleHIV1997}; \emph{vif} (``viral infectivity factor'') codes for the Vif protein which also increases viral infectivity \autocite{cohenRoleAuxiliaryProteins1996}; \emph{vpu} (``viral protein U'') codes for Vpu which likely helps during release of new virions \autocite{lambVpuVprHuman1997,cohenRoleAuxiliaryProteins1996} as well as preventing production of CD4 in the host cell, it is not believed to be present in the mature virion as it binds to host cellulat membranes \autocite{khanRoleViralProtein2021}; \emph{vpr} (``viral protein R'') likely helps viral DNA enter the host cell nucleus and prevents the natural host cell reproduction cycle \autocite{emermanHIV1VprCell1996}.

The existence of a 10th HIV-1 gene was suggested in 1988 \autocite{millerHumanImmunodeficiencyVirus1988}, overlapping the \emph{env} gene and coding for proteins on the other strand of viral DNA than the other genes. This putative gene was named \emph{asp} (``antisense protein'') and Asp transcripts were isolated during an HIV-1 infection in 2002 \autocite{briquetImmunolocalizationStudiesAntisense2002}. The function of this protein is still unknown but it has been shown to have a strong evolutionary correlation with HIV-1 group M responsible for the pandemic \autocite{cassanConcomitantEmergenceAntisense2016}. Although this Asp protein is still a source of debate, it is under active research \autocite{savoretPilotStudyHumoral2020}.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/HIV-Intro/HIV-structure.png}
  \extcaption{Structure and main components of a mature HIV-1 virion.}{Structural proteins MA, CA, SU and TM are represented in Blue, functional enzymes RT, IN and PR in pink, RNA binding proteins Rev, Tat and NC in orange and accessory proteins Vif, Nef, Vpr and Vpu in green. Viral RNA is shown in yellow. The phospholipd membrane of the virion is shown in a light purple color. The p6 protein is not represented as it is largely unsctructured. Vpu is not believed to be present in the HIV virion. \\
Figure adapted from PDB101 \autocite{zardecki2022} (\href{https://PDB101.rcsb.org}{PDB101.rcsb.org}, \textit{CC By 4.0 License}, detailed list of structures used available in Appendix \autoref{HIV-intro-appendix}).}
\label{fig:hivStructure}
\end{figure}

\hypertarget{drug-resistance-in-hiv}{%
\section{Drug resistance in HIV}\label{drug-resistance-in-hiv}}

Although the HIV/AIDS pandemic has been very deadly around the world, we are not completely defenseless against it. The first antiretroviral therapy (ART) drugs were made available in the late eighties, only a couple years after discovery of the virus. ART reduce the viral load in an HIV positive patient reducing its transmissibility \autocite{eisingerHIVViralLoad2019}, while ART is not a cure for an HIV infection it has been shown to drastically reduce mortality and morbidity \autocite{palellaDecliningMorbidityMortality1998}. ART is estimated to have saved the lives of 9.5 million individuals between 1995 and 2015 \autocite{forsytheTwentyYearsAntiretroviral2019}.

\hypertarget{a-quick-history-of-art}{%
\subsection{A quick history of ART}\label{a-quick-history-of-art}}

The first available anti-HIV drug was Zidovudine (ZDV, also known as azidothymidine or AZT) approved by the FDA for usage in the USA in 1987 \autocite{fischlEfficacyAzidothymidineAZT1987}, a few years only after the discovery of the virus. This drug was a reverse transcriptase inhibitor (RTI) therefore preventing the viral RNA from being transcribed into viral DNA. Unfortunately, 3 years later strains of HIV resistant to ZDV were circulating \autocite{richmanSusceptibilityNucleosideAnalogues1990}. This rapid emergence of resistance to treatment is common for HIV \autocite{yeoDeterminationHIV1RT2020} due to its very high evolution rate \autocite{cuevasExtremelyHighMutation2015} allowing it to explore many possible mutations in response to selective pressures. To counter this resistance new drugs were rapidly developed and, between 1988 and 1995, four more RTIs were approved by the FDA. Using a combination of these drugs was also shown to be effective and led to a slower rise of resistance \autocite{gulickTreatmentIndinavirZidovudine1997}.

Then focus was shifted to the development of a new type of drug: Protease Inhibitors (PI). Between 1995 and 1997 4 of them were approved. These, taken in combination with RTI made if harder for the virus to develop resistance \autocite{wensingFifteenYearsHIV2010}. A new class of RTIs was also explored, Non-Nucleoside RTIs (NNRTIs) that block the RT action in another manner than the previously approved Nucleoside RTIs (NRTIs). When taken in combination with other drugs they are also highly effective \autocite{pedersenNonNucleosideReverseTranscriptase1999}. As the years advanced even more drug targets were explored, with 5 Integrase inhibitors (INSTI) being approved since 2007 \autocite{scarsiHIV1IntegraseInhibitors2020}, A Fusion Inhibitor (FI) in 2003 \autocite{fletcherEnfuvirtideNewDrug2003}, and 3 other Entry inhibitors (EI) \autocite{esteHIVEntryInhibitors2007,kilbyNovelTherapiesBased2003} since 2007 all targeting different steps in the replication cycle of HIV \emph{(see Table} \ref{tab:tableDrugs} and Figure \ref{fig:timeline}\emph{)}.

In response to the rapid emergence of resistance in HIV when treated with a single drug, clinicians started systematically treating HIV with a combination of multiple drugs targeting different proteins, as early as 1996. This is now referred to as Highly active antiretroviral combination therapy (HAART, also known as tritherapy). HAART usually consists of 2 NRTIs coupled with another drug: NNRTI or PI at first and later FI or INSTI \autocite{yeniUpdateHAARTHIV2006}. As of 2008 22 anti-HIV single drugs were approved by the FDA \autocite{palmisanoBriefHistoryAntiretroviral2011}, and 27 as of today. This large array of available drugs made HAART possible and gave options to clinicians to switch targets when the multi-resistant HIV emerged.

With the advent of HAART, patients had access to more potent treatments, however complexity of the treatment regimen grew. Treatments often involved several pills a day to take at precise intervals. Complex drug regimens have been associated with poorer treatment adherence \autocite{mehtaPotentialFactorsAffecting1997,millerComplianceTreatmentRegimens1997}, which can lead to poor treatment outcome as well as the emergence of multi-resistant HIV strains \autocite{chesneyAdherenceHIVCombination2000} and its spread within the population. To avoid this issue, increasingly more single pill regimens are being developed with a staggering 7 new drugs approved by the FDA in 2018. These single pill regimens greatly reduce the burden of adherence for patients leading to better therapeutic outcomes and reduced healthcare costs \autocite{aldirSingletabletRegimensHIV2014}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figures/HIV-Intro/complete.pdf}
  \extcaption{Timeline of ART single drug FDA approvals.}{Colored by drug type: Nucleoside Reverse transcriptase inhibitors (NRTI), Non-Nucleoside Reverse transcriptase inhibitors (NNRTI), Protease Inhibitors (PI), Integrase inhibitors (INSTI), Entry Inhibitors (EI) and pharmacokinetic enhancers (PE). Fixed Dose Combination (FDC) single pill regimens are also shown.\\
* RPV is often also used as a pharmacokinetic enhancer in combination with other drugs.\\
\dag These drugs are no longer approved by the FDA or no longer recommended as first line regiment treatment.\\
\textit{Information collected from \url{https://hivinfo.nih.gov/understanding-hiv/fact-sheets/fda-approved-hiv-medicines}, 
\url{https://hivinfo.nih.gov/understanding-hiv/infographics/fda-approval-hiv-medicines} and \url{https://www.accessdata.fda.gov/scripts/cder/daf/index.cfm}}.\\
See also Table \ref{tab:tableDrugs}.
}
\label{fig:timeline}
\end{figure}

Most recently, some studies explored using some of these single pill regimens (such as Truvada, c.f. Table \ref{tab:tableDrugs}) as prophylactics \emph{i.e.} Pre-exposure prophylaxis (PrEP). Putting uninfected but at risk populations on ART before any known exposure has been shown to effectively lower the risk of infection \autocite{grantPreexposureChemoprophylaxisHIV2010,baetenAntiretroviralProphylaxisHIV2012,buchbinderPreexposureProphylaxisPromise2011}. When adherence is maintained, this risk reduction has been estimated to be between 44\% and 100\% \autocite{riddellHIVPreexposureProphylaxis2018}. As of 2022, Truvada is the only authorized drug for PrEP in Europe \autocite{emaTruvada2018}, although Descovy and Apretude are also authorized for PrEP in the USA \autocite{PrEPPrEPHIV2022}.

All of these drugs are widely used and are by now very well studied, therefore detailed guidelines on all the aspects of ART; when to start, which drugs to use, when to change drugs; are issued and updated regularly by practitioners \autocite{zolopaEvolutionHIVTreatment2010} and global organisms \autocite{worldhealthorganizationConsolidatedGuidelinesHIV2021} alike.

\hypertarget{drug-mechanisms}{%
\subsection{Main mechanisms of viral proteins, antiretroviral drugs and associated resistance.}\label{drug-mechanisms}}

Each ART drug targets a specific protein, most of them target one of the three \emph{pol} proteins: RT, PR and IN. The structure of these proteins is inherently linked to their function, and as such is essential to take into account when developing ART. Similarly the structure of these proteins is very important when studying the resistance mechanisms developed by the virus \autocite{ammaranondMechanismHIVAntiretroviral2012,clavelHIVDrugResistance2004}. In this section we will go over; for RT, IN and PR; the main structural elements and how they relate to treatment and resistance.

\hypertarget{reverse-transcriptase}{%
\subsubsection{Reverse Transcriptase}\label{reverse-transcriptase}}

The reverse transcriptase protein is the most targeted protein in number of ART drugs \emph{(c.f. Figure} \ref{fig:timeline} \emph{and Table} \ref{tab:tableDrugs}\emph{)}. The mature protein is formed of two subunits: p51 and p66. These two subunits are translated from the same section of the \emph{pol} gene, and such have the same amino acid sequence, but p51 is cleaved and as such is shorter than p66. The p66 subunit contains the active sites of RT whereas p51 plays a mainly structural role.

The p66 sububit can be separated into 5 domains \autocite{sarafianosStructureFunctionHIV12009}. The ``fingers'', ``palm'', and ``thumb'' domains are linked together and folded to form a canal through which the RNA template and newly synthesized viral DNA can pass through. The polymerase active site, responsible for incorporating nucleotides to the viral DNA molecule, is situated in the ``palm'' domain at the bottom of the canal. The ``RNase'' domain of RT contains a secondary active site responsible for cleaving the viral RNA template from the viral DNA so that the RT can fill out the complementary strand of viral DNA before integration into the host genome. The final ``connection'' domain is simply a link between the ``RNase'' and the ``thumb'' domains. A three dimensional view of RT with these domains highlighted can be seen in Figure \ref{fig:rtStruct}.

\begin{figure} 
  \centering 
  \includegraphics[width=0.8\textwidth]{./figures/HIV-Intro/rt.png}      \extcaption{3D structure of HIV-1 Reverse-transcriptase.}{The different domains of the p66 subunit are labeled and shown in different shades of blue and green. The structural p51 subunit is shown in orange. The RNA template is shown in dark gray and the newly synthesized DNA stran in light gray. The polymerase active site is shown in red although mostly hidden by the RNA template. The 3D visualization was produced with Illustrate \autocite{goodsellIllustrateSoftwareBiomolecular2019} using the \href{https://www.rcsb.org/structure/2HMI}{2hmi} PDB structure.}       
  \label{fig:rtStruct} 
\end{figure}

Reverse Transcriptase inhibitors can be separated into two classes: Nucleoside RTIs (NRTIs) and Non-Nucleoside RTIs (NNRTIs), inhibiting the action of RT in two disctinct manners:

\begin{itemize}
\item
  NRTIs are analogues of free nucleotides in the host cell. They competitively inhibit RT and can be used to elongate the viral DNA chain. Once an NRTI is incorporated, further elongation of the DNA molecule is impossible and the viral DNA cannot be synthesized anymore. This is similar to the chain terminating nucleotides introduced in Section \ref{obtaining-sequence-data}.
\item
  NNRTIs bind to a specific region of the p51 subunit: the Non Nucleoside Inhibitor Binding Pocket (NNIBP) \emph{(A view of RT with the NNIBP visible is shown in Figure} \ref{fig:figStructure}\emph{)}. This pocket, although it is on the p51 subunit is spatially situated very close to the polymerase active site. NNRTIs bind to the NNIBP to change the conformation of the active site, lowering its flexibility \autocite{esnoufUniqueFeaturesStructure1997}, and thusly non-competitively inhibiting the action of RT.
\end{itemize}

Research has been conducted into inhibition of the RNase active site of RT \autocite{hangActivityIsolatedHIV2004,klumppRecentProgressDesign2006} which could also inhibit the action of RT. There is however, to this day, no approved treatment that inhibits the RNase action of RT.

Drug resistance mutations (DRMs) that arise in HIV from the selective pressures resulting from RTI exposure can similarly be grouped into two categories: NRTI and NNRTI resistance mutations.

NRTI resistance mutations can further be subcategorized into two groups \autocite{menendez-ariasMechanismsResistanceNucleoside2008,sluis-cremerMolecularMechanismsHIV12000}. The first type of NRTI resistance mutations are mutations that prevent the incorporation of NRTIs into the viral DNA molecule. M184V and M184I, indicating the replacement, at site number 184, of a Methionine by a Valine or an Isoleucine respectively, are very common NRTI resistance mutations. These V and I amino acids have a different structure than the original M, interfering with the incorporation of lamiduvine (3TC) but not dNTP \autocite{sarafianosLamivudine3TCResistance1999}. The second type of mutation, allows RT to remove an incorporated NRTI from the viral DNA to resume synthesis. Thymidine Analog Mutations (TAMs), M41L, D67N, K70R, L210W, T215Y/F and K219Q/E confer resistance to azidothymidine (AZT) through this mechanism \autocite{meyerMechanismAZTResistance1999,boyerSelectiveExcisionAZTMP2001}.

Similarly, NNRTI resistance mutations work via several different mechanisms \autocite{deeksNonnucleosideReverseTranscriptase2001,renStructuralBasisDrug2008}. Some NNRTI resistance mutations, like Y181C, lower the affinity of the NNIBP to NNRTIs preventing binding of drugs to RT. Others, like K103N change the conformation of the p51 subunit, making the NNIBP disappear. NNRTI resistance mutations are particularly dangerous because they often confer cross-resistance to multiple NNRTIs without affecting the polymerase action very much \autocite{ammaranondMechanismHIVAntiretroviral2012}, giving rise to viruses that are both fit and highly resistant. This is contrast to NRTI resistance mutations that generally incur a fitness cost for the virus, lowering its efficacy \autocite{lloydHighCostFidelity2014}.

\hypertarget{protease}{%
\subsubsection{Protease}\label{protease}}

The Protease protein, also a major drug target for ART, cleaves the \emph{gag} and \emph{pol} polyproteins in order to produce functional viral proteins, essential to replication. It has a symmetric dimeric structure, that is two say that it is composed of two identical chains of amino acids \autocite{pearlStructuralModelRetroviral1987,gulnikHIVProteaseEnzyme2000}. A structural view of PR is shown in Figure \ref{fig:prStruct}.

These two chains are folded in order to create a ``tunnel'' through which the polyproteins enter. In the middle of this ``tunnel'', at the bottom, is the active site. The active site is composed of two Aspartate residues, one on each chain, with water they can participate in a chemical reaction that cleaves the polyprotein at a specific position \autocite{silvaInhibitionCatalyticMechanism1996}.

The roof of the ``tunnel'' is formed by the flaps, a flexible region from each of the two chains that can open or close the ``tunnel'' \autocite{hornakHIV1ProteaseFlaps2006}. These flaps most likely control the access of polyproteins to the active site \autocite{freedbergRapidStructuralFluctuations2002,yuStructuralInsightsHIV12017}.

\begin{figure} 
  \centering 
  \includegraphics[width=0.7\textwidth]{./figures/HIV-Intro/pr.png}      \extcaption{3D structure of HIV-1 Protease.}{The two identical chains are colored in orange and blue shades respectively. The flexible, flaps form the the "roof" of a tunnel, at the bottom of which is the active site: 2 Asp residues, one on each chain. The 3D visualization was produced with Illustrate \autocite{goodsellIllustrateSoftwareBiomolecular2019} using the \href{https://www.rcsb.org/structure/2P3B}{2p3b} PDB structure.}       
  \label{fig:prStruct} 
\end{figure}

All the approve Protease Inhibitors (PIs) share a similar mode of action, each PI binds to the active site of the PR, denying access to the ``tunnel'' for polyproteins and stopping the catalytic action of PR \autocite{robertsRationalDesignPeptideBased1990,lvHIVProteaseInhibitors2015}. Tipranavir, one of the more recent PIs, also binds with the flaps \autocite{lvHIVProteaseInhibitors2015}.

According to Prabu-Jeyabalan and colleagues, the Protease structure does not recognize the specific sequence of the polyprotein cleavage site but rather its shape \autocite{prabu-jeyabalanSubstrateShapeDetermines2002}. They proposed an inhibitor based on this shape for all polyproteins combined, which establishes more bonds with PR, making it suposedly more efficient \autocite{prabu-jeyabalanSubstrateEnvelopeDrug2006} than current approved PIs.

As is the case with RTIs, when under selective pressure due to PIs, the virus tends to develop PI associated DRMs. Most PI resistance mutations result in an enlarged ``tunnel'', this tends to lower the affinity of the PIs to the active site, but also the affinity of polyproteins, lowering the fitness of the virus significantly \autocite{wensingFifteenYearsHIV2010}. In addition, some mutations on the \emph{gag} polyprotein seem to lower the efficacy of PIs, although the underlying mechanism is not well known \autocite{wensingFifteenYearsHIV2010}.

Some mutations in the flaps of PR have also been shown to confer PI resistance. It seems likely that these mutations change conformation of the flaps, opening them leading to the release of inhibitors from the active site \autocite{kurtyilmazImprovingViralProtease2016}.

\hypertarget{integrase}{%
\subsubsection{Integrase}\label{integrase}}

The integrase protein is the third major anti-retroviral drug target, it is responsible for integrating the viral DNA into the host genome. It is a tetramer composed of four identical amino acid chains. \autocite{chiuStructureFunctionHIV12004,espositoHIVIntegraseStructure1999}. Each of these chains contain three domains linked together by flexible linker sequences: the N-terminal domain, the catalytic core and the C-terminal domain. In each tetramer, two chains provide the active site for the integration reaction while the other two have a mostly structural role. It is probable that the N-terminal domain which is very conserved is necessary for stable tetramerization of IN monomers \autocite{delelisIntegraseIntegrationBiochemical2008}, this tetrameric structure is shown in Figure \ref{fig:inStruct}.

\begin{figure} 
  \centering 
  \includegraphics[width=0.7\textwidth]{./figures/HIV-Intro/in.png}      \extcaption{3D structure of an Integrase.}{This Integrase tetramer is binded with viral (red) and host (orange) DNA, linked to the two light blue functional subunits via the C-terminal domain. The active site formed by the the catalytic cores of the two functional subunits \textit{(not visible in this representation)}, is where the strand transfer reaction will take place. The two dark blue IN subunits have a structural role.
  This figure was adapted from the PDB 101 molecule of the month Integrase entry by David S. Goodsell and the RCSB PDB (\url{https://pdb101.rcsb.org/motm/135}) with a CC By 4.0 license.}       
  \label{fig:inStruct} 
\end{figure}

Several steps are needed in order to integrate the viral DNA with the host genome \autocite{maertensStructureFunctionRetroviral2022}. First, IN binds to the both ends of the viral DNA, using the C-terminal domains, forming a closed loop. Secondly, both ends of the viral DNA molecule are then prepared for integration by the catalytic core. Third, the host DNA is captured with C-terminal domains. Then, the strand-transfer is done within the catalytic core: the host DNA is cut in two places and a single strand from each end of the viral DNA are attached to these two breakpoints. Finally, the IN tetramer detaches from the linked molecules and the final steps necessary to create a single hybrid DNA molecule are done by the host cellular machinery. A graphical representation of this process can be found in Figure 1 of the 2022 article by Maertens et al. \autocite{maertensStructureFunctionRetroviral2022}.

Integrase Strand Transfer Inhibitors (INSTIs), as their name indicates, block the strand transfer reaction. They achieve this by strongly binding to the active site of the IN tetramer after it has formed a complex with the viral DNA \autocite{pommierIntegraseInhibitorsTreat2005,maertensStructureFunctionRetroviral2022}. In doing so, INSTIs prevent the IN / viral DNA complex from binding to the host DNA, effectively preventing strand transfer.

In the presence of INSTIs during therapy, once more the HIV virus develops resistance mutations over time. These mutations all lower affinity of IN to INSTIs, preventing bonding \autocite{blancoHIV1IntegraseInhibitor2011,maertensStructureFunctionRetroviral2022}. Since most INSTIs behave similarly, this means that cross-resistance to INSTIs is quite common for INSTI DRMs \autocite{blancoHIV1IntegraseInhibitor2011,gerettiEmergingPatternsImplications2012}. Once more, these mutations tend to lower the overall viral fitness necessitating secondary compensatory mutations to restore fitness \autocite{gerettiEmergingPatternsImplications2012,blancoHIV1IntegraseInhibitor2011}.

\hypertarget{other-drug-targets}{%
\subsubsection{Other drug targets}\label{other-drug-targets}}

For now, resistance has not been observed for novel drugs like entry inhibitors. This might be because the genetic barrier to resistance is higher and because not enough time has passed since their introduction for resistance to emerge.

For all the other drug targets however, as stated earlier in this section, resistance is documented and problematic. Resistance has even been detected for PrEP which is prophylactic \autocite{knoxMultidrugResistantHIV1Infection2017,hurtPreexposureProphylaxisAntiretroviral2011}. This seems to be rare however and mostly due to an unknown pre-treatment HIV infection \autocite{gibasDrugResistanceHIV2019}.

\hypertarget{consequences-of-resistance-on-global-health}{%
\subsection{Consequences of resistance on global health}\label{consequences-of-resistance-on-global-health}}

HIV resistance to ART drugs is problematic from a global health perspective. Indeed circulation of resistant strains of HIV within populations can lead to treatment-naive individuals that will not respond well to treatment.

More concerning is the fact that transmission of resistant strains of HIV between treatment-naive individuals is the main mode of resistance transmission in the UK \autocite{mouradPhylotypebasedAnalysisHighlights2015,hueDemonstrationSustainedDrugResistant2009} and Switzerland \autocite{drescherTreatmentNaiveIndividualsAre2014}. This treatment-naive to treatment-naive transmission is particularly insidious since it can go undetected and creates long lasting drug resistant strain reservoirs in the treatment-naive population. This of course is dangerous since some infected individuals might experience poor therapeutic outcomes and even failure when administered first line regimens \autocite{boermaHighLevelsPretreatment2016}. To avoid this, genotypic resistance testing has become standard practice when choosing the therapeutic strategy in high-income countries, but more effort must be done to make resistance testing less expensive and more cost-efficient in lower and middle income countries \autocite{clutterHIV1DrugResistance2016}.

Although the transmitted drug resistance described above is problematic, a large portion of DRMs incur a fitness cost for the resistant strain \autocite{kuhnertQuantifyingFitnessCost2018,mespledeViralFitnessCost2013}. This means that although they are selected when exposed to the evolutionary pressure of ART, when the treatment is interrupted there is another pressure leading these costly mutations to disappear. This reversion is commonly observed after interruption of treatment, however the median reversion times vary widely from 1 to 13 years \autocite{castroPersistenceHIV1Transmitted2013} depending on the severity of the fitness loss and type of mutation. This means that although reversion can possibly lead to loss of resistance this can potentially take a long time and possibly longer than the treatment interruption.

In practice it is therefore very important to keep an eye on all drug resistance mutations, their population dynamics and spread as well as their presence or absence in a particular strain before starting treatment.

\hypertarget{finding-drms}{%
\subsection[Finding DRMS ]{\texorpdfstring{Finding DRMS \footnote{This sections build upon a review I participated in during my PhD \autocite{blasselDrugResistanceMutations2021}}}{Finding DRMS }}\label{finding-drms}}

Finding and categorizing mutations as DRMs is an important task in light of the public health implications mentioned in Section \ref{consequences-of-resistance-on-global-health}, as such this is an active part of the HIV research field.

The most important thing needed in order to study DRMs is of course viral sequences. To facilitate the search for DRMs several sequence databases exist. Sequences are often linked to metadata related to the treatment status of the patient from which the sequence was obtained. This metadata can be quite variable: from a coarse level binary indicator of treatment to a finely detailed list of all treatments recieved and associated phenotypic measurements like viral load.

Databases like the UK-CHIC \autocite{ukchicsteeringcommitteeCreationLargeUKbased2004}, uk HIV drug resistance database (\url{https://www.hivrdb.org.uk/}) and Swiss cohort study (\url{https://www.shcs.ch/}) host sequences on a national level, although access can be granted to international researchers. Other databases like the PANGEA database \autocite{abeler-dornerPANGEAHIVPhylogeneticsNetworks2019} host sequences from multiple countries in sub-Saharan Africa. The Stanford HIV drug resistance database (\url{https://hivdb.stanford.edu/}) hosts HIV sequences with some phenotypic data \autocite{rheeHumanImmunodeficiencyVirus2003,shaferRationaleUsesPublic2006}. Finally some database only host sequences, such as the Los Alamos HIV sequence database (\url{http://www.hiv.lanl.gov/}), however without any specific treatment or resistance related metadata \autocite{kuikenHIVSequenceDatabases2003} these have less direct applicability to the DRM search task.

Some databases, like the Standford HIV resistance database, also store specific knowledge about known resistance mutations, keeping and regularly updating lists of clinically important DRMs as well as their impact on ART \autocite{wensing2019UpdateDrug2019,clarkMutationsRetroviralGenes2007}. Additionaly, Stanford also offers tools for clinicians to do genotypic resistance testing with interpretable results \autocite{liuWebResourcesHIV2006d}.

The first step of mutations discovery is usually some kind of statistical association analysis\autocite{johnsonUpdateDrugResistance2016,wensing2019UpdateDrug2019} where the association between treatment status (coarse of fine grained) and specific mutations is statistically tested. This is usually done with Fisher association tests \autocite{villabona-arenasIndepthAnalysisHIV12016,shulmanGeneticCorrelatesEfavirenz2004} or correlation testing with the Spearman correlation for example \autocite{millerGenotypicPhenotypicPredictors2004}. This results in a list of mutations that are significantly associated with a given treatment and corresponding p-values.

Since, on a given sequence dataset, several mutations are usually tested at once this can lead to inflated false positive \autocite{brownMethodsCorrectingMultiple1997} and spurious \autocite{austinTestingMultipleStatistical2006} associations. Fortunately this is a well studied problem and many methods exist to control this effect by controlling the Familywise Error Rate (FWER) \emph{e.g.} with the Bonferroni procedure,\autocite{hochbergMultipleComparisonProcedures1987} or the False Discovery Rate (FDR) \emph{e.g.} with the Benjamini-Hochberg procedure \autocite{benjaminiControllingFalseDiscovery1995}. These methods are often applied when testing for resistance association \autocite{villabona-arenasIndepthAnalysisHIV12016,gonzalesExtendedSpectrumHIV12003,seoigheModelDirectionalSelection2007}. However these correction methods are a double-edged sword, some of them can be very conservative and lead to falsely rejecting true associations \autocite{shamStatisticalPowerSignificance2014}. In some studies on resistance phylogenetic correlation between the sequences is also accounted for in statistical analyses \autocite{alizonPhylogeneticApproachReveals2010,flynnDeepSequencingProtease2015}.

Statistical testing on treatment status, while informative, can only associate a mutation with a treatment. In order to actually validate whether a mutation causes resistance or not biological analyses are needed \autocite{johnsonUpdateDrugResistance2016,wensing2019UpdateDrug2019}. The easiest of these are \emph{in vitro} analyses where live viruses are subjected to a phenotypical assay. These assays measure the susceptibility of HIV viruses to a wide array of drugs, which can then be statistically associated with genetic traits like specific mutations. These assays like phenosense \autocite{petropoulosNovelPhenotypicDrug2000} or antivirogram \autocite{hertogsRapidMethodSimultaneous1998} are widely used \autocite{heilek-snyderRoleHIVPhenotypic2002,moyleEpidemiologyPredictiveFactors2005,gartlandSusceptibilityGlobalHIV12021}. The viruses can be obtained from clinical isolates \autocite{masquelierGenotypicPhenotypicResistance2001} or, viruses with specific mutations can be manufactured with site directed mutagenesis \autocite{larderMultipleMutationsHIV11989,devreeseResistanceHumanImmunodeficiency1992}. \emph{In vivo} can be conducted by sequencing viruses from patients failing ART, following over time and studying the association between their treatment response and HIV genetics \autocite{tambuyzerEffectMutationsPosition2011,katzensteinPhenotypicSusceptibilityVirological2003}.

More recently, as sequence database grow bigger and bigger \emph{(The UK-CHIC database contains more than 80,000 HIV sequences with treatment status)}, methods based on statistical and machine learning are being used to study resistance. Most approaches rely on training models to predict some type of resistance: either classifying sequences as resistant or not \autocite{blasselUsingMachineLearning2021,steinerDrugResistancePrediction2020a} of predicting a phenotypic response like fold resistance compared to wild type \autocite{sheikamamuddyImprovingFoldResistance2017b}. Initial approaches were mainly designed for clinical testing, rather than new DRM search, and distributed via web services \autocite{beerenwinkelGeno2phenoInterpretingGenotypic2001,riemenschneiderSHIVAWebApplication2016b}.

Initially these approaches were based on models like decision trees \autocite{beerenwinkelDiversityComplexityHIV12002c}, SVMs \autocite{beerenwinkelGeno2phenoInterpretingGenotypic2001} or logistic regression \autocite{heiderMultilabelClassificationExploiting2013b}. Over time the use of more complex models such as neural networks has increased, with increased prediction accuracy \autocite{sheikamamuddyImprovingFoldResistance2017b}.

By analyzing the important features used by trained models to predict resistance, it is possible to find features, corresponding to mutations, that are useful for predicting, and therefore likely associated with, drug resistance (see Chapter \ref{HIV-paper}). With the improvement in methods to interpret and extract features from complex models such as deep neural networks, this approach has been used with deep learning models \autocite{steinerDrugResistancePrediction2020a}. This novel way of finding resistance associated mutations has the potential to uncover complex mutational effects that simple association testing cannot.

\hypertarget{conclusion-1}{%
\section{Conclusion}\label{conclusion-1}}

Viruses are surprisingly complex in light of their apparent simplicity. They are ubiquitous and present an extreme diversity. Whether they are pathogenic or not, the role of viruses in a myriad of processes and niches make them interesting and important to study. The sequences of these viruses, although small can be very useful for evolutionary as well as clinical analyses.

Although the study of viruses as a whole can be very useful, HIV is particularly important to study. The impact of the HIV pandemic on global health has been severe, both in Lower and Higher income countries. It is therefore paramount to fully understand the underlying mechanisms and evolutionary adaptations of this virus. It's high mutation rate allow it to quickly explore evolutionary alternatives when exposed to drugs, making anti HIV therapy a complex endeavor.

Fortunately, with large scale sequencing efforts it is possible to study and track these evolutionary adaptations to treatments. This allows us to adapt therapeutic strategies as well as developing new compounds and approaches. In this context, studying and finding the virus' mutational processes is paramount. This is especially important when studying resistance to RTIs as they form the backbone of first line regimen combination therapies, and are the most common type of anti-HIV drug. This process is made easier by the large scale sequence repositories now available, and the usage of machine and statistical learning to leverage that data.

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

\hypertarget{HIV-paper}{%
\chapter{Contribution 2: Inferring mutation roles from sequence alignments using machine learning}\label{HIV-paper}}

Recall that:

\begin{itemize}
\item
  HIV pandemic is very serious (cf.~Sections \ref{quick-presentation-of-hiv} and \ref{consequences-of-resistance-on-global-health} )
\item
  Finding drug resistance mutations is useful to manage this pandemic
\item
  These mutations can be found with machine learning and/or statistical testing (c.f. Section \ref{finding-drms})
\item
  Alignments are used as inputs for this testing procedure (c.f. Chapter 4)
\end{itemize}

This chapter was written as an article titled:\\
\textbf{``Using Machine Learning and Big Data to Explore the Drug Resistance Landscape in HIV''}\\
it was originally published in \emph{August 2021}, in \emph{PLoS Computational Biology}
(\href{https://doi.org/10.1371/journal.pcbi.1008873}{\emph{doi:10.1371/journal.pcbi.1008873}}).\\
The author list, complete with affiliations is given below:\\
\strut \\

Luc Blassel\textsuperscript{\textbf{1},\textbf{2}*}, Anna Tostevin\textsuperscript{\textbf{3}}, Christian Julian
Villabona-Arenas\textsuperscript{\textbf{4},\textbf{5}}, Martine Peeters\textsuperscript{\textbf{6}}, Stéphane Hué\textsuperscript{\textbf{4},\textbf{5}}, Olivier
Gascuel\textsuperscript{\textbf{1},\textbf{7}\#} On behalf of the UK HIV Drug Resistance
Database\textsuperscript{\(\wedge\)}\\

\textbf{1} Unité de Bioinformatique Évolutive, Institut Pasteur, Paris,
France\\
\textbf{2} Sorbonne Université, Collège doctoral, Paris, France\\
\textbf{3} Institute for Global Health, UCL, London, UK\\
\textbf{4} Department of Infectious Disease Epidemiology, London School of
Hygiene and Tropical Medicine, London, UK\\
\textbf{5} Centre for Mathematical Modelling of Infectious Diseases, London
School of Hygiene and Tropical Medicine, London, UK\\
\textbf{6} TransVIHMI (Recherches Translationnelles sur VIH et Maladies
Infectieuses), Université de Montpellier, Institut de Recherche pour le
Développement, INSERM, Montpellier, France\\
\textbf{7} Institut de Systématique, Evolution, Biodiversité (ISYEB), UMR
7205 - Muséum National d'Histoire Naturelle, CNRS, SU, EPHE and UA,
Paris, France

\# Current address: Institut de Systématique, Evolution, Biodiversité
(ISYEB), UMR 7205 - Muséum National d'Histoire Naturelle, CNRS, SU, EPHE
and UA, Paris, France\\
* luc.blassel@pasteur.fr (LB)\\
* olivier.gascuel@mnhn.fr (OG)

\textsuperscript{\(\wedge\)} Membership list can be found in the acknowledgments section

\hypertarget{abstract-paper}{%
\section*{Abstract}\label{abstract-paper}}
\addcontentsline{toc}{section}{Abstract}

Drug resistance mutations (DRMs) appear in HIV under treatment pressure.
DRMs are commonly transmitted to naive patients. The standard approach
to reveal new DRMs is to test for significant frequency differences of
mutations between treated and naive patients. However, we then consider
each mutation individually and cannot hope to study interactions between
several mutations. Here, we aim to leverage the ever-growing quantity of
high-quality sequence data and machine learning methods to study such
interactions (i.e.~epistasis), as well as try to find new DRMs.\\
We trained classifiers to discriminate between Reverse Transcriptase
Inhibitor (RTI)-experienced and RTI-naive samples on a large HIV-1
reverse transcriptase (RT) sequence dataset from the UK
(\(n\approx 55,000\)), using all observed mutations as binary
representation features. To assess the robustness of our findings, our
classifiers were evaluated on independent data sets, both from the UK
and Africa. Important representation features for each classifier were
then extracted as potential DRMs. To find novel DRMs, we repeated this
process by removing either features or samples associated to known
DRMs.\\
When keeping all known resistance signal, we detected sufficiently
prevalent known DRMs, thus validating the approach. When removing
features corresponding to known DRMs, our classifiers retained some
prediction accuracy, and six new mutations significantly associated with
resistance were identified. These six mutations have a low genetic
barrier, are correlated to known DRMs, and are spatially close to either
the RT active site or the regulatory binding pocket. When removing both
known DRM features and sequences containing at least one known DRM, our
classifiers lose all prediction accuracy. These results likely indicate
that all mutations directly conferring resistance have been found, and
that our newly discovered DRMs are accessory or compensatory mutations.
Moreover, apart from the accessory nature of the relationships we found,
we did not find any significant signal of further, more subtle epistasis
combining several mutations which individually do not seem to confer any
resistance.

\hypertarget{author-summary}{%
\section*{Author summary}\label{author-summary}}
\addcontentsline{toc}{section}{Author summary}

Almost all drugs to treat HIV target the Reverse Transcriptase (RT) and
Drug resistance mutations (DRMs) appear in HIV under treatment pressure.
Resistant strains can be transmitted and limit treatment options at the
population level. Classically, multiple statistical testing is used to
find DRMs, by comparing virus sequences of treated and naive
populations. However, with this method, each mutation is considered
individually and we cannot hope to reveal any interaction (epistasis)
between them. Here, we used machine learning to discover new DRMs and
study potential epistasis effects. We applied this approach to a very
large UK dataset comprising \(\approx 55,000\) RT sequences. Results
robustness was checked on different UK and African datasets.\\
Six new mutations associated to resistance were found. All six have a
low genetic barrier and show high correlations with known DRMs.
Moreover, all these mutations are close to either the active site or the
regulatory binding pocket of RT. Thus, they are good candidates for
further wet experiments to establish their role in drug resistance.
Importantly, our results indicate that epistasis seems to be limited to
the classical scheme where primary DRMs confer resistance and associated
mutations modulate the strength of the resistance and/or compensate for
the fitness cost induced by DRMs.

\hypertarget{hiv-introduction}{%
\section{Introduction}\label{hiv-introduction}}

Drug resistance mutations (DRMs) arise in Human Immunodeficiency Virus-1
(HIV-1) due to antiretroviral treatment pressure, leading to viral
rebound and treatment failure
\autocite{lepriResistanceProfilesPatients2000,verhofstedeDetectionDrugResistance2007}.
Furthermore, drug-resistant HIV strains can be transmitted to
treatment-naive individuals and further spread throughout the population
over time
\autocite{hueDemonstrationSustainedDrugResistant2009,mouradPhylotypebasedAnalysisHighlights2015,zhukovaRolePhylogeneticsTool2017}.
These transmitted resistant variants limit baseline treatment options
and have clinical and public health implications worldwide. Almost all
drugs to treat HIV target the reverse transcriptase (RT), encoded by the
\emph{pol} gene. Lists of DRMs are regularly compiled and updated by experts
in the field, based on genotype analyses and phenotypic resistance tests
or clinical outcome in patients on ART
\autocite{bennettDrugResistanceMutations2009,hammondMutationsRetroviralGenes1998,wensing2017UpdateDrug2016}.
However, with the developement of new antiretroviral drugs that target
RT but also other regions of the \emph{pol} gene like protease or integrase,
and the use of anti-retrovirals in high risk populations by pre-exposure
prophylaxis (PREP), it is important to further our understanding of HIV
polymorphisms and notably the interactions between mutations and
epistatic effects.\\
Among known DRMs, some mutations, such as M184V, directly confer
resistance to antiretrovirals, more precisely the commonly used NRTI,
3TC (lamivudine) and FTC (emtricitabine), and are called primary or
major drug resistance mutations, while some mutations like E40F have an
accessory role and increases drug resistance when appearing alongside
primary DRMs. Moreover, some mutations like S68G seem to have a
compensatory role, but are not known to confer any resistance nor
modulate resistance induced by primary DRMs. All of these mutations
might have different functions in the virus, but they are all known to
be associated with drug resistance phenomena. Therefore, during the rest
of this article we will refer to all of these known mutations as
resistance associated mutations (RAMs), rather than DRMs which is too
specific, and our goal will be to search for new RAMs and study the
interactions between known RAMs and the new ones.\\
Classically, new RAMs have been found using statistical testing and
large multiple sequence alignments (MSA) of the studied protein
\autocite{dudoitMultipleTestingProcedures2007,villabona-arenasIndepthAnalysisHIV12016}.
Tests are performed for mutations of interest on a given MSA to check if
they are associated with the treatment status and outcome of the
individual the viral sequences were sampled from. The test significance
is corrected for multiple testing as all mutations associated to every
MSA position is virtually a resistance mutation and tested. After this
preliminary statistical search, the selected mutations are scrutinized
to remove the effects of phylogenetic correlation (i.e.~typically
counting two sequences which are identical or closely related due to
transmission rather than independent acquisition twice
\autocite{maddisonUnsolvedChallengePhylogenetic2015}) and check that the same
mutation occurred several times in different subtypes and populations
being treated with the same drug. Then, these mutations can be further
experimentally tested in vitro or in vivo to validate phenotypic
resistance. This method has worked well, but by design it is not ideal
for studying the effect of several mutations at once, since if we have
to test all couples or triplets of mutations, we quickly lose
statistical power when correcting for multiple testing
\autocite{shamStatisticalPowerSignificance2014}, due to the large number of
tests to perform. Moreover, phylogenetic correlation is again a critical
issue with such an approach.\\
Machine learning has been extensively used to predict resistance to
antiretrovirals from sequence data. There are two main approaches to
predicting resistance from sequence data. Regression, where machine
learning models are trained to predict the value of a drug resistance
indicator, typically \(IC_{50}\) fold change in response to a given drug
\autocite{lengauerBioinformaticsassistedAntiHIVTherapy2006} or other indicators
from phenotypic resistance assays such as PhenoSense
\autocite{zhangComparisonPrecisionSensitivity2005a}. Many methods have been used
to predict a resistance level: Support Vector Machines (SVMs)
\autocite{beerenwinkelGeno2phenoEstimatingPhenotypic2003b}, k-Nearest Neighbors
(KNN) and Random Forests (RFs) \autocite{shenAutomatedPredictionHIV2016a}, and
more recently Artificial Neural Networks (ANNs)
\autocite{yuPredictionHIVDrug2014a,sheikamamuddyImprovingFoldResistance2017b}.
Alternatively, this task has also been approached as a classification
problem. Given a certain threshold on a phenotypic resistance measure,
sequences are given a label of "resistant" or "susceptible" to a
certain drug. Machine learning classifiers are then trained to predict
that label. For this task, SVMs and decision trees have been used
\autocite{beerenwinkelGeno2phenoInterpretingGenotypic2001,arayaSupportVectorMachine2009},
ensemble classifier chains
\autocite{riemenschneiderExploitingHIV1Protease2016a,heiderMultilabelClassificationExploiting2013b}
and also ANNs \autocite{draghiciPredictingHIVDrug2003}. Most recently Steiner
\emph{et al.} \autocite{steinerDrugResistancePrediction2020a} have used Deep Learning
Architectures to predict resistance status (i.e.~classification) from
sequence data. Since phenotypic assays are more complicated and costly
to perform than simple genotyping, there is a limited number of
sequences paired with a resistance level. This is the main limitation of
these studies since machine learning methods typically benefit from a
large amount of training data. This is especially true for deep neural
networks which can need hundreds of thousands of training samples for
certain tasks and architectures. However, despite this limitation,
approaches proposed in these studies seem to have fairly good predictive
accuracy. It is important to note that all of these studies aim to
predict if a given sequence is resistant or not to a given drug, they do
not aim to find new potential RAMs. Although Steiner \emph{et al.}
\autocite{steinerDrugResistancePrediction2020a} have checked that known DRM
positions are captured by their models and found several positions
potentially associated to resistance, it is not the main goal of their
method.\\
It is accepted in machine learning that there is a trade-off between
model accuracy and model interpretability. In these previous studies the
goal was to make the most accurate predictions possible, using complex
models such as SVMs and ANNs, therefore sacrificing interpretability.
Here, we have a different approach, using simpler models that might be
less accurate but whose predictions we can understand and interpret. We
train these models to discriminate RTI-naive from RTI-experienced
sequences. Without the need for phenotypic data, we are able to use much
larger HIV-1 RT sequence datasets from the UK (\(n\approx55,000\))
(\url{http://www.hivrdb.org.uk/}) and Africa (\(n\approx4,000\))
\autocite{villabona-arenasIndepthAnalysisHIV12016}. By using interpretable
models, we can extract mutations that are important for determining if a
sequence is treated or not and potentially find new mutations
potentially associated to resistance. Furthermore, we aim to detect
associations between mutations and their effect on antiretroviral
resistance in order to study potential underlying epistasis. The African
and UK datasets are very different both from genetic and treatment
history standpoints, therefore training classifiers on the UK dataset
and testing them on the African one, should guarantee the robustness of
our findings and greatly alleviate phylogenetic correlation effects. In
the following sections, we first describe the data then the methods
used. Our results include the assessment of the performance of our
classifiers even when trained on data devoid of any known
resistance-associated signal; as well as a description of the main
features (prevalence and correlation to known mutations, genetic barrier
and structural analysis) of six potentially resistance associated
mutations, newly discovered thanks to our approach. These results and
perspectives are discussed in the concluding section.

\hypertarget{materials-and-methods}{%
\section{Materials and methods}\label{materials-and-methods}}

\hypertarget{data}{%
\subsection{Data}\label{data}}

In this study, we used all the drug resistance mutations that appeared
in the Stanford HIV Drug resistance database, both for NRTI (Nucleoside
Reverse Transcriptase Inhibitors;
\url{https://hivdb.stanford.edu/dr-summary/comments/NRTI/}) and NNRTI (Non
Nucleoside RTI; \url{https://hivdb.stanford.edu/dr-summary/comments/NNRTI/})
as known RAMs. To discover new RAMs, assess their statistical
significance and study potential epistatic effects, we used two datasets
of HIV-1 RT sequences. A large one (\(n=55,539\)) from the UK HIV Drug
Resistance Database (\url{http://www.hivrdb.org.uk/}) and a smaller
(\(n=3,990\)) one from 10 different western, eastern and central African
countries \autocite{villabona-arenasIndepthAnalysisHIV12016}. In the UK dataset,
sequences from RTI-naive individuals formed the majority class with
41,921 sequences (75\%). In the African dataset, both classes were more
balanced with 2,316 RTI-naive sequences (58\%). In the UK dataset,
RTI-naive sequences had at least one known RAM in 25\% of cases, most
likely due to transmissions to naive patients or undisclosed treatment
history, against 48\% in RTI-experienced sequences, thus making the
discrimination between the RTI-experienced and RTI-naive sequences
particularly difficult. In the African dataset this distribution was
more contrasted, with only 14\% of RTI-naive sequences having at least
one known RAM, versus 83\% of RTI-experienced sequences. The African
dataset was also much more genetically diverse with 24 different
subtypes and CRFs compared to the 2 subtypes (B and C) that we retained
for this study from the UK cohort. The majority of the sequences from
the African dataset were samples from Cameroon (27\%), Democratic
Republic of Congo (17\%), Burundi (15\%), Burkina Faso (13\%) and Togo
(11\%).\\
It is important to note that RTI-experienced sequences in both of these
datasets can be considered as resistant to treatment. Since the viral
load was sufficiently high to allow for sequencing of the virus, we can
consider that the ART has failed. However, in some cases this resistance
might be caused by non adherence to ART, rather than by the presence of
RAMs, therefore adding some noise to the relationship between treatment
status and resistance.\\
In addition to differences in size, balance between RTI-naive and
experienced classes, and the genetic difference between the UK and
African datasets, there are also significant differences resulting from
differing treatment strategies. In the UK and other higher income
countries, the treatment is often tailored to the individual with
genotype testing, which result in specific treatment as well as thorough
follow-ups and high treatment adherence. In the African countries of the
dataset that we used, the treatment is ZDV/ d4T (NRTI) + 3TC (NRTI) +
NVP/EFV (NNRTI) in most cases
\autocite{villabona-arenasIndepthAnalysisHIV12016}, and this treatment is
generalized to the affected population, with poorer follow-up and
adherence than in the UK. This discrepancy could lead to different
mutations arising in both datasets, however since the treatment strategy
is a combination of both NRTI and NNRTI drug classes, as in many
countries, similar RAMs arise
\autocite{villabona-arenasIndepthAnalysisHIV12016}. Furthermore, there is
potentially more uncertainty in the African dataset than in the UK. For
example some individuals may have unofficially taken antiretroviral
drugs, but still identify themselves as RTI-naive, or report having some
form of ART while not having been treated for HIV
\autocite{mooneySocialDesirabilityBias2018}. All of this explains the high
prevalence of multiple resistance in the African data set: the median
number of RAMs in sequences containing at least one RAM is 3 in the
African sequences, while it is 1 in UK sequences
(Table~\ref{tab:tableData}).
Thus, we can say that African sequences are highly resistant, with
possibly different mutations and epistatic effects, compared to their UK
counterparts.\\

\begin{table}
\centering
\begin{tabular}{@{}lll@{}ll@{}l@{}}
\hline
& & UK~~~~~ & & Africa & \\ \hline
\multicolumn{2}{@{}l}{size} & 55539 & & 3990 & \\
  & & & & & \\
\multirow[t]{2}{*}{RTI naive} & with known RAMs & 11429 & (21\%) & 318 & (8\%) \\
  & without known RAMs & 30492 & (55\%) & 1998 & (50\%) \\
\multirow[t]{2}{*}{RTI experienced} & with known RAMs & 6633 & (12\%)  & 1388 & (35\%) \\
  & without known RAMs & 6985 & (13\%)  & 286 & (7\%) \\
  & & & & & \\
\multicolumn{2}{@{}l}{sequences with $\geq$ 2 known RAMs} & 8034 & (14\%)  & 1308 & (33\%) \\
\multicolumn{2}{@{}l}{max known RAM number} & 13 & & 17 \\
\multicolumn{2}{@{}l}{Median known RAM number} & 1 & & 3 \\
\multicolumn{2}{@{}l}{number of subtypes / CRFs} & 2 & & 24 \\
  & & & & & \\
\multirow[t]{4}{*}{subtypes / CRFs} & A & 0 & (0\%) & 472 & (12\%)  \\
  & B & 37806 & (68\%) & 64 & (2\%)  \\
  & C & 17733 & (32\%) & 702 & (18\%)  \\
  & CRF02 AG & 0 & (0\%) & 1477 & (37\%) \\ \hline
\end{tabular}
\extcaption{Summary of the UK and African datasets.}{
Percentages are computed with regards to the size of the considered dataset (e.g. 21\% of the sequences of the UK dataset are RTI-naive and have at least one known RAM). The median number of RAMs was computed only on sequences that had at least one known RAM.
}
\label{tab:tableData}
\end{table}

All these differences between the two datasets helped us to assess the
generalizability of our method and the robustness of the results. That
is to say, if signal extracted from the UK dataset was still relevant on
such a different dataset as the African one, we could be fairly
reassured in regard to the biological and epidemiological relevance of
the observed signal.\\
Sequences in both African and UK datasets were already aligned. In order
to avoid overly gappy regions of our alignment we selected only
positions 41 to 235 of RT for our analysis. We used the Sierra web
service (\url{https://hivdb.stanford.edu/page/webservice/}) to get amino
acid positions relative to the reference HXB2 HIV genome. This allowed
us to determine all the amino acids present at each reference position
in both datasets, among which we distinguished the ``reference amino
acids'' for each position, corresponding to the B and C subtype reference
sequences obtained from the Los Alamos sequence database
(\url{http://www.hiv.lanl.gov/}). All the other, non-reference amino acids
are named ``mutations'' in the following, and the set of mutations was
explored to reveal new potential RAMs.\\
To train our supervised classification methods
\autocite{tibshiraniRegressionShrinkageSelection1996,brierVERIFICATIONFORECASTSEXPRESSED1950,gascuelTwelveNumericalSymbolic1998},
the sequence data needed to be encoded to numerical vectors. A common
and intuitive way to do so is to create a single feature in the dataset
for each position of the sequence to encode. Each amino acid is then
assigned an integer value, and an amino acid sequence is represented by
a succession of integers corresponding to each amino acid. There is,
however, one drawback with this method: by assigning an integer value to
amino acids, we transform a categorical variable into an ordinal
variable. Any ordering of amino acids is hard to justify and might
introduce bias. To avoid this, we represented each sequence by a binary
vector using one-hot encoding. For each position in the sequence to be
encoded, amino acids corresponding to mutations are mapped to a binary
vector denoting its presence or absence in the sequence. For example, at
site 184, amino acids M, G, I, L, T and V are present in the UK dataset.
After encoding we will have 5 binary features corresponding to the
M184G, M184I, M184L, M184T and M184V mutations. We did not encode the
reference amino acid M, but only the mutated amino acids. With this
method each mutation in the dataset (\(n=1,318\)) corresponds to a single
feature. Some of these features corresponded to known RAMs (e.g., M184I
and M184V) and are named (known) RAM features in the following
(\(n=121\)). This encoding allows the classifiers to consider specific
mutations and potentially link them to resistance.

\hypertarget{classifier-training}{%
\subsection{Classifier training}\label{classifier-training}}

In order to find new potential RAMs, we first followed the conventional
multiple testing approach \autocite{villabona-arenasIndepthAnalysisHIV12016}. We
first used Fisher exact tests to identify which of these mutations were
significantly associated with anti-retroviral treatment. All the
resulting p-values were then corrected for multiple testing using the
Bonferroni correction \autocite{goemanMultipleHypothesisTesting2014}. Those for
which the corrected p-value was \(≤ 0.05\) were then considered as
significantly associated with treatment and potentially implicated in
resistance.\\
This method was complemented by our parallel, machine learning based
approach. In order to extract potential RAMs, we trained several
classifiers to discriminate between RTI-experienced and RTI-naive
sequences represented by the binary vectors described above. This
classification task does not need any phenotypic resistance measure,
allowing us to use much larger and more readily available datasets than
other machine learning based approaches previously mentioned. Once the
classifiers were trained, we extracted the most important representation
features, which corresponded to potentially resistance-associated
mutations (PRAM in short). To this aim we chose three interpretable
supervised learning classification methods so as to be able to extract
those features:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Multinomial naive Bayes (NB), which estimates conditional
  probabilities of being in the RTI-experienced class given a set of
  representation features \autocite{rennie2003tackling}; the higher
  (\(\approx 1.0\)) and the lower (\(\approx0\)) conditional probabilities
  correspond to the most important features.
\item
  Logistic regression (LR) with L1 regularization (LASSO)
  \autocite{tibshiraniRegressionShrinkageSelection1996} which assigns weights
  to each of the features, whose sign denotes the importance to one of
  the 2 classes, and whose absolute value denotes the weight of this
  importance.
\item
  Random Forest (RF) , which has feature importance measures based on
  the Gini impurity in the decision trees \autocite{breimanRandomForests2001}.
\end{enumerate}

Interpretability was the main driver behind our classification method
choice, with the conditional probabilities of NB, the weight or LR and
the importance values of RF, we can easily extract which mutations are
driving the discrimination of RT sequences. This is why we did not
choose to use ANNs which could have led to an increase in accuracy at
the cost of interpretability
\autocite{alvarezmelisRobustInterpretabilitySelfExplaining2018,hastieElementsStatisticalLearning2009,zhangInterpretableConvolutionalNeural2018}.
Moreover, these three classification methods have the potential to
detect epistatic effects. With RF, the discrimination is based on the
combination of a few features (i.e.~mutations), while with LR the
features are weighted positively or negatively, thus making it possible
to detect cumulative effects resulting from a large number of mutations,
which individually have no discrimination power. Naive Bayes is a very
simple approach, generally fairly accurate, and in between the two
others in terms of explanatory power
\autocite{gascuelTwelveNumericalSymbolic1998}.\\
In order to be able to compare all these approaches in a common
framework, we devised a very simple classifier out of the results of the
Fisher exact tests. This "Fisher classifier" (FC) predicts a sequence
as RTI-experienced if it has at least one of the mutations significantly
associated to treatment. In this way, we were able to compute metrics
for all classification methods and compare their performance.\\
It is important to note that in all of these approaches we chose to
discriminate RTI-naive from RTI-experienced sequences, regardless of the
type of RTI received. One of the reasons is that we did not have
detailed enough treatment history for sequences in the UK and African
datasets. Moreover, even without segmenting by treatment type, the size
of the training set and the power of our classification methods were
both high enough to be able to detect all kinds of resistance associated
mutations. We shall see (Result section) that we were able to determine
the likely treatment involved by further examining the important
extracted features and comparing them to known RAMs. Furthermore, since
the treatment strategies are so different between the UK and African
sequences, training on sequences having received different treatments
should increase the robustness of our classifiers and the relevance of
the mutations selected as potentially associated to resistance.\\
To avoid phylogenetic confounding factors (e.g.~transmitted mutations
within a specific country or region), and avoid finding mutations
potentially specific to a given subtype, we split the training and
testing sets by HIV-1 M subtype. This resulted in training a set of
classifiers on all subtype B sequences of the UK dataset and testing
them on subtype C sequences from the UK dataset, training another set of
classifiers on the subtype C sequences of the UK dataset and testing on
the subtype B sequences from the UK dataset, as well as training a final
set of classifiers on the whole UK dataset, but testing it on the
smaller African dataset with a completely different phylogenetic makeup
and treatment context \autocite{villabona-arenasIndepthAnalysisHIV12016}.
Furthermore, in order to identify novel RAMs and study the behavior of
the classifiers, we repeated this training scheme on both datasets, each
time removing resistance-associated signal incrementally: first by
removing all representation features corresponding to known RAMs from
the dataset, and second by removing all sequences that had at least one
known RAM. This resulted in each type of classifier being trained and
tested 9 times, on radically different sets to ensure the
interpretability and robustness of the results (see
Table~\ref{tab:tableTrainTest}).

\begin{table}
% \begin{adjustwidth}{-2.25in}{0in}
\centering
\begin{tabular}{@{}p{.25\linewidth}llll@{}}
\hline
Signal removal level & Trained on & & Tested on & \\ \hline
\multirow[t]{3}{*}{None} & UK, subtype B & (37806) & UK, subtype C & (17733) \\
 & UK, subtype C & (17733) & UK, subtype B & (37806) \\
 & UK, subtypes B \& C & (55539) & Africa, all subtypes & (3990)  \\
 & & & & \\
\multirow[t]{3}{*}{\parbox{.8\linewidth}{Known RAM features removed}} & UK, subtype B & (37806) & UK, subtype C & (17733) \\
 & UK, subtype C & (17733) & UK, subtype B & (37806) \\
 & UK, subtypes B \& C & (55539) & Africa, all subtypes & (3990)  \\
 & & & & \\
\multirow[t]{3}{*}{\parbox{.8\linewidth}{Known RAM features \& sequences with $\geq1$ known RAM removed}} & UK, subtype B & (24422) & UK, subtype C & (13055) \\
 & UK, subtype C & (13055) & UK, subtype B & (24422) \\
 & UK, subtypes B \& C & (37477) & Africa, all subtypes & (2284) \\ \hline
\end{tabular}
\extcaption{All training and testing datasets used during this study.}{
  The number of sequences in each dataset is shown in parentheses
}
\label{tab:tableTrainTest}
% \end{adjustwidth}
\end{table}

\hypertarget{measuring-classifier-performance}{%
\subsection{Measuring classifier performance}\label{measuring-classifier-performance}}

To compare the performance of our classifiers we used balanced accuracy
\autocite{brodersenBalancedAccuracyIts2010}, which is the average of accuracies
(i.e.~percentages of well-classified sequences) computed separately on
each class of the test set. This score takes into account, and corrects
for, the imbalance between RTI-naive and RTI-experienced samples, which
would lead to a classifier always predicting a sequence as RTI-naive
getting a classical accuracy score of up to 77\% (i.e.~the frequency of
naive sequences in the UK dataset). We also computed the adjusted mutual
information (AMI) between predicted and true sequence labels, which is a
normalized version of MI allowing comparison of performance on
differently sized test sets \autocite{vinhInformationTheoreticMeasures2010}.
Additionally, mutual information (MI) was used to compute p-values and
assess the significance of the classifiers' predictive power. The
probabilistic performance of the classifiers was evaluated using an
adapted Brier score \autocite{brierVERIFICATIONFORECASTSEXPRESSED1950} more
suited to binary classification, which is the mean squared difference
between the actual class (coded by 1 and 0 for the RTI-experienced and
RTI-naive samples respectively) and the predicted probability of being
RTI-experienced. This approach refines the standard accuracy measure by
rewarding methods that well approximate the true status of the sample
(eg. predicting a probability of 0.9 while the true status is 1);
conversly, binary methods (predicting 0 or 1, but no probabilities) will
be penalized if they are often wrong. The Brier approach thus assigns
better scores to methods that recognize their ignorance than to methods
producing random predictions.

\hypertarget{hiv-results}{%
\section{Results}\label{hiv-results}}

\hypertarget{classifier-performance-interpretation}{%
\subsection{Classifier performance \& interpretation}\label{classifier-performance-interpretation}}

As can be seen in Fig~\ref{fig:figClassificationPerf}A and
\ref{fig:figClassificationPerf}B, when all RAM features and sequences
were kept in the training and testing sets, classifiers had good
prediction accuracy, with the machine learning classifiers slightly
outperforming the ``Fisher'' classifier. When removing RAM features from
the training and testing sets, the classifiers retained a significant
prediction accuracy, especially with the African data set and its
multiple RAMs that are observed in a large number of sequences (but
removed in this experiment). In this configuration the ML classifiers
had a similar performance to the ``Fisher'' classifier, except for the
random forest that is slightly less accurate, likely due to overfitting.
Also, when removing sequences that had known RAMs, every classifier lost
all prediction accuracy, and none could distinguish RTI-naive from
RTI-experienced sequences. Regarding the Brier sore, we see the
advantage of the machine learning classifiers over the ``Fisher''
classifier, which is worse than random predictions when known RAMs are
removed. The ability of machine learning classifiers to quantify the
resistance status should be an asset for many applications.

\begin{figure}
{
\centering
\includegraphics[width=\linewidth]{./figures/HIV-DRMs/Fig1.eps}
}
\extcaption{Classifier Performance on UK and African datasets.}{
\textbf{NB}: naive Bayes, \textbf{LR}: Logistic Regression with Lasso regularization, \textbf{RF}: Random Forest, \textbf{FC}: Fisher Classifier, \textbf{RD}: Agnostic random probabilistic classifier (this classifier predicts, as the probability of a sample belonging to a class, the frequency of that class in the training data). \textbf{A)} Adjusted mutual information (higher is better) between ground truth and predictions by classifiers trained on dataset with all features (blue), without features corresponding to known RAMs (orange) and without RAM features and without sequences that have at least 1 known RAM (green). Hatching indicates the training set on which a classifier was trained and the testing set on which the performance was measured. The expected value for a null classifier is 0, and 1 for a perfect classifier and a * denotes that the p-value derived from mutual information is $\leq 0.05$. For example when trained with all features all the classifiers have a significative MI. Conversly when removing RAM features and RAM sequences none of the classifiers have a significative MI and only LR trained on the entirety of the UK dataset has an AMI $>10^{-3}$ \textbf{B)} Balanced Accuracy score, i.e. average of accuracies per-class (higher is better) for the same classifiers as in a). The red line at $y=0.5$ is the expected balanced accuracy for a null classifier that only predicts the majority class as well as a random uniform (i.e. 50/50) classifier. \textbf{C)} Brier score, which is the mean squared difference between the sample's experience to RTI and the predicted probability of being RTI experienced (lower is better), for the same classifiers as in \textbf{A)} and \textbf{B)}.}
\label{fig:figClassificationPerf}
\end{figure}

The fact that classifiers retained prediction accuracy after removing
known RAM corresponding features suggests that there was some residual,
unknown resistance-associated signal in the data. The fact that this
same power was non-existent when removing the known RAM-containing
sequences from the training and testing sets, indicates that this
residual signal was contained in these already mutated sequences. This
suggests that the mutations that are found in the RAM removed experiment
(see list below) are most likely accessory mutations that accompany
known RAMs. This also suggests that all primary DRMs (i.e., that
directly confer antiretroviral resistance) have been identified, which
is reassuring from a public health perspective.\\
The performance discrepancy between the UK and African test sets can be
explained by several factors. Firstly, African sequences that have known
RAMs are more likely to have multiple RAMs, and thus more (known and
unknown) resistance-associated features than their UK counterparts (c.f.
Table~\ref{tab:tableData}). This
means that resistant African sequences are easier to detect even when
removing known RAMs. Secondly, RTI-naive sequences in the UK test sets
are more likely to have known RAMs than their African counterparts (c.f.
Table~\ref{tab:tableData}) and
therefore more companion mutations. This means that the RTI-naive
sequences in the UK test set are more likely to be misclassified as
RTI-experienced than in the African test set.

\hypertarget{additional-classification-results}{%
\subsection{Additional classification results}\label{additional-classification-results}}

The fact that, when looking at classifiers trained without known RAMs ,
``Fisher'' classifiers perform as well as the machine learning ones, leads
us to believe that there is little interaction between mutations that
would explain resistance better than taking each mutation separately. It
is therefore likely that the kind of epistatic phenomena we were looking
for, combining several mutations that do not induce any resistance when
taken separately, do not come into play here. We are in a classical
scheme where primary DRMs confer resistance and associated mutations
reinforce the strength of the resistance and/or compensate for the
fitness cost induced by primary DRMs.\\
It is important to remember that in the previous section we were trying
(as usual, e.g.~see \autocite{villabona-arenasIndepthAnalysisHIV12016}) to find
novel mutations associated with resistance by discriminating RTI-naive
from RTI-experienced sequences, both with the statistical tests and the
classifiers. However, this is intrinsically biased and noisy. Indeed, a
RTI-naive sequence is not necessarily susceptible to RTIs as a resistant
strain could have been transmitted to the individual. Conversely, an
RTI-experienced sequence may not be resistant to treatment, due to poor
ART adherence for example. We must therefore keep in mind that the noisy
nature of the relationship between resistance and treatment status is
partly responsible for the lower performance of classifiers trained on
the UK sequences with reduced signal.\\
Moreover, as all the additional resistance signal we detected is
associated to the sequences having at least one known RAM (see above),
we performed another analysis trying to discriminate between the
sequences having at least one known RAM and those having none. The goal
was to check that the mutations we discovered by discriminating
RTI-experienced from RTI-naive samples, are truly accessory and
compensatory mutations. As can be seen in
Fig~\ref{fig:figMultiTasks}A
and \ref{fig:figMultiTasks}B,
the classifiers trained to discriminate sequences that have at least one
known RAM from those that have none, on datasets from which all features
corresponding to known RAMs were removed, perform much better than
classifiers trained to discriminate RTI-experienced from RTI-naive
sequences. This increase in performance is especially visible for
classifiers tested on UK sequences (more difficult to classify than the
African ones, see above), with an AMI often almost one order of
magnitude higher for the known-RAM presence/absence classification task.
This further reinforces our belief that all there is a fairly strong
residual resistance-signal in sequences that contain known RAMs, due to
new accessory and compensatory mutations identified by our classifiers
and Fisher tests. As a side note, Logistic regression (LR) consistently
outperforms other classifiers, a tendency already observed in
Fig~\ref{fig:figClassificationPerf}.

\begin{figure}
{
\centering
\includegraphics[width=\linewidth]{./figures/HIV-DRMs/Fig2.eps}
}
\extcaption{Discrimination between sequences having at least one RAM, and those having none on sequences with training features corresponding to known RAMs removed.}{
\textbf{NB}: naive Bayes, \textbf{LR}: Logistic Regression with Lasso regularization, \textbf{RF}: Random Forest, \textbf{FC}: Fisher Classifier. \textbf{A)} Adjusted mutual information (higher is better) for classifiers trained without features corresponding to known RAMs. The classifiers are either trained to discriminate RTI-naive from RTI-experienced sequences (blue), or sequences with at least one known RAM from sequences that have none (orange). Hatching and braced annotations indicate the training and testing sets resulting in a given performance measure. \textbf{B)} Balanced accuracy, i.e. average of accuracies per-class for the same classifiers as in \textbf{A)} (higher is better). The red line at $y=0.5$ is the expected value for a classifier only predicting the majority class as well as a random uniform (50/50) classifier.}
\label{fig:figMultiTasks}
\end{figure}

\hypertarget{identifying-new-mutations-from-classifiers}{%
\subsection{Identifying new mutations from classifiers}\label{identifying-new-mutations-from-classifiers}}

We assessed the importance of each mutation in the learned internal
model of all the classifiers, in the setting where all known RAMs have
been removed from the training dataset. For the Fisher classifier, we
used one minus the p-value of the exact Fisher test as the importance
value, therefore the more significantly associated mutations have the
higher importance value and were ranked first. For a given
classification task, we ranked each mutation according to the
appropriate importance value for each classifier (see above), trained on
the B or C subtypes, with the highest importance value having a rank of 0. We then computed the average rank for each mutation and each
classification task (RTI-naive/RTI-experienced and RAM present/RAM
absent). This gave us, for each classification task, a ranking of
mutations potentially associated with resistance that took into account
the importance given to this new mutation by each classifier trained on
this task. Mutations that were in the 10 most important mutations for
both of the classification tasks were considered of interest. Based on
these criteria we selected the following potentially
resistance-associated mutations (w.r.t. the HXB2 reference genome):
L228R, L228H, E203K, D218E, I135L and H208Y. These mutations are
referred to as ``new mutations'' in the rest of this study.\\
To check the epistatic nature of these selected mutations we computed
the relative risk \(RR(new, X)\) between a new mutation and a binary
character \(X\). \(RR(new,X)\) was computed from the contingency table
between \(new\) and \(X\) as follows:

\begin{htmlonly}
\[
RR(new,X) = \frac{A}{A+C} \div \frac{B}{B+D}
\]

\end{htmlonly}

\begin{minipage}{0.45\textwidth}
    \bigskip
    \begin{tabular}{ccc}
                    & X present & X absent\\ \midrule
        new present & A         & B       \\
        new absent  & C         & D       \\ \bottomrule
    \end{tabular}
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \begin{equation*}
        RR(new,X) = \frac{A}{A+C} \div \frac{B}{B+D}
    \end{equation*}
\end{minipage}
\bigskip

The RR gives us a measure for how over-represented each of our new
mutations is in sequences that have the \(X\) character compared to those
that don't.\\
To get a general idea of this over-representation, for each new mutation
we computed \(RR(new, treatment)\) comparing the prevalence of the new
mutation in RTI-experienced and RTI-naive sequences. We also computed
\(RR(new, with RAM)\) comparing the prevalence the new mutation in
sequences having at least one known RAM and sequences that have none.
Both of these RRs are shown in
Table~\ref{tab:tabMutations}
for each new mutation.\\
We then computed \(RR(new, RAM)\) for each known RAM present in more than
0.1\% of UK sequences and the new mutations. In
Fig~\ref{fig:figUkRatios} we
see the RRs for which the lower bound of the 95\% confidence interval,
computed on 1000 bootstrap samples from the UK dataset, was greater than 4.

\hypertarget{detailed-analysis-of-potentially-resistance-associated-mutations}{%
\subsection{Detailed analysis of potentially resistance-associated mutations}\label{detailed-analysis-of-potentially-resistance-associated-mutations}}

As can be seen in Table~\ref{tab:tabMutations}, all of these new mutations except for I135L,
are highly over-represented in RTI-experienced sequences and sequences
that already have known RAMs, with lower bounds on the 95\% RR CI always
greater than 5, and often exceeding 10. When looking at the RRs computed
for individual RAMs on the UK dataset
(Fig~\ref{fig:figUkRatios}),
this impression is confirmed with very high over-representation of these
new mutations potentially associated with resistance in sequences that
have a given known RAM, with 95\% RR lower CI bounds sometimes greater
than 80 (H208Y/L210W and D218E/D67N), and most of the time greater than 10. with the noticeable exception of I135L where only 2 known RAMs give
RRs with lower CI bounds greater than 4. The RRs computed on the African
dataset (\ref{fig:s1}) tell a similar story albeit with smaller RR values due to a
smaller number of occurrences of both new mutations and known RAMs.\\

\begin{table}
% \begin{adjustwidth}{-2.25in}{0in}
\centering
\begin{tabular}{@{}lllllllll@{}}
\hline
\multirow{3}{*}{} & \multicolumn{2}{l}{\multirow{2}{*}{codon distance}} & & \multicolumn{3}{l}{UK} & \\ \cmidrule(lr){5-7}
& \multicolumn{2}{l}{} & & & \multicolumn{2}{l}{$RR(new,X)$} & \\ \cmidrule(lr){2-3} \cmidrule(lr){6-7}
& min & avg & B62 & count & $treatment$ & $any~RAM$ & p-value \\ \hline
\multirow{2}{*}{\textbf{L228R}} & \multirow{2}{*}{1} & \multirow{2}{*}{1.16} & \multirow{2}{*}{-2} & \multirow{2}{*}{227 (0.4\%)} & 18.1 & 115.7 & \multirow{2}{*}{$3.4\cdot10^{-31}$} \\
& & & & & {[}12.9;27.3{]} & {[}55.1;507.3{]} & \\
\multirow{2}{*}{\textbf{E203K}} & \multirow{2}{*}{1} & \multirow{2}{*}{1.31} & \multirow{2}{*}{1} & \multirow{2}{*}{256 (0.5\%)} & 11 & 20.1 & \multirow{2}{*}{$1.1\cdot10^{-14}$} \\
& & & & & {[}8.2;15.1{]} & {[}13.7;32.1{]} & \\
\multirow{2}{*}{\textbf{D218E}} & \multirow{2}{*}{1} & \multirow{2}{*}{1} & \multirow{2}{*}{2} & \multirow{2}{*}{168 (0.3\%)} & 13.1 & 27 & \multirow{2}{*}{$3.3\cdot10^{-10}$} \\
& & & & & {[}9.0;19.6{]} & {[}16.3;57.0{]} & \\
\multirow{2}{*}{\textbf{L228H}} & \multirow{2}{*}{1} & \multirow{2}{*}{1.12} & \multirow{2}{*}{-3} & \multirow{2}{*}{287 (0.5\%)} & 6.4 & 9.2 & \multirow{2}{*}{$4.4\cdot10^{-16}$} \\
& & & & & {[}5.1;8.4{]} & {[}6.9;12.6{]} & \\
\multirow{2}{*}{\textbf{I135L}} & \multirow{2}{*}{1} & \multirow{2}{*}{1.16} & \multirow{2}{*}{2} & \multirow{2}{*}{540 (1.0\%)} & 1.8 & 2.4 & \multirow{2}{*}{$5.9\cdot10^{-08}$} \\
& & & & & {[}1.5;2.1{]} & {[}2.0;2.8{]} & \\
\multirow{2}{*}{\textbf{H208Y}} & \multirow{2}{*}{1} & \multirow{2}{*}{1.10} & \multirow{2}{*}{2} & \multirow{2}{*}{205 (0.4\%)} & 8.8 & 14.9 & \multirow{2}{*}{$1.2\cdot10^{-05}$} \\
& & & & & {[}6.5;12.5{]} & {[}9.9;23.6{]} & \\ \hline
\textbf{RAMs} & 1 & 1.35 & 0 & 58~~(0.1\%) & 8.3 & 26.4 & $3.1\cdot10^{-2}$\\
  & \footnotesize{[1;2]} & \footnotesize{[1;2.44]} & \footnotesize{[-2;3]} & \footnotesize{[2;1842]} & \footnotesize{[0.6;$\infty$]}& \footnotesize{[1.4;$\infty$]}& \footnotesize{[$2.3\cdot10^{-58}$;1]}\\ \hline
\end{tabular}
\extcaption{Analysis of new potential RAMs.}{
  \textbf{Codon distance:} For each new mutation we computed the minimum number of nucleotide mutations to go from the wild amino acid codons to those of the mutated amino acid, as well as the average codon distance between both amino acids, weighted by the prevalence of each wild and mutated codon at the given position in the UK  dataset.
  \textbf{B62:} BLOSUM62 similarity values (e.g. D218E = 2, reflecting that E and D are both negatively charged and highly similar).
  \textbf{Count:} We looked at the number of occurrences of each new potential RAM in the UK dataset and the corresponding prevalence in parentheses.
  \textbf{Relative risks:} We computed $RR(new, treatment)$ (e.g. L228R is 18.1 times more prevalent in RTI-experienced sequences compared to RTI-naive sequences in the UK dataset). We also computed $RR(new, any~RAM)$ (e.g. L228R is 115.7 times more prevalent in sequences that have at least one known RAM than in sequences that have none in the UK dataset). The 95\% confidence intervals shown under each RR were computed with 1000 bootstrap samples of size $n=55,000$ drawn with replacement from the whole UK dataset.
  \textbf{p-values:} Fisher exact tests were done on the African dataset (to avoid confounding effects due to phylogenetic correlation) to see if each of these new mutations were more prevalent in RTI-experienced sequences. The same metrics were computed for all known RAMs, the median values are shown in the last two lines of this table, as well as the 5\textsuperscript{th} and 95\textsuperscript{th} percentiles which are shown underneath. $RR(RAM,any~RAM)$ values were computed for any RAM except itself to avoid always having infinite ratios.
}
\label{tab:tabMutations}
% \end{adjustwidth}
\end{table}

\begin{figure}
{
\centering
\includegraphics[width=\linewidth]{./figures/HIV-DRMs/Fig3.eps}
}
\extcaption{Relative risk of the new mutations with regards to known RAMs on the UK dataset}{
(i.e. the prevalence of the new mutation in sequences with a given known RAM divided by the prevalence of the new mutation in sequences without this RAM). RRs were only computed for mutations (new and RAMs) that appeared in at least 0.1\% (=55) sequences. 95\% confidence intervals, represented by vertical bars, were computed with 1000 bootstrap samples of UK sequences. Only RRs with a lower CI boundary greater than 4 are shown. The shape and color of the point represents the type of RAM as defined by Stanford’s HIVDB. Blue circle: NRTI, orange square: NNRTI, green diamond: Other. RR values are shown from left to right, by order of decreasing values on the lower bound of the 95\% CI.}
\label{fig:figUkRatios}
\end{figure}

The genetic barrier to resistance for each of these new mutations is
quite low, with a minimum of 1 base change for each of them
(Table~\ref{tab:tabMutations}
). We also computed the average codon distance
(i.e.~number of different bases), weighted by the prevalence of wild and
mutated codons at the given positions in the UK
(Table~\ref{tab:tabMutations}
) and Africa (Table \ref{S1-Table}) datasets, and in each case the
average codon distance was always close to 1. In other words, at the
amino acid level these mutations are expected to be relatively frequent.
However, their frequencies are much higher in treated/with-RAM sequences
than in naive/without-RAM ones
(Table~\ref{tab:tabMutations}
). Moreover, if we look at the BLOSUM62 scores
(Table~\ref{tab:tabMutations}
), some of these mutations induce some
substantial changes in physicochemical properties, most notably at site
228, which reinforces again the likelihood that these mutations are
associated with resistance. These metrics were also computed for all
known RAMs (Table~\ref{tab:tabMutations}
). For all these metrics, and the 6 new
potential RAMs, values are contained between the 5\textsuperscript{th} and 95\textsuperscript{th}
percentiles computed on known RAMs, except for the BLOSUM score of L228H
that corresponds to a drastic physicochemical change.\\
To gain more insight on these new mutations we also observed their
spatial location on the 3-D HIV-1 RT structure using PyMol \autocite{PyMOL}.
HIV-1 RT is a heterodimer with two subunits translated from the same
sequence with different lengths and 3-D structures. The smaller p51
subunit (440 AAs) has a mainly structural role, while the larger p66
(560 AAs) subunit has the active site at positions 110, 185 and 186. The
p66 subunit also has a regulatory pocket behind the active site: the
non-nucleoside inhibitor binding pocket (NNIBP) formed of several sites
of the p66 subunit as well as site 138 of the p51 subunit. Nucleoside RT
Inhibitors (NRTI) are nucleotide analogs and bind in the active site,
blocking reverse transcription. Non-Nucleoside RT Inhibitors (NNRTI)
bind in the NNIBP, changing the protein conformation and blocking
reverse transcription. More details on the structure and function of
HIV-1 RT can be found in \autocite{sarafianosStructureFunctionHIV12009}. A
general view of where the new mutations are situated with regards to the
other important sites of HIV-1 RT is shown in
Fig~\ref{fig:figStructure},
and is detailed below.

\begin{figure}
{
\centering
\includegraphics[width=\linewidth]{./figures/HIV-DRMs/Fig4.png}
}
\extcaption{Structure of HIV-1 RT with highlighted important sites.}{
The p66 subunit is colored dark gray and the p51 subunit white. The active site is highlighted in blue, and the NNIBP is highlighted in yellow. The sites of new mutations are colored in red.}
\label{fig:figStructure}
\end{figure}

\hypertarget{l228r-l228h}{%
\subsubsection{L228R / L228H}\label{l228r-l228h}}

L228R is the most important of these new mutations according to the
feature importance ranking done above. This is reflected in the very
high over-representation in RTI-experienced sequences and sequences with
known RAMs shown in Table~\ref{tab:tabMutations}
. When looking at the detailed RRs shown in
Fig~\ref{fig:figUkRatios}, we
observe that L228R presents high RR values with mainly NRTI RAMs, but
also with NNRTI RAMs such as Y181C and L100I, and this is even more so h
for RRs computed on the African dataset (\ref{fig:s1}). L228H is very similar in all
regards to L228R, however its highest RRs are exclusively with NRTI
RAMs.\\
Site 228 of the p66 subunit is located very close to the active site of
RT, where NRTIs operate (Figs~\ref{fig:figStructure} and \ref{fig:s3}) which could explain the role that L228R
and L228H seem to have in NRTI resistance. However, site 228 of the p66
subunit is also between sites 227 and 229 which are both part of the
NNIBP. Furthermore, both L228H and L228R have very low BLOSUM62 score,
of -3 and -2 respectively (Table~\ref{tab:tabMutations}
). Arginine (R) and Histidine (H) are both less
hydrophobic that Leucine (L), and have positively charged side-chains.
This important change in physicochemical properties could explain the
role they both seem to have in NRTI resistance. However, while both
Arginine and Histidine are larger than Leucine, Arginine is also fairly
larger than Histidine, which is aromatic. This difference between both
residues might explain the association L228R seems to have with NNRTI
resistance that L228H does not have.

\hypertarget{e203k-h208y}{%
\subsubsection{E203K / H208Y}\label{e203k-h208y}}

Both E203K and H208Y are highly over-represented in RTI-experienced
sequences and sequences with known RAMs. They both have high RR values
for NRTI RAMs. Furthermore the most highly valued RAM RRs in
Fig~\ref{fig:figUkRatios}, are
very similar for E203K and H208Y. Structurally they are close to each
other on an alpha helix which is close to the active site.\\
Both E203K and H208Y have positive, albeit not maximal, BLOSUM62 scores,
meaning they are fairly common substitutions. However, these mutations
induce some change in physicochemical properties with Tyrosine (Y) being
less polar than Histidine (H), and the change from Glutamic Acid (E) to
Lysine (K) corresponding to a change from a negatively charged side
chain to a positively charged one.\\
All this, combined with their structural proximity and the shared high
RR values for single RAMs, suggests a similar role in NRTI resistance.

\hypertarget{i135l}{%
\subsubsection{I135L}\label{i135l}}

In Table~\ref{tab:tabMutations} and Fig~\ref{fig:figUkRatios},
we observe that I135L has the lowest RR values
of all the new mutations, with CI bounds lower than 2 in
Table~\ref{tab:tabMutations}'s general RRs. However, it is the most
prevalent of the new mutations. If we look at the detailed RRs of
Fig~\ref{fig:figUkRatios}, we
see that I135L is significantly over-represented in sequences with NNRTI
RAMs, specifically A98G and P225H. Structurally this makes sense: On the
p66 subunit, site 135 is on the outside, far from both the active site
and the NNIBP. However, site 135 on the p51 subunit is located very
close to the NNIBP (Figs~\ref{fig:figUkRatios} and \ref{fig:s2}).\\
The BLOSUM62 score for this substitution is quite high
(Table~\ref{tab:tabMutations}), which is expected since both residues are
very similar to one another, differing only by the positioning of one
methyl group. However, Leucine (L) is less hydrophobic than Isoleucine
(I), despite they are still both classified as hydrophobic residues (Table \ref{S1-Table}).\\
The proximity between site 135 and the pocket in which NNRTI RAMs bind,
as well as the high RR values for these NNRTI RAMs leads us to believe
that I135L could play a subtle accessory role in NNRTI resistance,
either by enhancing the effect of some NNRTI RAMs (typically, A98G and
P225H), or by compensating for loss of fitness.

\hypertarget{d218e}{%
\subsubsection{D218E}\label{d218e}}

D218E is also highly over-represented in both RTI-experienced sequences
and sequences with known RAMs. It has infinite RR values in the African
dataset (Table~\ref{tab:tabMutations}), because it is quite rare in this dataset,
and all of its 25 occurrences are in sequences that have at least one
known RAM and are RTI-experienced. In fact, from the UK dataset we can
see that D218E has some of the highest RR values for individual RAMs
(along with H208Y). The majority of these very high RR values occur for
NRTI RAMs. Site 218 on the p66 subunit is quite close to the RT active
site, which could explain the role D218E seems to have in NRTI
resistance. Aspartic acid (D) and Glutamic acid (E) are very similar
amino acids, both acidic with negatively charged side-chains, as
reflected in their fairly high BLOSUM62 score, the main difference
between both being molecular weight, with E being slightly larger than
D.

\hypertarget{discussion-and-perspectives}{%
\section{Discussion and perspectives}\label{discussion-and-perspectives}}

Our method has allowed us to identify six mutations that might play a
role in drug resistance in HIV. These mutations are significantly
over-represented in RTI-experienced sequences, as well as sequences
exhibiting at least one other known RAM. The fact that models trained on
the UK are still performant on such a different dataset as the African
one strongly suggests that the learned classifier models have acquired
generalized knowledge on resistance. For all of these new mutations
their spatial positioning on HIV-1 RT is consistent with our
conclusions, as all were either close to the active site or the
regulatory binding pocket.\\
Some of the mutations we have identified as potentially associated with
resistance have been mentioned in previous studies. L228R/H have been
observed before \autocite{rheeHIV1SubtypeProtease2007} and were suggested to be
associated with reduced susceptibility to didanosine
\autocite{delucaImprovedInterpretationGenotypic2007,marcelinImpactHIV1Reverse2006}.
I135L has been observed in sequences with reduced susceptibility to
NNRTIs \autocite{brownReducedSusceptibilityHuman2000}. H208Y has been associated
with NNRTI and NRTI resistance \autocite{clarkReverseTranscriptaseMutations2006}
and it has been suggested that it has an accessory role in NRTI
resistance \autocite{nebbiaEmergenceH208YMutation2007}. E203K, D218E, L228RH and
H208Y have all been mentioned in \autocite{saracinoImpactUnreportedHIV12006} as
probably linked to phenotypic resistance to NRTI and NNRTI.\\
However, none of these mutations has been experimentally confirmed as
conferring or helping with drug resistance to the best of our knowledge.
The fact that we find them again with a big data analysis of highly
different sequences and involved statistical selection procedure
combining multiple testing and machine learning, and that we have very
high significance, clearly indicates their potential role in resistance.
Therefore, we believe they are sufficiently linked to drug resistance
that they garner a closer inspection either in-vitro or in-vivo to
determine the mechanisms that could allow them to play a role in
resistance.\\
With our machine classifiers we seem to have found some RAMs of an
accessory nature, over-represented in sequences already containing known
RAMs. This is a form of epistasis, where the interaction between the
main RAM and the accessory RAM is important. However, we did not manage
to find subtler forms of epistasis, in our dataset, where two mutations
separately have no effect on resistance but have an effect together.
This is partly indicated by the fact that there is a limited performance
gap between the Fisher exact tests and more sophisticated classifiers,
that are able to reveal significant association of mutations, while each
individual mutation has low prediction power. However, one advantage of
machine learning classifiers, is that they are probabilistic, meaning
that they can give more nuanced insights into the nature or resistance
level of a given sequence than the classical binary presence/absence of
RAMs approach. In this regard logistic regression appears as a method of
choice, showing similar or better performance than other classifiers,
and an easy interpretation that is facilitated by the lasso
regularization which performs a simple feature selection and retains the
most important ones. Similar results were already observed on other
sequence analysis tasks \autocite{wuGenomewideAssociationAnalysis2009}. In order
to investigate the second form of epistasis further we tested each pair
of mutations in the UK dataset (\(n=867,903\)) with Fisher exact tests to
see if they were linked to treatment status. In order to mitigate the
effects of phylogenetic correlation which are sure to have an effect in
this type of setting, we tested the pairs that were significantly
associated to treatment (\(n=1,309\)) again on the African dataset. We
also compared these results to the Fisher exact tests executed for each
single mutation. We did not find any pair of mutations that was
significantly associated, to treatment where neither member were
significantly associated individually. Moreover, we only found 3
significantly associated pairs of mutations that did not include at
least one known RAM, and they all included one of our newly found
potential RAM: L228R + I142V, L228R + F214L and L228H + F214L (see appendix \ref{S2-Appendix} for
details).\\
With therapeutic strategies targeting multiple proteins that are now
used, there might be some epistatic effects with other regions of the
HIV genome that are targeted by some of the drugs. These potential
effects however, lie outside the scope of this study.\\
Because of the lack of detailed treatment history metadata, we did not
distinguish mutations arising from NRTIs or NNRTIs. We believe that a
large amount of high quality sequence data, along with a sufficiently
detailed log of treatments and drugs the sequences were exposed to,
could allow us to use our machine-learning approach to find mutations
related to specific drugs and thus furthering our knowledge of HIV drug
resistance, giving clinicians more tools to manage and help infected
patients.

\hypertarget{hiv-acknowledgments}{%
\section*{Acknowledgments}\label{hiv-acknowledgments}}
\addcontentsline{toc}{section}{Acknowledgments}

We thank Anna Zhukova, Frédéric Lemoine and Marie Morel for their help
and suggestions.\\
We also thank the UK HIV Drug Resistance Database and the UK
Collaborative HIV Cohort:\\

\begin{description}
\item[Steering committee:]
David Asboe, Anton Pozniak (Chelsea \& Westminster Hospital, London);
Patricia Cane (Public Health England, Porton Down); David Chadwick
(South Tees Hospitals NHS Trust, Middlesbrough); Duncan Churchill
(Brighton and Sussex University Hospitals NHS Trust); Simon Collins
(HIV i-Base, London); Valerie Delpech (National Infection Service,
Public Health England); Samuel Douthwaite (Guy's and St.~Thomas' NHS
Foundation Trust, London); David Dunn, Kholoud Porter, Anna
Tostevin, Oliver Stirrup (Institute for Global Health, UCL);
Christophe Fraser (University of Oxford); Anna Maria Geretti
(Institute of Infection and Global Health, University of Liverpool);
Rory Gunson (Gartnavel General Hospital, Glasgow); Antony Hale
(Leeds Teaching Hospitals NHS Trust); Stéphane Hué (London School of
Hygiene and Tropical Medicine); Michael Kidd (Public Health England,
Birmingham Heartlands Hospital); Linda Lazarus (Expert Advisory
Group on AIDS Secretariat, Public Health England); Andrew
Leigh-Brown (University of Edinburgh); Tamyo Mbisa (National
Infection Service, Public Health England); Nicola Mackie (Imperial
NHS Trust, London); Chloe Orkin (Barts Health NHS Trust, London);
Eleni Nastouli, Deenan Pillay, Andrew Phillips, Caroline Sabin
(University College London, London); Kate Templeton (Royal Infirmary
of Edinburgh); Peter Tilston (Manchester Royal Infirmary); Erik Volz
(Imperial College London, London); Ian Williams (Mortimer Market
Centre, London); Hongyi Zhang (Addenbrooke's Hospital, Cambridge).
\item[Coordinating Center:]
Institute for Global Health, UCL (David Dunn, Keith Fairbrother,
Anna Tostevin, Oliver Stirrup)
\item[Centers contributing data:]
Clinical Microbiology and Public Health Laboratory, Addenbrooke's
Hospital, Cambridge (Justine Dawkins); Guy's and St Thomas' NHS
Foundation Trust, London (Emma Cunningham, Jane Mullen); PHE --
Public Health Laboratory, Birmingham Heartlands Hospital, Birmingham
(Michael Kidd); Antiviral Unit, National Infection Service, Public
Health England, London (Tamyo Mbisa); Imperial College Health NHS
Trust, London (Alison Cox); King's College Hospital, London (Richard
Tandy); Medical Microbiology Laboratory, Leeds Teaching Hospitals
NHS Trust (Tracy Fawcett); Specialist Virology Centre, Liverpool
(Elaine O'Toole); Department of Clinical Virology, Manchester Royal
Infirmary, Manchester (Peter Tilston); Department of Virology, Royal
Free Hospital, London (Clare Booth, Ana Garcia-Diaz); Edinburgh
Specialist Virology Centre, Royal Infirmary of Edinburgh (Lynne
Renwick); Department of Infection \& Tropical Medicine, Royal
Victoria Infirmary, Newcastle (Matthias L Schmid, Brendan Payne);
South Tees Hospitals NHS Trust, Middlesbrough (David Chadwick);
Department of Virology, Barts Health NHS Trust, London (Mark
Hopkins); Molecular Diagnostic Unit, Imperial College, London (Simon
Dustan); University College London Hospitals (Stuart Kirk); West of
Scotland Specialist Virology Laboratory, Gartnavel, Glasgow (Rory
Gunson, Amanda Bradley-Stewart).
\end{description}

\hypertarget{supporting-information}{%
\section*{Supporting Information}\label{supporting-information}}
\addcontentsline{toc}{section}{Supporting Information}

Supporting Information can be found in the appendix \ref{HIV-appendix}

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

\hypertarget{learning-alignments-an-interesting-perspective}{%
\chapter{Learning alignments, an interesting perspective}\label{learning-alignments-an-interesting-perspective}}

\hypertarget{learning-pairwise-alignment}{%
\section{Learning pairwise alignment}\label{learning-pairwise-alignment}}

\hypertarget{dedal}{%
\subsection{DEDAL}\label{dedal}}

\begin{itemize}
\item
  reference to transformer embedding
\item
  Predict substitution matrix
\item
  Reference other similar works
\item
  drawback: only on proteins
\end{itemize}

\hypertarget{predicting-an-alignment}{%
\subsection{predicting an alignment}\label{predicting-an-alignment}}

\begin{itemize}
\item
  Transformer models can also predict tokens -\textgreater{} predict ``CIGAR string'' or a an aligned sequence.
\item
  Challenges:

  \begin{itemize}
  \item
    Longer sequences in DNA
  \item
    Size difference in the case of mapping
  \item
    Less information in a single nucleotide token than in proteins\ldots.
  \end{itemize}
\end{itemize}

\hypertarget{what-else-could-we-learn}{%
\section{What else could we learn ?}\label{what-else-could-we-learn}}

\hypertarget{learn-to-predict-seeds-or-starting-positions}{%
\subsection{Learn to predict seeds or starting positions}\label{learn-to-predict-seeds-or-starting-positions}}

\begin{itemize}
\item
  DeepMinimizer
\item
  predict start position given a pair of sequences
\end{itemize}

\hypertarget{learn-pre-processing-functions}{%
\subsection{Learn pre-processing functions}\label{learn-pre-processing-functions}}

i.e.~either connections in MSR graph or sequence 2 sequence models

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

\hypertarget{global-conclusion}{%
\chapter*{Global conclusion}\label{global-conclusion}}
\addcontentsline{toc}{chapter}{Global conclusion}

\hypertarget{hpc-part}{%
\section*{HPC part}\label{hpc-part}}
\addcontentsline{toc}{section}{HPC part}

\begin{itemize}
\item
  We have developed a method to improve mapping by pre-processing biological sequences

  \begin{itemize}
  \item
    In terms of error rate and fraction of mapped reads
  \item
    Although transformations selected on whole human genome generalises to D. melanogaster and E coli genomes.
  \end{itemize}
\item
  Where to go next ?

  \begin{itemize}
  \item
    explore a larger space of transformations:

    \begin{itemize}
    \item
      develop more efficient ways to explore search space
    \item
      new search space reduction mehods
    \end{itemize}
  \item
    Explore different applications: assembly, clustering, \ldots{}
  \item
    Explore different types of transformations, i.e.~less constraints given by us, ML/Seq2Seq,\ldots{}
  \end{itemize}
\end{itemize}

\hypertarget{hiv-part}{%
\section*{HIV part}\label{hiv-part}}
\addcontentsline{toc}{section}{HIV part}

\begin{itemize}
\item
  We have used machine learning in order to find new drug resistance mutations in HIV

  \begin{itemize}
  \item
    We have showed a link to drug resistance for 6 mutations of the RT-pol protein, currently not classified as DRMS, but they have been identified as potentially linked to resistance previously
  \item
    These mutations seem to be accessory mutations and do not
  \item
    There seems to be no complex epistasis phenomena
  \end{itemize}
\item
  Where to go next:

  \begin{itemize}
  \item
    Confirm experimentally / in vivo the role these mutations have in resistance
  \item
    Explore more sensitive methods, lots of work on interpretable DL -\textgreater{} restricted by dataset size\ldots{}
  \item
    Explore metadata rich data, e.g.~per treatment,
  \item
    Explore other organisms for which we have data -\textgreater{} HCV ?
  \end{itemize}
\end{itemize}

\hypertarget{final-words}{%
\section*{Final words}\label{final-words}}
\addcontentsline{toc}{section}{Final words}

Alignments are fundamental, improvements in the creation and analysis steps are crucial and likely to help other people gain insight in biological processes. \textbf{\emph{etc\ldots{}}}

\appendix

\fancyhead[LE]{APPENDIX \thechapter}

\hypertarget{HPC-appendix}{%
\chapter{Supporting Information for ``Mapping-friendly sequence reductions: going beyond homopolymer compression''}\label{HPC-appendix}}

\hypertarget{appendix:tandemtools}{%
\section{``TandemTools'' dataset generation}\label{appendix:tandemtools}}

This dataset was obtained by taking a human X chromosome HOR sequence, concatenating it 500 times with added mutations in order to obtain an approximately 1 Mbp long sequence. Then 1200 reads were simulated from the sequence using \texttt{nanosim} \autocite{yangNanoSimNanoporeSequence2017} and assembled using a centromere-tailored pipeline \autocite{bzikadzeAutomatedAssemblyCentromeres2020}. A 10kbp deletion was then added to this assembly. The resulting sequence is the one we refer to as the ``Centromeric sequence''.

\hypertarget{msr-performance-comparison}{%
\section{MSR performance comparison}\label{msr-performance-comparison}}

\begin{table}[H]
\extcaption{Comparing performance of MSRs on the whole human genome, whole \textit{Drosophila melanogaster} genome, repeated regions of the whole human genome and synthetic centromeric sequence.}{Results using  \minimap \autocite{liMinimap2PairwiseAlignment2018} and \winnowmap  \autocite{jainWeightedMinimizerSampling2020}. The number of simulated reads for each reference sequence is given in parentheses and called $n$. Results are reported for mapq thresholds of 60, 50 and 0. The best performance for each category is highlighted in bold. The percentage difference are computed w.r.t HPC at each given threshold.}
\label{tab:mapperComparison}
{\centering
\resizebox{0.93\textwidth}{!}{%
\begin{tabular}{@{}lr@{}lr@{}lr@{}lr@{}lr@{}lr@{}l@{}}
\toprule
                                    & \multicolumn{4}{l}{mapq=60}                                                                                 & \multicolumn{4}{l}{mapq$\geq50$}                                                                            & \multicolumn{4}{l}{any mapq}                                                                              \\ \cmidrule(l){2-5}\cmidrule(l){6-9}\cmidrule(l){6-9}\cmidrule(l){10-13}
mapping friendly sequence reduction & \multicolumn{2}{l}{fraction}                       & \multicolumn{2}{l}{error}                              & \multicolumn{2}{l}{fraction}                       & \multicolumn{2}{l}{error}                              & \multicolumn{2}{l}{fraction}                      & \multicolumn{2}{l}{error}                             \\ \midrule
                                                                                                                                                                                                                                                                                                                                                                            \\
\multicolumn{13}{l}{\textbf{Whole Drosophila melanogaster genome - \minimap (n = \numprint{25764})}}                                                                                                                                                                                                                                                                                  \\
HPC                                 & 0.957          & \footnotesize{\;$+$0\%}           & 2.27e-03          & \footnotesize{\;$+$  0\%}          & \textbf{0.963} & \textbf{\footnotesize{\;$+$0\%}}  & 2.34e-03          & \footnotesize{\;$+$  0\%}          & \textbf{0.998} & \textbf{\footnotesize{\;$+$0\%}} & 1.48e-02          & \footnotesize{\;$+$ 0\%}          \\
raw                                 & \textbf{0.958} & \textbf{\footnotesize{\;$+$0\%}}  & 2.27e-03          & \footnotesize{\;$-$  0\%}          & 0.962          & \footnotesize{\;$-$0\%}           & 2.34e-03          & \footnotesize{\;$+$  0\%}          & 0.997          & \footnotesize{\;$-$0\%}          & \textbf{1.17e-02} & \textbf{\footnotesize{\;$-$21\%}} \\
\msr{F}                             & 0.952          & \footnotesize{\;$-$1\%}           & 1.18e-03          & \footnotesize{\;$-$ 48\%}          & 0.960          & \footnotesize{\;$-$0\%}           & 1.37e-03          & \footnotesize{\;$-$ 41\%}          & \textbf{0.998} & \textbf{\footnotesize{\;$+$0\%}} & 1.36e-02          & \footnotesize{\;$-$ 8\%}          \\
\msr{E}                             & 0.946          & \footnotesize{\;$-$1\%}           & \textbf{0}     & \textbf{\footnotesize{\;$-$100\%}} & 0.954          & \footnotesize{\;$-$1\%}           & \textbf{0}     & \textbf{\footnotesize{\;$-$100\%}} & \textbf{0.998} & \textbf{\footnotesize{\;$+$0\%}} & 1.53e-02          & \footnotesize{\;$+$ 3\%}          \\
\msr{P}                             & 0.950          & \footnotesize{\;$-$1\%}           & 4.90e-04          & \footnotesize{\;$-$ 78\%}          & 0.957          & \footnotesize{\;$-$1\%}           & 8.11e-04          & \footnotesize{\;$-$ 65\%}          & \textbf{0.998} & \textbf{\footnotesize{\;$-$0\%}} & 1.39e-02          & \footnotesize{\;$-$ 6\%}          \\
                                                                                                                                                                                                                                                                                                                                                                            \\
\multicolumn{13}{l}{\textbf{Whole Drosophila melanogaster genome - \winnowmap (n = \numprint{25764})}}                                                                                                                                                                                                                                                                                \\
HPC                                 & 0.923          & \footnotesize{\;$+$0\%}           & 1.51e-03          & \footnotesize{\;$+$ 0\%}           & 0.930          & \footnotesize{\;$+$0\%}           & 1.59e-03          & \footnotesize{\;$+$ 0\%}           & 0.989          & \footnotesize{\;$+$0\%}          & 1.50e-02          & \footnotesize{\;$+$ 0\%}          \\
raw                                 & \textbf{0.949} & \textbf{\footnotesize{\;$+$3\%}}  & 1.92e-03          & \footnotesize{\;$+$27\%}           & \textbf{0.954} & \textbf{\footnotesize{\;$+$3\%}}  & 1.99e-03          & \footnotesize{\;$+$26\%}           & \textbf{0.995} & \textbf{\footnotesize{\;$+$1\%}} & \textbf{1.33e-02} & \textbf{\footnotesize{\;$-$12\%}} \\
\msr{F}                             & 0.918          & \footnotesize{\;$-$1\%}           & \textbf{1.27e-03} & \textbf{\footnotesize{\;$-$16\%}}  & 0.925          & \footnotesize{\;$-$0\%}           & \textbf{1.30e-03} & \textbf{\footnotesize{\;$-$18\%}}  & 0.987          & \footnotesize{\;$-$0\%}          & 1.37e-02          & \footnotesize{\;$-$ 9\%}          \\
\msr{P}                             & 0.905          & \footnotesize{\;$-$2\%}           & 1.33e-03          & \footnotesize{\;$-$12\%}           & 0.912          & \footnotesize{\;$-$2\%}           & 1.53e-03          & \footnotesize{\;$-$3\%}            & 0.983          & \footnotesize{\;$-$1\%}          & 1.40e-02          & \footnotesize{\;$-$ 7\%}          \\
\msr{E}                             & 0.905          & \footnotesize{\;$-$2\%}           & 1.42e-03          & \footnotesize{\;$-$ 6\%}           & 0.912          & \footnotesize{\;$-$2\%}           & 1.49e-03          & \footnotesize{\;$-$ 6\%}           & 0.983          & \footnotesize{\;$-$1\%}          & 1.44e-02          & \footnotesize{\;$-$ 4\%}          \\
                                                                                                                                                                                                                                                                                                                                                                            \\
\multicolumn{13}{l}{\textbf{Synthetic centromeric sequence - \minimap (n = \numprint{12673})}}                                                                                                                                                                                                                                                                                        \\
HPC                                 & 0.870          & \footnotesize{\;$+$0\%}           & \textbf{1.36e-03} & \textbf{\footnotesize{\;$+$ 0\%}}  & 0.964          & \footnotesize{\;$+$0\%}           & \textbf{1.56e-03} & \textbf{\footnotesize{\;$+$  0\%}} & \textbf{1.000} & \textbf{\footnotesize{\;$+$0\%}} & 9.00e-03          & \footnotesize{\;$+$ 0\%}          \\
raw                                 & \textbf{0.936} & \textbf{\footnotesize{\;$+$8\%}}  & 1.86e-03          & \footnotesize{\;$+$ 36\%}          & \textbf{0.984} & \textbf{\footnotesize{\;$+$2\%}}  & 2.09e-03          & \footnotesize{\;$+$ 34\%}          & \textbf{1.000} & \textbf{\footnotesize{\;$+$0\%}} & \textbf{4.50e-03} & \textbf{\footnotesize{\;$-$50\%}} \\
\msr{E}                             & 0.885          & \footnotesize{\;$+$2\%}           & 3.39e-03          & \footnotesize{\;$+$149\%}          & 0.962          & \footnotesize{\;$-$0\%}           & 3.53e-03          & \footnotesize{\;$+$127\%}          & \textbf{1.000} & \textbf{\footnotesize{\;$+$0\%}} & 1.20e-02          & \footnotesize{\;$+$33\%}          \\
\msr{F}                             & 0.850          & \footnotesize{\;$-$2\%}           & 2.04e-03          & \footnotesize{\;$+$ 50\%}          & 0.968          & \footnotesize{\;$+$0\%}           & 2.12e-03          & \footnotesize{\;$+$ 36\%}          & \textbf{1.000} & \textbf{\footnotesize{\;$+$0\%}} & 6.63e-03          & \footnotesize{\;$-$26\%}          \\
\msr{P}                             & 0.898          & \footnotesize{\;$+$3\%}           & 1.58e-03          & \footnotesize{\;$+$ 16\%}          & 0.968          & \footnotesize{\;$+$0\%}           & 1.79e-03          & \footnotesize{\;$+$ 15\%}          & \textbf{1.000} & \textbf{\footnotesize{\;$+$0\%}} & 9.78e-03          & \footnotesize{\;$+$ 9\%}          \\
                                                                                                                                                                                                                                                                                                                                                                            \\
\multicolumn{13}{l}{\textbf{Synthetic centromeric sequence - \winnowmap (n = \numprint{12673})}}                                                                                                                                                                                                                                                                                      \\
HPC                                 & 0.775          & \footnotesize{\;$+$ 0\%}          & \textbf{1.32e-03} & \textbf{\footnotesize{\;$+$ 0\%}}  & \textbf{0.822} & \textbf{\footnotesize{\;$+$0\%}}  & \textbf{1.82e-03} & \textbf{\footnotesize{\;$+$ 0\%}}  & 0.997          & \footnotesize{\;$+$0\%}          & 8.37e-02          & \footnotesize{\;$+$ 0\%}          \\
raw                                 & \textbf{0.850} & \textbf{\footnotesize{\;$+$10\%}} & 2.04e-03          & \footnotesize{\;$+$54\%}           & 0.890          & \footnotesize{\;$+$8\%}           & 1.95e-03          & \footnotesize{\;$+$ 7\%}           & \textbf{0.999} & \textbf{\footnotesize{\;$+$0\%}} & \textbf{4.60e-02} & \textbf{\footnotesize{\;$-$45\%}} \\
\msr{E}                             & 0.795          & \footnotesize{\;$+$ 2\%}          & 2.28e-03          & \footnotesize{\;$+$73\%}           & 0.846          & \footnotesize{\;$+$3\%}           & 2.52e-03          & \footnotesize{\;$+$38\%}           & 0.997          & \footnotesize{\;$-$0\%}          & 6.96e-02          & \footnotesize{\;$-$17\%}          \\
\msr{F}                             & 0.820          & \footnotesize{\;$+$ 6\%}          & 1.83e-03          & \footnotesize{\;$+$38\%}           & 0.867          & \footnotesize{\;$+$6\%}           & 2.27e-03          & \footnotesize{\;$+$25\%}           & 0.997          & \footnotesize{\;$-$0\%}          & 5.97e-02          & \footnotesize{\;$-$29\%}          \\
\msr{P}                             & 0.780          & \footnotesize{\;$+$ 1\%}          & 1.62e-03          & \footnotesize{\;$+$22\%}           & 0.829          & \footnotesize{\;$+$1\%}           & 2.09e-03          & \footnotesize{\;$+$15\%}           & 0.997          & \footnotesize{\;$-$0\%}          & 8.65e-02          & \footnotesize{\;$+$ 3\%}          \\
                                                                                                                                                                                                                                                                                                                                                                            \\
\multicolumn{13}{l}{\textbf{Whole human genome - \minimap (n = \numprint{655594})}}                                                                                                                                                                                                                                                                                                   \\
HPC                                 & \textbf{0.935} & \textbf{\footnotesize{\;$+$0\%}}  & 1.85e-03          & \footnotesize{\;$+$ 0\%}           & \textbf{0.942} & \textbf{\footnotesize{\;$+$0\%}}  & 1.85e-03          & \footnotesize{\;$+$ 0\%}           & \textbf{1.000} & \textbf{\footnotesize{\;$+$0\%}} & 1.46e-02          & \footnotesize{\;$+$ 0\%}          \\
raw                                 & 0.921          & \footnotesize{\;$-$1\%}           & 1.86e-03          & \footnotesize{\;$+$ 0\%}           & 0.927          & \footnotesize{\;$-$2\%}           & 1.86e-03          & \footnotesize{\;$+$ 1\%}           & 0.998          & \footnotesize{\;$-$0\%}          & \textbf{1.29e-02} & \textbf{\footnotesize{\;$-$11\%}} \\
\msr{E}                             & 0.926          & \footnotesize{\;$-$1\%}           & \textbf{6.92e-05} & \textbf{\footnotesize{\;$-$96\%}}  & 0.936          & \footnotesize{\;$-$1\%}           & \textbf{1.17e-04} & \textbf{\footnotesize{\;$-$94\%}}  & 0.999          & \footnotesize{\;$-$0\%}          & 1.76e-02          & \footnotesize{\;$+$20\%}          \\
\msr{P}                             & 0.929          & \footnotesize{\;$-$1\%}           & 2.20e-04          & \footnotesize{\;$-$88\%}           & 0.938          & \footnotesize{\;$-$0\%}           & 4.15e-04          & \footnotesize{\;$-$78\%}           & 0.999          & \footnotesize{\;$-$0\%}          & 1.55e-02          & \footnotesize{\;$+$ 6\%}          \\
\msr{F}                             & 0.930          & \footnotesize{\;$-$1\%}           & 1.09e-03          & \footnotesize{\;$-$41\%}           & 0.938          & \footnotesize{\;$-$0\%}           & 1.29e-03          & \footnotesize{\;$-$30\%}           & \textbf{1.000} & \textbf{\footnotesize{\;$-$0\%}} & 1.51e-02          & \footnotesize{\;$+$ 4\%}          \\
                                                                                                                                                                                                                                                                                                                                                                            \\
\multicolumn{13}{l}{\textbf{Whole human genome - \winnowmap (n = \numprint{655594})}}                                                                                                                                                                                                                                                                                                 \\
HPC                                 & 0.894          & \footnotesize{\;$+$ 0\%}          & 1.43e-03          & \footnotesize{\;$+$ 0\%}           & 0.902          & \footnotesize{\;$+$0\%}           & 1.49e-03          & \footnotesize{\;$+$ 0\%}           & 0.988          & \footnotesize{\;$+$0\%}          & 1.92e-02          & \footnotesize{\;$+$ 0\%}          \\
raw                                 & \textbf{0.932} & \textbf{\footnotesize{\;$+$ 4\%}} & 1.75e-03          & \footnotesize{\;$+$23\%}           & \textbf{0.937} & \textbf{\footnotesize{\;$+$4\%}}  & 1.79e-03          & \footnotesize{\;$+$20\%}           & \textbf{0.994} & \textbf{\footnotesize{\;$+$1\%}} & \textbf{1.43e-02} & \textbf{\footnotesize{\;$-$26\%}} \\
\msr{F}                             & 0.874          & \footnotesize{\;$-$ 2\%}          & 2.81e-04          & \footnotesize{\;$-$80\%}           & 0.886          & \footnotesize{\;$-$2\%}           & 3.82e-04          & \footnotesize{\;$-$74\%}           & 0.984          & \footnotesize{\;$-$0\%}          & 1.94e-02          & \footnotesize{\;$+$ 1\%}          \\
\msr{E}                             & 0.795          & \footnotesize{\;$-$11\%}          & \textbf{6.33e-05} & \textbf{\footnotesize{\;$-$96\%}}  & 0.820          & \footnotesize{\;$-$9\%}           & \textbf{8.93e-05} & \textbf{\footnotesize{\;$-$94\%}}  & 0.971          & \footnotesize{\;$-$2\%}          & 2.08e-02          & \footnotesize{\;$+$ 9\%}          \\
\msr{P}                             & 0.826          & \footnotesize{\;$-$ 8\%}          & 8.68e-05          & \footnotesize{\;$-$94\%}           & 0.845          & \footnotesize{\;$-$6\%}           & 1.14e-04          & \footnotesize{\;$-$92\%}           & 0.975          & \footnotesize{\;$-$1\%}          & 2.11e-02          & \footnotesize{\;$+$10\%}          \\
\\
\multicolumn{13}{l}{\textbf{Whole Human genome (repeated regions) - \minimap (n = \numprint{68811})}}                                                                                                                                                                                                                                                                                 \\
HPC                                 & \textbf{0.619} & \textbf{\footnotesize{\;+ 0\%}}   & 3.29e-04          & \footnotesize{\;+ 0\%}             & 0.656          & \footnotesize{\;+ 0\%}            & 3.10e-04          & \footnotesize{\;+ 0\%}             & \textbf{0.998} & \textbf{\footnotesize{\;+0\%}}   & 7.79e-02          & \footnotesize{\;+ 0\%}            \\
raw                                 & 0.514          & \footnotesize{\;-17\%}            & 1.98e-04          & \footnotesize{\;-40\%}             & 0.539          & \footnotesize{\;-18\%}            & 2.16e-04          & \footnotesize{\;-30\%}             & 0.981          & \footnotesize{\;-2\%}            & \textbf{6.69e-02} & \textbf{\footnotesize{\;-14\%}}   \\
\msr{F}                             & 0.601          & \footnotesize{\;- 3\%}            & 2.18e-04          & \footnotesize{\;-34\%}             & 0.640          & \footnotesize{\;- 2\%}            & 2.27e-04          & \footnotesize{\;-27\%}             & \textbf{0.998} & \textbf{\footnotesize{\;-0\%}}   & 8.15e-02          & \footnotesize{\;+ 5\%}            \\
\msr{E}                             & 0.618          & \footnotesize{\;- 0\%}            & 1.41e-04          & \footnotesize{\;-57\%}             & \textbf{0.658} & \textbf{\footnotesize{\;+ 0\%}}   & \textbf{1.55e-04} & \textbf{\footnotesize{\;-50\%}}    & 0.997          & \footnotesize{\;-0\%}            & 8.23e-02          & \footnotesize{\;+ 6\%}            \\
\msr{P}                             & 0.616          & \footnotesize{\;- 1\%}            & \textbf{1.18e-04} & \textbf{\footnotesize{\;-64\%}}    & 0.656          & \footnotesize{\;+ 0\%}            & 1.99e-04          & \footnotesize{\;-36\%}             & 0.997          & \footnotesize{\;-0\%}            & 8.31e-02          & \footnotesize{\;+ 7\%}            \\
                                                                                                                                                                                                                                                                                                                                                                            \\
\multicolumn{13}{l}{\textbf{Whole Human genome (repeated regions) - \winnowmap (n = \numprint{68811})}}                                                                                                                                                                                                                                                                               \\
HPC                                 & 0.525          & \footnotesize{\;+ 0\%}            & 1.24e-03          & \footnotesize{\;+ 0\%}             & 0.557          & \footnotesize{\;+ 0\%}            & 1.49e-03          & \footnotesize{\;+ 0\%}             & 0.950          & \footnotesize{\;+0\%}            & 1.19e-01          & \footnotesize{\;+ 0\%}            \\
raw                                 & \textbf{0.648} & \textbf{\footnotesize{\;+23\%}}   & 1.26e-03          & \footnotesize{\;+ 1\%}             & \textbf{0.672} & \textbf{\footnotesize{\;+21\%}}   & 1.49e-03          & \footnotesize{\;+ 0\%}             & \textbf{0.968} & \textbf{\footnotesize{\;+2\%}}   & \textbf{8.09e-02} & \textbf{\footnotesize{\;-32\%}}   \\ 
\msr{F}                             & 0.482          & \footnotesize{\;- 8\%}            & \textbf{1.63e-03} & \textbf{\footnotesize{\;+31\%}}    & 0.516          & \footnotesize{\;- 7\%}            & 1.83e-03          & \footnotesize{\;+23\%}             & 0.940          & \footnotesize{\;-1\%}            & 1.21e-01          & \footnotesize{\;+ 2\%}            \\
\msr{E}                             & 0.366          & \footnotesize{\;-30\%}            & 6.35e-04          & \footnotesize{\;-49\%}             & 0.405          & \footnotesize{\;-27\%}            & \textbf{9.32e-04} & \textbf{\footnotesize{\;-37\%}}    & 0.911          & \footnotesize{\;-4\%}            & 1.38e-01          & \footnotesize{\;+17\%}            \\
\msr{P}                             & 0.415          & \footnotesize{\;-21\%}            & 9.45e-04          & \footnotesize{\;-24\%}             & 0.451          & \footnotesize{\;-19\%}            & 1.16e-03          & \footnotesize{\;-22\%}             & 0.920          & \footnotesize{\;-3\%}            & 1.39e-01          & \footnotesize{\;+17\%}            \\ \bottomrule
\end{tabular}%
}
}
\end{table}

\hypertarget{analyzing-read-origin-on-whole-human-genome}{%
\section{Analyzing read origin on whole human genome}\label{analyzing-read-origin-on-whole-human-genome}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/HPC-MSRs/raw_distrib.centro.rstart.pdf}\\
    \extcaption{Origin of correctly and incorrectly mapped raw reads}{
    Distribution of the origin of correctly and incorrectly mapped simulated reads (in teal and red respectively) on the different chromosomes of the whole human genome. The dark grey rectangle for each chromosome represents the centromere of that chromosome. The lighter gray rectangle on chromosomes 13, 14, 15, 21 and 22 correspond to satellites denoted as ``stalk'', another repetitive region.}
    \label{fig:hist-raw}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/HPC-MSRs/hpc_distrib.centro.pdf}
    \extcaption{Origin of correctly (teal) and incorrectly (red) mapped reads, transformed with HPC}{Distribution of the origin of correctly and incorrectly mapped simulated reads (in teal and red respectively) on the different chromosomes of the whole human genome. The dark grey rectangle for each chromosome represents the centromere of that chromosome. The lighter gray rectangle on chromosomes 13, 14, 15, 21 and 22 correspond to satellites denoted as ``stalk'', another repetitive region.}
    \label{fig:hist-hpc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/HPC-MSRs/msr_e_distrib.centro.pdf}
    \extcaption{Origin of correctly (teal) and incorrectly (red) mapped reads, transformed with \msr{E}}{Distribution of the origin of correctly and incorrectly mapped simulated reads (in teal and red respectively) on the different chromosomes of the whole human genome. The dark grey rectangle for each chromosome represents the centromere of that chromosome. The lighter gray rectangle on chromosomes 13, 14, 15, 21 and 22 correspond to satellites denoted as ``stalk'', another repetitive region.}
    \label{fig:hist-msr-e}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/HPC-MSRs/msr_p_distrib.centro.pdf}
    \extcaption{Origin of correctly (teal) and incorrectly (red) mapped reads, transformed with \msr{P}}{Distribution of the origin of correctly and incorrectly mapped simulated reads (in teal and red respectively) on the different chromosomes of the whole human genome. The dark grey rectangle for each chromosome represents the centromere of that chromosome. The lighter gray rectangle on chromosomes 13, 14, 15, 21 and 22 correspond to satellites denoted as ``stalk'', another repetitive region.}
    \label{fig:hist-msr-p}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/HPC-MSRs/msr_f2_distrib.centro.pdf}
    \extcaption{Origin of correctly (teal) and incorrectly (red) mapped reads, transformed with \msr{F}}{Distribution of the origin of correctly and incorrectly mapped simulated reads (in teal and red respectively) on the different chromosomes of the whole human genome. The dark grey rectangle for each chromosome represents the centromere of that chromosome. The lighter gray rectangle on chromosomes 13, 14, 15, 21 and 22 correspond to satellites denoted as ``stalk'', another repetitive region.}
    \label{fig:hist-msr-f}
\end{figure}

\hypertarget{performance-of-msrs-on-the-drosophila-genome}{%
\section{Performance of MSRs on the Drosophila genome}\label{performance-of-msrs-on-the-drosophila-genome}}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/HPC-MSRs/droso_ecoli.pdf}
    \extcaption{Results of the \texttt{paftools mapeval} evaluation on reads simulated and mapped to whole \textit{Drosophila melanogaster} and \textit{Escherichia coli} (Genbank ID \href{https://www.ncbi.nlm.nih.gov/nuccore/U00096.2}{U00096.2}) genomes.}{MSRs E, F and P are shown in different shades of blue to differentiate them from other MSRs. Reads were simulated with \texttt{nanosim}, and mapped with \minimap.}
    \label{fig:drosophila-results}
\end{figure}
\newpage

\hypertarget{key-resource-table}{%
\section{Key Resource Table}\label{key-resource-table}}

\begin{table}[H]
\small
\centering
\begin{tabular}{p{0.3\textwidth}p{0.2\textwidth}p{0.5\textwidth}}
\toprule
REAGENT or RESOURCE  & SOURCE                 & IDENTIFIER                             \\ \midrule
\multicolumn{3}{l}{\textbf{Deposited Data}}                                                     \\ \midrule
T2T CHM13 v1.1, whole human genome assembly &
  (Nurk et al., 2022) &
  Genbank accession number \href{https://www.ncbi.nlm.nih.gov/assembly/GCA_009914755.3/}{GCA\_009914755.3} \\ \addlinespace
Release 6 plus ISO1 MT, whole drosophila melanogaster genome assembly &
  (Adams et al., 2000) &
  Genbank accession number \href{https://www.ncbi.nlm.nih.gov/assembly/GCA_000001215.4/}{GCA\_000001215.4} \\ \addlinespace
Synthetic centrormeric sequence &
  (Mikheenko et al., 2020) &
  \url{https://github.com/ablab/TandemTools/blob/master/test\_data/simulated\_del.fasta} \\ \addlinespace
Escherichia coli str. K-12 substr. MG1655, complete genome &
  (Blattner et al., 1997) &
  Genbank accession number \href{https://www.ncbi.nlm.nih.gov/nuccore/U00096.2}{U00096.2} \\ \addlinespace
Coordinates of repeated regions of the CHM13 whole genome assembly &
  Telomere to Telomere consortium &
  \url{https://t2t.gi.ucsc.edu/chm13/hub/t2t-chm13-v1.1/rmsk/rmsk.bigBed} \\ \addlinespace
\multicolumn{3}{l}{\textbf{Software and Algorithms}}                                            \\ \midrule
minimap2 v2.22-r1101 & (Li, 2018)             & \url{https://github.com/lh3/minimap2}        \\
Winnowmap v2.0       & (Jain et al., 2020)    & \url{https://github.com/marbl/Winnowmap}     \\
NanoSim v3.0.0       & (Yang et al., 2017)    & \url{https://github.com/bcgsc/NanoSim}       \\
Bedtools v2.30.0     & (Quinlan et al., 2010) & \url{https://github.com/arq5x/bedtools2}     \\
Meryl v1.0           & (Rhie et al., 2020)    & \url{https://github.com/marbl/Winnowmap}     \\
Analysis pipelines   & This paper             & \url{https://doi.org/10.5281/zenodo.6859636} \\ \bottomrule
\end{tabular}
\end{table}

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for appendix \thechapter}]

\hypertarget{HIV-intro-appendix}{%
\chapter{Supporting Information for ``HIV and DRMs''}\label{HIV-intro-appendix}}

\hypertarget{detailed-list-of-hiv-1-protein-structures-used-for-figure-generation.}{%
\section{Detailed list of HIV-1 protein structures used for figure generation.}\label{detailed-list-of-hiv-1-protein-structures-used-for-figure-generation.}}

The images for HIV-1 structures used in Figure \ref{fig:hivStructure} were obtained from: \url{https://cdn.rcsb.org/pdb101/learn/resources/structural-biology-of-hiv/}. They are licensed under a Creative Commons By 4.0 license which allows reuse and adaptation for non commercial use.

PDB structure IDs:

\begin{itemize}
\item
  \textbf{SU} and \textbf{TM}: \href{http://www.rcsb.org/pdb/explore.do?structureId=4nco}{4nco}
\item
  \textbf{MA}: \href{http://rcsb.org/pdb/explore/explore.do?structureId=1hiw}{1hiw}
\item
  \textbf{CA}: \href{http://rcsb.org/pdb/explore/explore.do?structureId=3h47}{3h47}
\item
  \textbf{NC}: \href{http://rcsb.org/pdb/explore/explore.do?structureId=1a1t}{1a1t}
\item
  \textbf{RT}: \href{http://www.rcsb.org/pdb/explore.do?structureId=1hys}{1hys} (for Figure \ref{fig:hivStructure}) and \href{http://www.rcsb.org/pdb/explore.do?structureId=2hmi}{2hmi} (for Figure \ref{fig:rtStruct})
\item
  \textbf{IN}: \href{http://www.rcsb.org/pdb/explore.do?structureId=1ex4}{1ex4}
\item
  \textbf{PR}: \href{http://www.rcsb.org/pdb/explore.do?structureId=1hpv}{1hpv}
\item
  \textbf{Vpu}: \href{http://www.rcsb.org/pdb/explore.do?structureId=1pi7}{1pi7} and \href{http://www.rcsb.org/pdb/explore.do?structureId=1vpu}{1vpu}
\item
  \textbf{Vif}: \href{http://www.rcsb.org/pdb/explore.do?structureId=3dcg}{3dcg}
\item
  \textbf{Vpr}: \href{http://www.rcsb.org/pdb/explore.do?structureId=1esx}{1esx}
\item
  \textbf{Nef}: \href{http://www.rcsb.org/pdb/explore.do?structureId=1avv}{1avv} and \href{http://www.rcsb.org/pdb/explore.do?structureId=1qa5}{1qa5}
\item
  \textbf{Rev}: \href{http://www.rcsb.org/pdb/explore.do?structureId=1etf}{1etf}
\item
  \textbf{Tat}: \href{http://www.rcsb.org/pdb/explore.do?structureId=1biv}{1biv} and \href{http://www.rcsb.org/pdb/explore.do?structureId=1jfw}{1jfw}
\end{itemize}

\hypertarget{list-of-all-antiretroviral-drugs}{%
\section{List of all antiretroviral drugs}\label{list-of-all-antiretroviral-drugs}}

\begin{longtable}{lllll}
        \toprule
        Name & Brand name & Abbreviation & Class & Approval date \\ \midrule
        zidovudine & retrovir & ZDV & NRTI & 1987-03-19 \\ 
        didanosine\textsuperscript{\dag} & videx & ddI & NRTI & 1991-10-09 \\ 
        zalcitabine\textsuperscript{\dag} & hivid & ddC & NRTI & 1992-06-19 \\ 
        stavudine\textsuperscript{\dag} & zerit & d4T & NRTI & 1994-06-24 \\ 
        lamivudine & epivir & 3TC & NRTI & 1995-11-17 \\ 
        saquinavir & invirase & SQV & PI & 1995-12-06 \\ 
        ritonavir\textsuperscript{*} & norvir & RTV & PI & 1996-03-01 \\ 
        indinavir\textsuperscript{\dag} & crixivan & IDV & PI & 1996-03-13 \\ 
        neviparine & viramune & NVP & NNRTI & 1996-06-21 \\ 
        nelfinavir\textsuperscript{\dag} & viracept & NFV & PI & 1997-03-14 \\ 
        delavirdine\textsuperscript{\dag} & rescriptor & DLV & NNRTI & 1997-04-04 \\ 
        combivir & combivir & 3TC+ZDV & FDC & 1997-09-27 \\ 
        efavirenz & sustiva & EFV & NNRTI & 1998-09-17 \\ 
        abacavir & ziagen & ABC & NRTI & 1998-12-17 \\ 
        amprenavir\textsuperscript{\dag} & agenerase & APV & PI & 1999-04-15 \\ 
        kaletra & kaletra & LPV+RTV & FDC & 2000-09-15 \\ 
        didanosine-ec\textsuperscript{\dag} & videx-ec & ddI-EC & NRTI & 2000-10-31 \\ 
        trizivir & trizivir & ABC+3TC+ZDV & FDC & 2000-11-14 \\ 
        tenofovir-df & viread & TDF & NRTI & 2001-10-26 \\ 
        enfuvirtide & fuzeon & T-20 & FI & 2003-03-13 \\ 
        atazanavir & reyataz & ATC & PI & 2003-06-20 \\ 
        emtricitabine & emtriva & FTC & NRTI & 2003-07-02 \\ 
        fosamprenavir & lexiva & FPV & PI & 2003-10-20 \\ 
        epzicom & epzicom & ABC+3TC & FDC & 2004-08-02 \\ 
        truvada & truvada & FTC+TDF & FDC & 2004-08-02 \\ 
        tipranavir & aptivus & TPV & PI & 2005-06-22 \\ 
        darunavir & prezista & DRV & PI & 2006-06-23 \\ 
        atripla & atripla & EFV+FTC+TDF & FDC & 2006-07-12 \\ 
        maraviroc & selzentry & MVC & CA & 2007-08-06 \\ 
        raltegravir & isentress & RAL & INSTI & 2007-10-12 \\ 
        etravirine & intelence & ETR & NNRTI & 2008-01-18 \\ 
        neviparine-xr & viramune-xr & NVP-XR & NNRTI & 2011-03-25 \\ 
        rilpivirine & edurant & RPV & NNRTI & 2011-05-20 \\ 
        complera & complera & FTC+RPV+TDF & FDC & 2011-08-10 \\ 
        stribild & stribild & EVG+COBI+FTC+TDF & FDC & 2012-08-27 \\ 
        dolutegravir & tivicay & DTG & INSTI & 2013-08-12 \\ 
        triumeq & triumeq & ABC+DTG+3TC & FDC & 2014-08-22 \\ 
        elvitegravir\textsuperscript{\dag} & vitekta & EVG & INSTI & 2014-09-14 \\ 
        cobicistat & tybost & COBI & PE & 2014-09-24 \\ 
        evotaz & evotaz & ATV+COBI & FDC & 2015-01-29 \\ 
        prezcobix & prezcobix & DRV+COBI & FDC & 2015-01-29 \\ 
        genvoya & genvoya & EVG+COBI+FTC+TAF & FDC & 2015-11-05 \\ 
        odefsey & odefsey & FTC+RPV+TAF & FDC & 2016-03-01 \\ 
        descovy & descovy & FTC+TAF & FDC & 2016-04-04 \\ 
        raltegravir & isentress-hd & RAL & INSTI & 2017-05-26 \\ 
        juluca & juluca & DTG+RPV & FDC & 2017-11-21 \\ 
        symfi-lo & symfi-lo & EFV+3TC+TDF & FDC & 2018-02-05 \\ 
        biktarvy & biktarvy & BIC+FTC+TAF & FDC & 2018-02-07 \\ 
        cimduo & cimduo & 3TC+TDF & FDC & 2018-02-28 \\ 
        ibalizumab-uiyk & trogarzo & TNX-355 & PAI & 2018-03-06 \\ 
        symfi & symfi & EFV+3TC+TDF & FDC & 2018-03-22 \\ 
        symtuza & symtuza & DRV+COBI+FTC+TAF & FDC & 2018-07-17 \\ 
        delstrigo & delstrigo & DOR+3TC+TDF & FDC & 2018-08-30 \\ 
        doravirine & pifeltro & DOR & NNRTI & 2018-08-30 \\ 
        temixys & temixys & 3TC+TDF & FDC & 2018-11-16 \\ 
        dovato & dovato & DTG+3TC & FDC & 2019-04-08 \\ 
        dolutegravir & tivicay-pd & DTG & INSTI & 2020-06-12 \\ 
        fostemsavir & rukobia & FTR & AI & 2020-07-02 \\ 
        cabenuva & cabenuva & CAB+RPV & FDC & 2021-01-22 \\ 
        cabotegravir & vocabria & CAB & INSTI & 2021-01-22 \\ \bottomrule
    \extcaption{List of all antiretroviral drugs used in HIV therapy}{
Zidovudine (ZDV) is also referred to as Azidothymidine (AZT) in litterature, Fixed Dose combinations (i.e. single pills combining multiple drugs) are referred to by their commercial name, the composition of these can be seen in the abbreviation. Drugs were ordered by FDA approval date.\\    
\textbf{AI}: Attachment Inhibitor,
\textbf{CA}: CCR5 Antagonist,
\textbf{FDC}: Fixed Dose Combination,
\textbf{FI}: Fusion Inhibitor,
\textbf{INSTI}: Integrase Inhibitor
\textbf{NNRTI}: Non-Nucleoside Reverse Transcriptase Inhibitor, 
\textbf{NRTI}: Nucleoside Reverse Transcriptase Inhibitor,
\textbf{PE}: Pharmacokinetic Enhancer,
\textbf{PAI}: Post-Attachment Inhibitor,  
\textbf{PI}: Protease Inhibitor. 
\textbf{AI}, \textbf{CA}, \textbf{FI} and \textbf{PAI} can be grouped in a class of Entry inhibitors.\\
* Although Ritonavir is originally a PI it is now mainly used as a PE to boost the action of other drugs.\\
\dag These drugs are no longer available or recommended in HIV treatment guidelines. They may still be used in FDC regimens.\\
Adapted from \url{https://hivinfo.nih.gov/understanding-hiv/infographics/fda-approval-hiv-medicines} and \url{https://hivinfo.nih.gov/understanding-hiv/fact-sheets/fda-approved-hiv-medicines}
}
\label{tab:tableDrugs}
\end{longtable}

\hypertarget{HIV-appendix}{%
\chapter{Supporting Information for ``Using Machine Learning and Big Data to Explore the Drug Resistance Landscape in HIV''}\label{HIV-appendix}}

\hypertarget{S1-Appendix}{%
\section{S1 Appendix (Technical appendix).}\label{S1-Appendix}}

\hypertarget{data-appendix}{%
\subsection{Data}\label{data-appendix}}

\hypertarget{data-availability}{%
\subsubsection{Data Availability}\label{data-availability}}

The policy of the UK HIV Drug Resistance Database is to make DNA sequences available to any bona fide researcher who submits a scientifically robust proposal, provided data exchange complies with Information Governance and Data Security Policies in all the relevant countries. This includes replication of findings from published studies, although the researcher would be encouraged to work with the main author of the published paper to understand the nuances of the data. Enquiries should be addressed to \href{mailto:iph.hivrdb@ucl.ac.uk}{\nolinkurl{iph.hivrdb@ucl.ac.uk}} in the first instance. More information on the UK dataset is also available on the UK CHIC homepage: \href{http://www.ukchic.org.uk/}{www.ukchic.org.uk}. Amino acid sequences are made available along with a metadata file.\\
The West and central African dataset is available as supplementary information along with a metadata file containing HIV subtype, treatment information and known RAM presence/absence for each sequence.\\
Predictions made for each sequence of both datasets, by all of the trained classifiers are made available as part of the supplementary data as well as synthetic results from which the figures of the paper were drawn. The importance values for each mutation and each trained classifier are also made available.\\
All the data and metadata files made available are hosted in the online repository linked to this project at the following URL:\\
\href{https://github.com/lucblassel/HIV-DRM-machine-learning/tree/main/data}{github.com/lucblassel/HIV-DRM-machine-learning/tree/main/data}

\hypertarget{data-preprocessing}{%
\subsubsection{Data Preprocessing}\label{data-preprocessing}}

For both the African and UK datasets, the sequences were truncated to keep sites 41 to 235 of the RT protein sequence before encoding. This truncation was needed to avoid the perturbation to classifier training due to long gappy regions at the beginning and end of the UK RT alignment caused by shorter sequences. These positions were determined with the Gblocks software \autocite{castresanaSelectionConservedBlocks2000} with default parameters, except for the Maximum number of sequences for a flanking position, set to 50,000, and the Allowed gap positions, which was set to ``All''. The encoding was done with the \texttt{OneHotEncoder} from the category-encoders python module \autocite{mcginnisScikitLearnContribCategoricalEncodingRelease2018}.

\hypertarget{classifiers}{%
\subsection{Classifiers}\label{classifiers}}

We used classifier implementations from the scikit-learn python library \autocite{pedregosaScikitlearnMachineLearning2011}, \texttt{RandomForestClassifier} for the random forest classifier, \texttt{MultinomialNB} for Naïve Bayes and \texttt{LogisticRegressionCV} for logistic regression.\\
\texttt{RandomForestClassifier} was used with default parameters except:

\begin{itemize}
\tightlist
\item
  \texttt{"n\_jobs"=4}
\item
  \texttt{"n\_estimators"=5000}
\end{itemize}

\texttt{LogisticRegressionCV} was used with the following parameters:

\begin{itemize}
\tightlist
\item
  \texttt{"n\_jobs"=4}
\item
  \texttt{"cv"=10}
\item
  \texttt{"Cs"=100}
\item
  \texttt{"penalty"=’l1’}
\item
  \texttt{"multi\_class"=’multinomial’}
\item
  \texttt{"solver"=’saga’}
\item
  \texttt{"scoring"=’balanced\_accuracy’}
\end{itemize}

\texttt{MultinomialNB} was used with default parameters.

For the Fisher exact tests, we used the implementation from the scipy python library \autocite{virtanenSciPyFundamentalAlgorithms2020}, and corrected p-values for multiple testing with the statsmodels python library \autocite{seaboldStatsmodelsEconometricStatistical2010} using the \texttt{"Bonferroni"} method.

\hypertarget{scoring}{%
\subsection{Scoring}\label{scoring}}

To evaluate classifier performance several measures were used. We computed balanced accuracy instead of classical accuracy, because it can be overly optimistic, especially when assessing a highly biased classifier on an unbalanced test set \autocite{brodersenBalancedAccuracyIts2010}.The balanced accuracy is computed using the following formula, where \(TP\) and \(TN\) are the number of true positives and true negatives respectively, and \(FP\) and \(FN\) are the number of false positives and false negatives respectively:\\
\[
balanced~accuracy = \frac{1}{2}\left(
      \frac{TP}{TP + FP} + \frac{TN}{TN + FN}
  \right)
\]

We also computed adjusted mutual information (AMI). We chose it over mutual information (MI) because it has an upper bound of 1 for a perfect classifier and is not dependent on the size of the test set, allowing us to compare the performance for differently sized test sets \autocite{vinhInformationTheoreticMeasures2010}. The adjusted mutual information of variables \(U\) and \(V\) is defined by the following formula, where \(MI(U,V)\) is the mutual information between variables \(U\) and \(V\), \(H(X)\) is the entropy of the variable \(X\) (= \(U\) or \(V\)) and \(E\{MI(U,V)\}\) is the expected MI, as explained in \autocite{vinhNovelApproachAutomatic2009}.\\
\[
AMI(U,V) = \frac{
      MI(U,V) - E\{MI(U,V)\} 
  }{
      \frac{1}{2}[H(U) + H(V)] - E\{MI(U,V)\}
  }
\]

MI was used to compute the \(G\) statistic, which follows the chi-square distribution under the null hypothesis \autocite{harremoesMutualInformationContingency2014}. This was used to compute p-values for each of our classifiers and assess the significance of their performance. \(G\) is defined by equation below, where \(N\) is the number of samples.\\
\[G = 2\cdot N \cdot MI(U,V)\]

Finally, to check the probabilistic predictive power of the classifiers we also computed the Brier score which is the mean squared difference between the ground truth and the predicted probability of being of the positive class for every sequence in the test set (therefore lower is better for this metric). The Brier score is defined in equation below, where \(p_t\) is the predicted probability of being of the positive class for sample \(t\) and \(o_t\) is the actual class (0 or 1, 1=positive class) of sample \(t\):\\
\[Brier~score=\frac{1}{N}\sum_{t=1}^N(p_t-o_t)^2\]

We used the following implementations from the scikit-learn python library \autocite{pedregosaScikitlearnMachineLearning2011} with default options:

\begin{itemize}
\tightlist
\item
  \texttt{balanced\_accuracy\_score}
\item
  \texttt{mutual\_info\_score}
\item
  \texttt{adjusted\_mutual\_info\_score}
\item
  \texttt{brier\_score\_loss}
\end{itemize}

We used the relative risk to observe the relationship between one of our new mutations and a binary character \(X\) such as treatment status or presence/absence of a known RAM. \[
\begin{aligned}
  RR(new, X) &= \frac{prevalence\left(new~mutation\mid X=1\right)}{prevalence\left(new~mutation\mid X=0\right)} \nonumber\\
  \nonumber\\
  &= \frac{|(new=1)\cap(X=1)|}{|(X=1)|}\div\frac{|(new=1)\cap(X=0)|}{|(X=0)|} \\
  \end{aligned}
\] \newpage

\hypertarget{s1-fig.}{%
\section{S1 Fig.}\label{s1-fig.}}

\begin{figure}[H]
{
\centering
\includegraphics[width=0.9\linewidth]{./figures/HIV-DRMs/S1_Fig.png}
}
\extcaption{Relative risks of the new mutations with regards to known RAMs on the African dataset}{
(i.e. the prevalence of the new mutation in sequences with a given RAM divided by the prevalence of the new mutation in sequences without the RAM). RRs were only computed for mutations (new and RAMs) that appeared in at least 30 sequences, which is why RRs were not computed for H208Y and D218E. 95\% confidence intervals, represented by vertical bars, were computed with 1000 bootstrap samples of the African sequences. Only RRs with a lower CI boundary greater than 2 are shown. The shape and color of the point represents the type of RAM as defined by Stanford’s HIVDB. Blue circle: NRTI, orange square: NNRTI, green diamond: Other. For the RR of L228H with regards to M184V, the upper CI bound is infinite. The new RAMs have high RR values for known RAMs similar to those obtained on the UK dataset. We also arrive at similar conclusions, I135L being associated with NNRTIs, E203K and L228H to NRTI and L228R to both. RR values are shown from left to right, by order of decreasing values on the lower bound of the 95\% CI.}
\label{fig:s1}
\end{figure}
\newpage

\hypertarget{s2-fig.}{%
\section{S2 Fig.}\label{s2-fig.}}

\begin{figure}[H]
{
\centering
\includegraphics[width=0.9\linewidth]{./figures/HIV-DRMs/S2_Fig.png}
}
\extcaption{Closeup structural view of the entrance of the NNIBP of HIV-1 RT}{
The p66 subunit is  colored in dark gray, the p51 subunit in light gray. The NNIBP is highlighted in yellow. The active site is colored in blue. We can see the physical proximity of I135 (red) to the entrance of the NNIBP. We can also see how L228 (red) is between 2 AAs of the NNIBP.}
\label{fig:s2}
\end{figure}
\newpage

\hypertarget{s3-fig.}{%
\section{S3 Fig.}\label{s3-fig.}}

\begin{figure}[H]
{
\centering
\includegraphics[width=0.9\linewidth]{./figures/HIV-DRMs/S3_Fig.png}
}
\extcaption{Closeup structural view of the active site of HIV-1 RT.}{
The p66 subunit is colored in dark gray, the p51 subunit in light gray. The active site is highlighted in blue. The NNIBP is colored in yellow. L228, E203 and D218 (red) are also very close on either side of the active site.}
\label{fig:s3}
\end{figure}
\newpage

\hypertarget{S1-Table}{%
\section{S1 Table.}\label{S1-Table}}

\begin{landscape}
\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}lllllll@{}ll@{}ll@{}ll@{}ll@{}ll@{}llllllll@{}} 
\toprule
\multirow{3}{*}{} & \multicolumn{2}{l}{\multirow{2}{*}{rank}} & \multicolumn{3}{l}{\multirow{2}{*}{codon distance}} & \multicolumn{6}{l}{UK} & \multicolumn{7}{l}{Africa} & \multirow{3}{*}{B62} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Dayhoff \\ category \\ shift \end{tabular}} & \multicolumn{4}{l}{Change in} \\
 & \multicolumn{2}{l}{} & \multicolumn{3}{l}{} & \multicolumn{2}{l}{\multirow{2}{*}{count}} & \multicolumn{4}{l}{ratio} & \multicolumn{2}{l}{\multirow{2}{*}{count}} & \multicolumn{4}{l}{ratio} & \multirow{2}{*}{p-value} &  &  & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}net \\ charge \end{tabular}} & \multirow{2}{*}{polarity} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}hydrophobicity \\ index \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}molecular \\ weight \end{tabular}} \\
 & T/N & W/W & min & UK & Africa & \multicolumn{2}{l}{} & \multicolumn{2}{l}{$\rho(new, treatment)$ } & \multicolumn{2}{l}{$\rho(new, with~RAM)$ ~} & \multicolumn{2}{l}{} & \multicolumn{2}{l}{$\rho(new, treatment)$~~ } & \multicolumn{2}{l}{$\rho(new, with~RAM)$~~ } &  &  &  &  &  &  &  \\ 
\midrule
 \textbf{L228R}  & 0 & 0 & 1 & 1.16 & 1.21 & 227~ & (0.4\%) & 18.1~ & {[}12.9;27.3] & 115.7~ & {[}55.1;507.3] & 98~ & (2.5\%) & 32.5 & {[}15.4;147.1] & 42.4~ & {[}17.8;$\infty$] & 2.0E-30 & -2 & e $\rightarrow$ d  & 1 & 5.6 & -0.93 & 43.03 \\
\textbf{E203K}  & 1 & 1 & 1 & 1.31 & 1.33 & 256~ & (0.5\%) & 11.0~ & {[}8.2;15.1] & 20.1~ & {[}13.7;32.1] & 56~ & (1.4\%) & 14.1 & {[}6.7;71.9] & 17.4 & {[}8.2;83.7] & 6.4E-14 & 1 & c $\rightarrow$ d  & 2 & -1 & 0.68 & -0.94 \\
\textbf{D218E}  & 2 & 3 & 1 & 1 & 1 & 168~ & (0.3\%) & 13.1~ & {[}9.0;19.6] & 27.0~ & {[}16.3;57.0] & 25~ & (0.6\%) & $\infty$~ & {[}$\infty$;$\infty$] & $\infty$ & {[}$\infty$;$\infty$]~ & 2.0E-09 & 2 & c $\rightarrow$ c  & 0 & -0.7 & 0.01 & 14.03 \\
\textbf{L228H}  & 3 & 4 & 1 & 1.12 & 1.17 & 287 & (0.5\%) & 6.4~ & {[}5.1;8.4] & 9.2~ & {[}6.9;12.6] & 53~ & (1.3\%) & 23.1 & {[}9.4;$\infty$] & 34.1 & {[}12.0;$\infty$] & 2.7E-15 & -3 & e $\rightarrow$ d  & 0 & 5.5 & -0.92 & 23.99 \\
\textbf{I135L}  & 4 & 6 & 1 & 1.16 & 1.13 & 540 & (1.0\%) & 1.8~ & {[}1.5;2.1] & 2.4 & {[}2.0;2.8] & 134 & (3.4\%) & 2.6~ & {[}1.8;3.8] & 2.4 & {[}1.7;3.4] & 2.6E-07 & 2 & e $\rightarrow$ e  & 0 & -0.3 & -0.69 & 0 \\
\textbf{H208Y}  & 8 & 9 & 1 & 1.10 & 1.12 & 205~ & (0.4\%) & 8.8~ & {[}6.5;12.5] & 14.9 & {[}9.9;23.6] & 13~ & (0.3\%) & $\infty$~~ & {[}$\infty$;$\infty$] & $\infty$ & {[}$\infty$;$\infty$]~ & 7.3E-05 & 2 & d $\rightarrow$ f  & 0 & -4.2 & 1.27 & 26.03 \\
\bottomrule
\end{tabular}
}
 
  \extcaption{Detailed view of the characteristics of new potential RAMs}{\textbf{Rank:} For each new mutation we computed the aggregate feature importance ranks for the RTI-naive / RTI-experienced and known RAM present / known RAM absent classification tasks.
  \textbf{Codon distance:} We computed the minimum number of nucleotide mutations to go from the wild amino acid codons to those of the mutated amino acid, as well as the average codon distance between both amino acids, weighted by the prevalence of each wild and mutated codon in the UK and the African datasets. 
  \textbf{Count (both UK and Africa):} We looked at the number of apparitions of each new potential RAM in the UK and African datasets and the corresponding prevalence in parentheses. 
  \textbf{Ratio (both UK and Africa):} We computed the prevalence ratio $\rho(new, treatment)$ (e.g. L228R is 18.1 times more prevalent in RTI-experienced sequences compared to RTI-naive sequences in the UK dataset). We also computed the prevalence ratio $\rho(new,any RAM)$ (e.g. L228R is 115.7 times more prevalent in sequences that have at least one known RAM than in sequences that have none in the UK dataset). The 95\% confidence intervals shown under each ratio were computed with 1000 bootstrap samples of size $n=55,000$ drawn with replacement from the whole UK dataset (The same procedure was done on the African dataset with size $n=3990$). 
  \textbf{p-values:} Fisher exact tests were done on the African dataset to see if each of these new mutations were more prevalent in RTI-experienced sequences; p-value were corrected with the Bonferroni method for the six simultaneous tests.
  \textbf{B62:} BLOSUM62 similarity values (e.g. D218E = 2, reflecting that E and D are both negatively charged and highly similar). 
  \textbf{Dayhoff category shift:} The change in Dayhoff amino acid category is written thusly: “starting category $\rightarrow$ ending category”. These categories are as follows: \textit{a:} Sulfur polymerization. \textit{b:} Small, \textit{c:} Acid and amide, \textit{d:} Basic, \textit{e:} Hydrophobic and \textit{f:} aromatic.
  \textbf{Physico-chemical change:} Change in physicochemical properties was obtained by subtracting the property value of the wil-type amino acid from the mutated amino acid. All values were obtained from the AAindex database  \autocite{kawashima2008}}
\end{table}
\end{landscape}

\hypertarget{S2-Appendix}{%
\section{S2 Appendix. (Fisher exact tests)}\label{S2-Appendix}}

\textbf{Fisher exact tests on pairs of mutations.} A detailed explanation of the procedure followed to test pairs of mutations for association with treatment. Detailed numerical results are also given.

In order to study epistasis further we conducted conducted Fisher exact tests between every pair of mutations in the UK dataset (\(n=867,903\)) and the treatment status, corrected the p-values with the Bonferroni method with an overall risk level \(\alpha=0.05\).\\
Out of these tests, \(1,309\) pairs were significantly associated with treatment status. \(424\) out of \(1,309\) these pairs were two known RAMs, \(806\) of these pairs contained one known RAM and only \(79\) tests had pairs involving no known RAM at all. Furthermore out of these \(1,309\) significantly associated pairs, \(829\) contained two mutations that were significantly associated to treatment when testing mutations one by one. In \(478\) pairs, one of the two mutations is associated to treatment on its own, and the remaining 2 pairs, none of the mutations were significantly associated with treatment on their own.\\
These 2 pairs were K103R + V179D and T165I + K173Q. The first pair, is a pair of known RAMs and this interaction is characterized in the HIVDb database (\url{https://hivdb.stanford.edu/dr-summary/comments/NNRTI/}). The second pair is made up of new mutations, and the corrected p-value is \(0.02\). In the Standford HIVDB, T165I has been associated to a reduction in EFV susceptibility.\\
Out of the \(1,309\) pairs significantly associated to treatment, \(151\) contained at least one of our 6 new potential RAMs, in \(6\) cases the pair was made up of 2 of them.\\
In the UK dataset, phylogenetic correlation is likely very impactful with regards to these tests. Indeed, the sequences are far from being independent. In order to alleviate this effect we decided to test the sigficative pairs again on the African dataset, and once more correct with the Bonferroni procedure.\\
Out of the \(1,309\) tests \(294\) have significative p-values after correction. Out of these \(221\) pairs were composed of 2 mutations individually significatively associated with treatment. The remaining \(73\) pairs had one mutation significantly associated with treatment.\\
Out of the \(221\) significative tests, 156 pairs were composed of 2 known RAMS while \(135\) had one known RAM in the pair. The remaining 3 pairs that do not contain a known RAM all contained either L228R or L228H which are both part of our 6 potential RAMS.

\hypertarget{s1-data.}{%
\section{S1 Data.}\label{s1-data.}}

\textbf{Archive of figure generating data.} A zip archive containing the processed data used to generate each panel of the main figures.

\url{https://doi.org/10.1371/journal.pcbi.1008873.s007} (ZIP)

\hypertarget{s2-data.}{%
\section{S2 Data.}\label{s2-data.}}

\textbf{List of known DRMs.} A .csv file containing all the known RAMs used in this project as well as the corresponding feature name in the encoded datasets. Obtained from (\href{https://hivdb.stanford.edu/dr-summary/comments/NRTI/}{hivdb.stanford.edu/dr-summary/comments/NRTI/}) and (\href{https://hivdb.stanford.edu/dr-summary/comments/NNRTI/}{hivdb.stanford.edu/dr-summary/comments/NNRTI/}).

\url{https://doi.org/10.1371/journal.pcbi.1008873.s008} (CSV)

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for Appendix \thechapter}]

\backmatter
\fancyhead[RO,LE]{REFERENCES}

\printbibliography[title=Global References]


\cleardoubleevenpage
{

  \KOMAoptions{twoside = false}
  \pagestyle{empty}

  \section*{Abstract}
  This will be the abstract of my PhD. 
  \lipsum[1-2]

  \section*{Résumé}
  Ceci sera le résumé de la thèse. 
  \lipsum[4-5]
  
}

\end{document}

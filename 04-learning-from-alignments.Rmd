# Learning from sequences and alignments

## Why learn from alignments ?

Sequences and sequence alignments are a very rich source of information. As was sated in Chapters \@ref(aligning-sequence-data) and \@ref(HPC-paper), many downstream analyses rely on sequence alignments.

In whole genome assembly, where sequencing reads are combined together to deduce the sequence genome, pairwise sequence alignment is used in reference-based assembly [@martinNextgenerationTranscriptomeAssembly2011; @kyriakidouCurrentStrategiesPolyploid2018] as well as de novo [@paszkiewiczNovoAssemblyShort2010; @sohnPresentFutureNovo2018] assembly. It has also been used to deduce protein function [@sleatorOverviewSilicoProtein2010]. It has been used for sequence clustering [@sahlinNovoClusteringLongRead2020] as well as detecting genetic [@koboldtBestPracticesVariant2020] and structural variants [@alkanGenomeStructuralVariation2011; @hoStructuralVariationSequencing2020]. Multiple sequence alignments are also very widely used, mainly in phylogenetic analyses where the evolutionary history of a set of sequences are studied and represented as trees [@morrisonPhylogeneticTreebuilding1996; @kapliPhylogeneticTreeBuilding2020], but they have also been used extensively in protein structure prediction [@kuhlmanAdvancesProteinStructure2019].

More recently, as computational power and datasets have grown, more and more machine learning methods are being used on sequence alignments in order to gain biological insight. In this chapter, we will explore how this can be done as an introduction to Chapter \@ref(HIV-paper) where we present an application: predicting HIV drug resistance mutations.

## What to learn ?

-   The first question one might have is what and how can we learn with ML ?

-   There are different learning paradigms that inform what you can learn and predict with your ML method

### Supervised

Here we have extra knowledge -\> labels and we can use his information to train the model.

#### Regression

Here the goal is to predict some type of meaningful continuous numerical value: like a size or a weight. Useful in lots of different fields/

-   predict a quantified resistance level of a virus to a drug (fold change) [@steinerDrugResistancePrediction2020a]

-   predict drug response in cancer (i.e. IC~50~) [@ammad-ud-dinSystematicIdentificationFeature2017], they use genome-wide features pre-extracted from sequences and not sequence directly.

-   predict angles or contact maps in protein structure -\> very active field [@noeMachineLearningProtein2020; @pearceSolutionProteinStructure2021; @tunyasuvunakoolHighlyAccurateProtein2021; @chengMachineLearningMethods2008; @alquraishiMachineLearningProtein2021] either directly from MSAs [@jumperHighlyAccurateProtein2021] or from derived features like contact maps [@wangAccurateNovoPrediction2017]

-   Score MSAs [@ortunoComparingDifferentMachine2015]

-   predicting protein fitness for *in silico* protein engineering [@wittmannAdvancesMachineLearning2021; @yangMachinelearningguidedDirectedEvolution2019; @liCanMachineLearning2019]

-   predicting gene expression levels from SNPs (meaning we need alignments... to a reference) [@xieDeepAutoencoderModel2017]

#### Classification

Here we predict discrete categorical values numerically encoded

-   resistant virus or not [@hagaMachineLearningbasedTreatment2020; @zazziPredictingResponseAntiretroviral2012] (also Chapter \@ref(HIV-paper)) also many studies on antimicrobial resistance [@renPredictionAntimicrobialResistance2022; @kimMachineLearningAntimicrobial2022]

-   Cellular localization of protein [@weiPredictionHumanProtein2018]

-   Secondary structure of amino acid in protein sequence [@jonesProteinSecondaryStructure1999] ($\alpha$, $\beta$ or other)

-   localized gene expression in cells [@kelleyBassetLearningRegulatory2016]

-   splice site detection [@ratschLearningInterpretableSVMs2006]

-   Methylation site prediction [@wangPredictingDNAMethylation2016]

-   Protein function prediction [@wangProteinSequenceProtein2017]

### Unsupervised

-   clustering, align all pairs of sequences for clustering [@zoritaStarcodeSequenceClustering2015], phylogenetics could be considered as a specific type of clustering [@balabanTreeClusterClusteringBiological2019].

-   predict mutational effects [@hopfMutationEffectsPredicted2017]

-   Predict recombination hotspots in humans with a MaxLik approach [@castroModelSelectionApproach2018]

-   Approximating the edit distance and bypassing the need for alignment [@corsoNeuralDistanceEmbeddings2021]

-   dimensionality reduction:

    -   traditionally apply PCA to some type of matrix, like distance matrix obtained from aln [@haschkaMNHNTreeToolsToolboxTree2021] or other matrices like gene expression

    -   Also can be used for clustering [@ben-hurDetectingStableClusters2003; @KmeansClusteringPrincipal; @casariSequencespaceToolFamily1995; @clampJalviewJavaAlignment2004]

    -   Work done to compute PCA directly on alignment [@konishiPrincipalComponentAnalysis2019]

### Others

-   task based: end-to-end training like aligning sequences, this is harder because it requires developing a custom differentiable scoring function based on the task.

    -   Predict P/P interactions [@townshendEndtoEndLearning3D2019]

    -   microRNA target prediction [@leeDeepTargetEndtoendLearning2016]

-   self-supervised, i.e. train on a proxy task and hope the model learns important features and useful stuff to continue on fully-unsupervised.

    -   Used to find useful info in disordered protein regions [@luDiscoveringMolecularFeatures2022]

    -   Protein language models: [@elnaggarProtTransCrackingLanguage2021]

-   semi-supervised, combined a small amount of labelled data with large amount of unlabeled data that the models can leverage.

    -   used to predict drug-protein interactions [@xiaSemisupervisedDrugproteinInteraction2010]

    -   and transmembrane protein topology [@tamposisSemisupervisedLearningHidden2019]

## How to learn ?

Machine learning regroups a multitude of techniques and methods to extract knowledge and make data-driven predictions. In this section we will quickly go over some of these supervised-learning methods, and go into more detail for techniques used in Chapter \@ref(HIV-paper).

### The general supervised learning paradigm

-   All these methods rely on data (here sequences and alignments)

    -   Usually we split available data into training data (on which the learning is done) and testing data (which we can use to asses the model performance). Sometimes we also have validation data used to evaluate the learning process.

    -   Important that each of these datasets are completely separate -\> avoid data leaks [@kaufmanLeakageDataMining2011].

-   Learning -\> optimization process:

    -   A method has an associated cost, i.e. a performance measure that translates how bad the method is doing at predicting the target on the training data (or a score that measure the good)

    -   We minimize the cost (maximize the score) iteratively over the training data.

    -   The cost function is also called loss function

-   Some semantics (however in a lot of literature cost = loss, e.g. [@Goodfellow-et-al-2016]).

    -   Loss = error we give to a single prediction

    -   Cost = sum of losses over the whole training set/batch

    -   Objective function = what we actually optimize, e.g. cost + regularization

-   Example of a classical cost function, many loss functions in ML [@wangComprehensiveSurveyLoss2022]:

    -   $y_i$ = i^th^real value of the target, $\hat{y}_i$ = i^th^predicted value, for binary classification, $p_i$ = probability that i^th^ observation is in positive class.

    -   regression: $RMSE = \sqrt{\frac{\Sigma^{N}_{i=1} (y_i - \hat{y}_i)^2 }{N}}$\

    -   classification: Cross-Entropy for binary classification: $\frac{1}{N}\sum_{i=1}^N-(y\cdot \log(p) + (1-y)\cdot\log(1-p))$

-   Some performance measures many performance measures (especially in classification [@jiaoPerformanceMeasuresEvaluating2016]) :

    -   Regression:

        -   RMSE see above

        -    $MAE=\frac{\Sigma_{i=1}^N \vert y_i - \hat{y}_i \vert}{N}$

    -   Classification:

        -   Accuracy $\frac{TP + TN}{P + N}$

        -   Balanced accuracy $\frac{TPR + TNR}{2}$

```{r, overfittingCaption}
overfittingCaption <- "**Overfitting behaviour in loss functions.**  
The two curves show how the loss calculated on the training set (blue) and the testing set (red) evolve as training time increases. At first both decrease showing that the model learns informative and generalizable features. At some point, training loss keeps decreasing and testing loss increases, meaning that the model is learning over-specific features on the training set and is no longer generalizable: it is overfitting."
```

```{r, overfitting, label="overfitting", fig.cap=overfittingCaption, eval=knitr::is_html_output(), out.width="70%"}
knitr::include_graphics("figures/Encode-seqs/Overfitting.png")
```

```{=tex}
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figures/Encode-seqs/Overfitting.png}
\extcaption{Overfitting behaviour in loss functions.}{The two curves show how the loss calculated on the training set (blue) and the testing set (red) evolve as training time increases. At first both decrease showing that the model learns informative and generalizable features. At some point, training loss keeps decreasing and testing loss increases, meaning that the model is learning over-specific features on the training set and is no longer generalizable: it is overfitting.}
\label{fig:overfitting}
\end{figure}
```
-   Overfitting [@hastieElementsStatisticalLearning2009] :

    -   What is it ?

    -   How to detect it -\> schematic here of training and testing loss ref to Figure \@ref(fig:overfitting)

-   Cross validation, example of cross validation and schematic ?

    -   Figure to show cross validation

### Tests and statistical learning

-   Simple statistical methods have been used for a long time
-   Simplest possible method:
    -   Statistical testing (e.g. Fisher or Student test)

    -   You use 1 feature and see if if informative of the target, or build ensembles (c.f. Chapter \@ref(HIV-paper))

    -   Relatively poor predictive power.
-   More sophisticated is linear and logistic regression, for regression and classification respectively. [@hastieElementsStatisticalLearning2009]
    -   Model: prediction is weighted sum of numerical features + intercept.

    -   In logistic regression this sum is run through logistic function to get threshold for classification

    -   The loss function is RMSE

    -   Gradient descent is used to optimize the cost, Also exact analytical solution in the simplest case

    -   To avoid overfitting- \> regularization where the either ridge [@hoerlRidgeRegressionBiased1970] or lasso [@tibshiraniRegressionShrinkageSelection1996] .
-   Naive Bayes classifier [@hastieElementsStatisticalLearning2009]
    -   Conditional probabilities

    -   Plug in whatever distribution to compute probabilities

    -   Train with maximum likelihood

    -   Violation of principle (i.e. features are rarely independent)

    -   Retains pretty good predictive power [@zhangOptimalityNaiveBayes; @rishEmpiricalStudyNaive] despite the assumptions being violated
-   Other simple methods like KNN are also often used.

### More complex methods, machine learning

-   One of the first that was widely used and achieved good results in many problems: SVM for classification
    -   Optimal separation hyperplane -\> linear separation [@vapnikEstimationDependencesBased1982]

    -   Can perform non-linear classification with the kernel trick in the early 90's [@boserTrainingAlgorithmOptimal1992; @cortesSupportvectorNetworks1995]

    -   adapted to regression [@druckerSupportVectorRegression1996]
-   A very important algorithm RF:
    -   Introduced by Leo Breiman [@breimanRandomForests2001]

    -   Based on the CART decision tree method [@breimanClassificationRegressionTrees1983]

    -   Based on the Gini index / purity index

    -   Select random features when building a tree

    -   Make the trees vote to get prediction

    -   Good performance compared to other methods presented before [@caruanaEmpiricalComparisonSupervised2006a]
-   More complex variants of RF with boosting where features and input examples are weighted before sampling

Deep learning has been use more frequently and more broadly to get good results across a large number of tasks. This is also true in biological contexts, a short introduction to deep learning will be presented in Chapter \@ref(learning-alignments-an-interesting-perspective).

## Preprocessing the alignment for machine learning

In order to do some learning we need to have the data in digestible form

Most of the methods will transform an aligned sequence into a vector, and therefore the alignment as an input matrix.

Some methods do not use the info of alignment in the embedding directly but use in the training process.

### General purpose embeddings

review [@potdarComparativeStudyCategorical2017]

-   Simple labeling / ordinal coding, i.e. assigning an integer to each character (A=1, C=2, G=3, T=4)

    -   pb -\> assigning artificial order to a purely categorical variable, used in [@steinerDrugResistancePrediction2020a]

-   BItwise labeling -\> encode each character with $n$ bits, for nucleotides 2bits if no gaps (A=00, C=01, G=10, T=11) [@dufresneKmerFileFormat2022; @wrightUsingDECIPHERV22016] for AAs 5 bits necessary explored in [@zamaniAminoAcidEncoding2011]

    -   still a problem of order, for aas used in hashing and bloom filters ...

    -   Often used for hashing/indexing a sequence or compressing, but is also a used scheme in categorical variable encoding in machine learning.

-   One-Hot encoding, widely used: a variable with $d$ levels (for DNA $d=4$) is transformed to a length $d$ sparse binary vector, when the variable is equal to the $i^{th}$ level then the resulting vector has a 1 set in the $i^{th}$ position. (also called orthonormal [@singhEvolutionaryBasedOptimal2018])

    -   Used in Chapter \@ref(HIV-paper) and other papers [@budachPyssterClassificationBiological2018],

    -   one of the problems is the "curse of dimensionality" since the for a sequence of length $n$ we get $n\times d$ features, which can be quite large.

    -   Performance between OH and Ordinal can be quite similar [@choongEvaluationConvolutionaryNeural2017] but easily interpretable (important in Bio)

    -   Has been used for encoding protein sequences as early as 1988 [@qianPredictingSecondaryStructure1988]

```{r, generalEncodingCaption}
generalEncodingCaption <- "**Example of 3 general categorical encoding schemes**  
TODO
"
```

```{r, generalEncoding, label="generalEncoding", cache=FALSE, eval=knitr::is_html_output(), fig.cap=generalEncodingCaption, out.width="70%"}
knitr::include_graphics("figures/Encode-seqs/general_purpose.png")
```

```{=tex}
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figures/Encode-seqs/general_purpose.pdf}
\extcaption{Example of 3 general categorical encoding schemes}{TODO}
\label{fig:generalEncoding}
\end{figure}
```
-   Many general purpose categorical encodings, library in Python [@mcginnisScikitLearnContribCategoricalEncodingRelease2018]

### Biological sequence-specific embeddings

These embeddings were inspired by physico-chemical properties, many encodings in [@zamaniAminoAcidEncoding2011]:

-   AAIndex embedding, based on the the AAIndex database [@kawashima2008],

    -   each amino acid has a number real-valued physico-chemical properties so you can select a subset of them.

    -   A subset of properties identified as informative [@liPredictionProteinStructural2008]

    -   Available in the iFeature python package [@chenIFeaturePythonPackage2018]

    -   Also, combine features from AAIndex with PCA to get ensemble features and use top components [@nanniNewEncodingTechnique2011]

-   Groups:

    -   Based on the Venn diagram defined by Taylor [@taylorClassificationAminoAcid1986], grouping AAs by 9 properties (hydrophobic, polar, small, ...)

    -   We can get a 9-D vector for each residue

    -   An encoding based on that was used as early as 1987 to predict protein secondary structure [@zvelebilPredictionProteinSecondary1987]

    -   Later 5 more groups defined [@kremerMethodSystemComputer2009]

-   BLOMAP [@maetschkeBlomapEncodingAmino2005]:

    -   Non-linear projection of the BLOSUM62 substitution matrix to encode an amino acid in 5 dimensions

    -   Used to predict cleavage sites of HIV-1 protease [@singhEvolutionaryBasedOptimal2018]

-   Others:

    -   frequencies of amino acids -\> very coarse (but low dimensional)

    -   $k$-mer frequencies, often reffered to as $n$-gram encoding [@sahaNovelApproachFind2019], often $n=2$ since as $k$/$n$ grow the dimensions grow exponentially -\> with 20 AAs and $20^n$ dimensions per sequence.

    -   OETMAP, extension of BLOMAP [@gokNewFeatureEncoding2013]

    -   Codon graph encoding proposed in [@zamaniAminoAcidEncoding2011], where an AA is represented by a directed graph of all the codons that make up the AA. The 4x4 adjacency matrix is converted into a length 16 vector representing a single AA.

Other weirder embeddings: chaos game theory used to embed sequences:

-   Applications in BIoinfo [@lochelChaosGameRepresentation2021]

-   Applied for phylogenetics:

    -   classify SARS-CoV 2 sequences [@cartesAccurateFastClade2022] w/ DL

    -   [@niApplyingFrequencyChaos2021]

-   Predict AMR [@renPredictionAntimicrobialResistance2022]

In recent years there has been an explosion of learned embeddings, we will not be talking about it here since it is not useful for Chapter \@ref(HIV-paper), I will go over them shortly in Chapter \@ref(learning-alignments-an-interesting-perspective) however.

## Conclusion

-   Alignments and sequences rich in info

-   plenty to learn with ML

-   the choice of embedding / vector representation of sequences in the alignment is not a simple task

-   Guides what you are able to learn

-   Haven't gone over DL -\> chapter 7

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

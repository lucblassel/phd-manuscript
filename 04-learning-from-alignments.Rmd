# Learning from alignments

As in the previous chapter, sequence alignments are rich source of information.\
Pairwise: reference-based [@martinNextgenerationTranscriptomeAssembly2011; @kyriakidouCurrentStrategiesPolyploid2018] and *de novo* [@paszkiewiczNovoAssemblyShort2010; @sohnPresentFutureNovo2018] assembly, protein function prediction [@sleatorOverviewSilicoProtein2010], variant calling [@koboldtBestPracticesVariant2020] and structural variant detection [@alkanGenomeStructuralVariation2011; @hoStructuralVariationSequencing2020], clustering [@sahlinNovoClusteringLongRead2020] MSA: phylogeny [@morrisonPhylogeneticTreebuilding1996; @kapliPhylogeneticTreeBuilding2020], protein structure prediction [@kuhlmanAdvancesProteinStructure2019] etc...

More recently focus on machine learning: to predict drug resistance (cf. Chapter \@ref(HIV-paper)), predict protein structure and function, annotate genes, predict protein location, GWAS...

For ML and DL, we need to transform sequences and alignments into numerical vectors.

## Preprocessing the alignment for machine learning

In order to do some learning we need to have the data in digestible form

We need a way to represent, the position and the character in a sequence

### General purpose embeddings

review [@potdarComparativeStudyCategorical2017]

-   Simple labeling / ordinal coding, i.e. assigning an integer to each character (A=1, C=2, G=3, T=4)

    -   pb -\> assigning artificial order to a purely categorical variable, used in [@steinerDrugResistancePrediction2020a]

-   BItwise labeling -\> encode each character with $n$ bits, for nucleotides 2bits if no gaps (A=00, C=01, G=10, T=11) [@dufresneKmerFileFormat2022; @wrightUsingDECIPHERV22016] for AAs 5 bits necessary explored in [@zamaniAminoAcidEncoding2011]

    -   still a problem of order, for aas used in hashing and bloom filters ...

    -   Often used for hashing/indexing a sequence or compressing, but is also a used scheme in categorical variable encoding in machine learning.

-   One-Hot encoding, widely used: a variable with $d$ levels (for DNA $d=4$) is transformed to a length $d$ sparse binary vector, when the variable is equal to the $i^{th}$ level then the resulting vector has a 1 set in the $i^{th}$ position. (also called orthonormal)

    -   Used in Chapter \@ref(HIV-paper) and other papers [@budachPyssterClassificationBiological2018],

    -   one of the problems is the "curse of dimensionality" since the for a sequence of length $n$ we get $n\times d$ features, which can be quite large.

    -   Performance between OH and Ordinal can be quite similar [@choongEvaluationConvolutionaryNeural2017] but easily interpretable (important in Bio)

    -   Has been used for encoding protein sequences as early as 1988 [@qianPredictingSecondaryStructure1988]

```{r, generalEncodingCaption}
generalEncodingCaption <- "**Example of 3 general categorical encoding schemes**\
"
```

```{r, generalEncoding, label="generalEncoding", cache=FALSE, eval=knitr::is_html_output(), fig.cap=generalEncodingCaption, out.width="70%"}
knitr::include_graphics("figures/Encode-seqs/general_purpose.png")
```

```{=tex}
\begin{figure}
\centering
\includegraphics[with=0.7\textwidth]{figures/Encode-seqs/general_purpose.pdf}
\extcaption{Example of 3 general categorical encoding schemes}{TODO}
\label{fig:generalEncoding}
\end{figure}
```
-   Many general purpose categorical encodings, library in Python [@mcginnisScikitLearnContribCategoricalEncodingRelease2018]

### Biological sequence-specific embeddings

These embeddings were inspired by physico-chemical properties, many encodings in [@zamaniAminoAcidEncoding2011]:

-   AAIndex embedding, based on the the AAIndex database [@kawashima2008],

    -   each amino acid has a number real-valued physico-chemical properties so you can select a subset of them.

    -   A subset of properties identified as informative [@liPredictionProteinStructural2008]

    -    Available in the iFeature python package [@chenIFeaturePythonPackage2018]

-   BLOMAP [@maetschkeBlomapEncodingAmino2005]:

    -   Non-linear projection of the BLOSUM62 substitution matrix to encode an amino acid in 5 dimensions

    -   Used to predict cleavage sites of HIV-1 protease [@singhEvolutionaryBasedOptimal2018]

-   Groups

-   Others:

    -   frequencies of amino acids -\> very coarse (but low dimensional)

    -   2grams -\>

### Learned embeddings

dna2vec [@ngDna2vecConsistentVector2017]

language models, transformers, etc...

Powerful but hard to interpret what the model actually learns. i.e. "black box"

Other weirder embeddings: chaos game theory [@lochelChaosGameRepresentation2021]

## What to learn ?

Of course one we have the data in digestible form we need an objective, a goal and once again a multitude

### Supervised

Here we have extra knowledge -\> labels and we can use his information to train the model.

#### Regression

Here the goal is to predict some type of meaningful continuous numerical value: like a size or a weight. Useful in lots of different fields/

-   predict a quantified resistance level of a virus to a drug (fold change) [@steinerDrugResistancePrediction2020a]

-   predict drug response in cancer (i.e. IC~50~) [@ammad-ud-dinSystematicIdentificationFeature2017], they use genome-wide features pre-extracted from sequences and not sequence directly.

-   predict angles or contact maps in protein structure -\> very active field [@noeMachineLearningProtein2020; @pearceSolutionProteinStructure2021; @tunyasuvunakoolHighlyAccurateProtein2021; @chengMachineLearningMethods2008; @alquraishiMachineLearningProtein2021] either directly from MSAs [@jumperHighlyAccurateProtein2021] or from derived features like contact maps [@wangAccurateNovoPrediction2017]

-   Score MSAs [@ortunoComparingDifferentMachine2015]

-   predicting protein fitness for *in silico* protein engineering [@wittmannAdvancesMachineLearning2021; @yangMachinelearningguidedDirectedEvolution2019; @liCanMachineLearning2019]

-   predicting gene expression levels from SNPs (meaning we need alignments... to a reference) [@xieDeepAutoencoderModel2017]

#### Classification

Here we predict discrete categorical values numerically encoded

-   resistant virus or not [@hagaMachineLearningbasedTreatment2020; @zazziPredictingResponseAntiretroviral2012] (also Chapter \@ref(HIV-paper)) also many studies on antimicrobial resistance [@renPredictionAntimicrobialResistance2022; @kimMachineLearningAntimicrobial2022]

-   Cellular localization of protein [@weiPredictionHumanProtein2018]

-   Secondary structure of amino acid in protein sequence [@jonesProteinSecondaryStructure1999] ($\alpha$, $\beta$ or other)

-   localized gene expression in cells [@kelleyBassetLearningRegulatory2016]

-   splice site detection [@ratschLearningInterpretableSVMs2006]

-   Methylation site prediction [@wangPredictingDNAMethylation2016]

-   Protein function prediction [@wangProteinSequenceProtein2017]

### Unsupervised

-   clustering, align all pairs of sequences for clustering [@zoritaStarcodeSequenceClustering2015], phylogenetics could be considered as a specific type of clustering [@balabanTreeClusterClusteringBiological2019].

-   predict mutational effects [@hopfMutationEffectsPredicted2017]

-   Predict recombination hotspots in humans with a MaxLik approach [@castroModelSelectionApproach2018]

-   Approximating the edit distance and bypassing the need for alignment [@corsoNeuralDistanceEmbeddings2021]

-   dimensionality reduction:

    -   traditionally apply PCA to some type of matrix, like distance matrix obtained from aln [@haschkaMNHNTreeToolsToolboxTree2021] or other matrices like gene expression

    -   Also can be used for clustering [@ben-hurDetectingStableClusters2003; @KmeansClusteringPrincipal; @casariSequencespaceToolFamily1995; @clampJalviewJavaAlignment2004]

    -   Work done to compute PCA directly on alignment [@konishiPrincipalComponentAnalysis2019]

### Others

-   task based: end-to-end training like aligning sequences, this is harder because it requires developing a custom differentiable scoring function based on the task.

    -   Predict P/P interactions [@townshendEndtoEndLearning3D2019]

    -   microRNA target prediction [@leeDeepTargetEndtoendLearning2016]

-   self-supervised, i.e. train on a proxy task and hope the model learns important features and useful stuff to continue on fully-unsupervised.

    -   Used to find useful info in disordered protein regions [@luDiscoveringMolecularFeatures2022]

    -   Protein language models: [@elnaggarProtTransCrackingLanguage2021]

-   semi-supervised, combined a small amount of labelled data with large amount of unlabeled data that the models can leverage.

    -   used to predict drug-protein interactions [@xiaSemisupervisedDrugproteinInteraction2010]

    -   and transmembrane protein topology [@tamposisSemisupervisedLearningHidden2019]

## How to learn ?

### Tests and statistical learning

-   correlation
-   Fisher
-   Multiple testing ?
-   linear regression / logistic regression

### More complex methods, machine learning

-   regularization
-   RF
-   SVM
-   ABC
-   KNN

### Deep Learning

-   Steiner et al...
-   plenty of other refs (DRMs + ML section from our minireview in Current opinions in virology 2021)
-   Complex architectures

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

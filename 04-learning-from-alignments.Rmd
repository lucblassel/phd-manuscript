# Learning from alignments

As in the previous chapter, sequence alignments are rich source of information.\
Pairwise: assembly, protein function prediction, MSA: clustering, phylogeny, etc...

More recently focus on machine learning: to predict drug resistance (cf. Chapter \@ref(HIV-paper)), predict protein structure and function, annotate genes, predict protein location, GWAS...

For ML and DL, we need to transform sequences and alignments into numerical vectors.

## Preprocessing the alignment for machine learning

In order to do some learning we need to have the data in digestible form

We need a way to represent, the position and the character in a sequence

### Physico-chemical embeddings

AAIndex, or other embeddings, we add extra info, but we also make a string choice when deciding what features to add

### Generalistic categorical embeddings

One-Hot, etc..., easily interpretable...

### Learned embeddings

language models, transformers, etc...

Powerful but hard to interpret what the model actually learns. i.e. "black box"

Other weirder embeddings: chaos game theory

## What to learn ?

Of course one we have the data in digestible form we need an objective, a goal and once again a multitude

### Supervised

Here we have extra knowledge -\> labels and we can use his information to train the model.

#### Regression

Here the goal is to predict some type of meaningful continuous numerical value: like a size or a weight. Useful in lots of different fields/

-   predict a quantified resistance level of a virus to a drug (fold change) [@steinerDrugResistancePrediction2020a]

-   predict drug response in cancer (i.e. IC~50~) [@ammad-ud-dinSystematicIdentificationFeature2017], they use genome-wide features pre-extracted from sequences and not sequence directly.

-   predict angles or contact maps in protein structure -\> very active field [@noeMachineLearningProtein2020; @pearceSolutionProteinStructure2021; @tunyasuvunakoolHighlyAccurateProtein2021; @chengMachineLearningMethods2008; @alquraishiMachineLearningProtein2021] either directly from MSAs [@jumperHighlyAccurateProtein2021] or from derived features like contact maps [@wangAccurateNovoPrediction2017]

-   Score MSAs [@ortunoComparingDifferentMachine2015]

-   predicting protein fitness for *in silico* protein engineering [@wittmannAdvancesMachineLearning2021; @yangMachinelearningguidedDirectedEvolution2019; @liCanMachineLearning2019]

-   predicting gene expression levels from SNPs (meaning we need alignments... to a reference) [@xieDeepAutoencoderModel2017]

#### Classification

Here we predict discrete categorical values numerically encoded

-   resistant virus or not [@hagaMachineLearningbasedTreatment2020; @zazziPredictingResponseAntiretroviral2012] (also Chapter \@ref(HIV-paper)) also many studies on antimicrobial resistance [@renPredictionAntimicrobialResistance2022; @kimMachineLearningAntimicrobial2022]

-   Cellular localization of protein [@weiPredictionHumanProtein2018]

-   Secondary structure of amino acid in protein sequence [@jonesProteinSecondaryStructure1999] ($\alpha$, $\beta$ or other)

-   localized gene expression in cells [@kelleyBassetLearningRegulatory2016]

-   splice site detection [@ratschLearningInterpretableSVMs2006]

-   Methylation site prediction [@wangPredictingDNAMethylation2016]

-   Protein function prediction [@wangProteinSequenceProtein2017]

### Unsupervised

-   clustering, align all pairs of sequences for clustering [@zoritaStarcodeSequenceClustering2015], phylogenetics could be considered as a specific type of clustering [@balabanTreeClusterClusteringBiological2019].

-   predict mutational effects [@hopfMutationEffectsPredicted2017]

-   Predict recombination hotspots in humans with a MaxLik approach [@castroModelSelectionApproach2018]

-   Approximating the edit distance and bypassing the need for alignment [@corsoNeuralDistanceEmbeddings2021]

-   dimensionality reduction:

    -   traditionally apply PCA to some type of matrix, like distance matrix obtained from aln [@haschkaMNHNTreeToolsToolboxTree2021] or other matrices like gene expression

    -   Also can be used for clustering [@ben-hurDetectingStableClusters2003; @KmeansClusteringPrincipal; @casariSequencespaceToolFamily1995; @clampJalviewJavaAlignment2004]

    -   Work done to compute PCA directly on alignment [@konishiPrincipalComponentAnalysis2019]

### Others

-   task based: end-to-end training like aligning sequences, this is harder because it requires developing a custom differentiable scoring function based on the task.

    -   Predict P/P interactions [@townshendEndtoEndLearning3D2019]

    -   microRNA target prediction [@leeDeepTargetEndtoendLearning2016]

-   self-supervised, i.e. train on a proxy task and hope the model learns important features and useful stuff to continue on fully-unsupervised.

    -   Used to find useful info in disordered protein regions [@luDiscoveringMolecularFeatures2022]

    -   Protein language models: [@elnaggarProtTransCrackingLanguage2021]

-   semi-supervised, combined a small amount of labelled data with large amount of unlabeled data that the models can leverage.

    -   used to predict drug-protein interactions [@xiaSemisupervisedDrugproteinInteraction2010]

    -   and transmembrane protein topology [@tamposisSemisupervisedLearningHidden2019]

## How to learn ?

### Tests and statistical learning

-   correlation
-   Fisher
-   Multiple testing ?

### More complex methods, machine learning

-   Regressions w/ regularization
-   RF
-   SVM
-   ABC

### Deep Learning

-   Steiner et al...
-   plenty of other refs (DRMs + ML section from our minireview in Current opinions in virology 2021)

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]

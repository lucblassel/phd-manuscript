# Learning from sequences and alignments

## Why learn from alignments ? 

Sequences and sequence alignments are a very rich source of information. As was sated in Chapters \@ref(aligning-sequence-data) and \@ref(HPC-paper), many downstream analyses rely on sequence alignments.

In whole genome assembly, where sequencing reads are combined together to deduce the sequence genome, pairwise sequence alignment is used in reference-based assembly [@martinNextgenerationTranscriptomeAssembly2011; @kyriakidouCurrentStrategiesPolyploid2018] as well as de novo [@paszkiewiczNovoAssemblyShort2010; @sohnPresentFutureNovo2018] assembly. It has also been used to deduce protein function [@sleatorOverviewSilicoProtein2010], and structure **[REF]**. It has been used for sequence clustering [@sahlinNovoClusteringLongRead2020] as well as detecting genetic [@koboldtBestPracticesVariant2020] and structural variants [@alkanGenomeStructuralVariation2011; @hoStructuralVariationSequencing2020]. Multiple sequence alignments are also very widely used, mainly in phylogenetic analyses where the evolutionary history of a set of sequences are studied and represented as trees [@morrisonPhylogeneticTreebuilding1996; @kapliPhylogeneticTreeBuilding2020], but they have also been used extensively in protein structure prediction [@kuhlmanAdvancesProteinStructure2019].

More recently, as computational power and datasets have grown, more and more machine learning methods are being used on sequence alignments in order to gain biological insight. In this chapter, we will explore how this can be done as an introduction to Chapter \@ref(HIV-paper) where we present an application: predicting HIV drug resistance mutations.

## What to learn ?

-   The first question one might have is what and how can we learn with ML ?

-   There are different learning paradigms that inform what you can learn and predict with your ML method

### Supervised

Here we have extra knowledge -\> labels and we can use his information to train the model.

#### Regression

Here the goal is to predict some type of meaningful continuous numerical value: like a size or a weight. Useful in lots of different fields/

-   predict a quantified resistance level of a virus to a drug (fold change) [@steinerDrugResistancePrediction2020a]

-   predict drug response in cancer (i.e. IC~50~) [@ammad-ud-dinSystematicIdentificationFeature2017], they use genome-wide features pre-extracted from sequences and not sequence directly.

-   predict angles or contact maps in protein structure -\> very active field [@noeMachineLearningProtein2020; @pearceSolutionProteinStructure2021; @tunyasuvunakoolHighlyAccurateProtein2021; @chengMachineLearningMethods2008; @alquraishiMachineLearningProtein2021] either directly from MSAs [@jumperHighlyAccurateProtein2021] or from derived features like contact maps [@wangAccurateNovoPrediction2017]

-   Score MSAs [@ortunoComparingDifferentMachine2015]

-   predicting protein fitness for *in silico* protein engineering [@wittmannAdvancesMachineLearning2021; @yangMachinelearningguidedDirectedEvolution2019; @liCanMachineLearning2019]

-   predicting gene expression levels from SNPs (meaning we need alignments... to a reference) [@xieDeepAutoencoderModel2017]

#### Classification

Here we predict discrete categorical values numerically encoded

-   resistant virus or not [@hagaMachineLearningbasedTreatment2020; @zazziPredictingResponseAntiretroviral2012] (also Chapter \@ref(HIV-paper)) also many studies on antimicrobial resistance [@renPredictionAntimicrobialResistance2022; @kimMachineLearningAntimicrobial2022]

-   Cellular localization of protein [@weiPredictionHumanProtein2018]

-   Secondary structure of amino acid in protein sequence [@jonesProteinSecondaryStructure1999] ($\alpha$, $\beta$ or other)

-   localized gene expression in cells [@kelleyBassetLearningRegulatory2016]

-   splice site detection [@ratschLearningInterpretableSVMs2006]

-   Methylation site prediction [@wangPredictingDNAMethylation2016]

-   Protein function prediction [@wangProteinSequenceProtein2017]

### Unsupervised

-   clustering, align all pairs of sequences for clustering [@zoritaStarcodeSequenceClustering2015], phylogenetics could be considered as a specific type of clustering [@balabanTreeClusterClusteringBiological2019].

-   predict mutational effects [@hopfMutationEffectsPredicted2017]

-   Predict recombination hotspots in humans with a MaxLik approach [@castroModelSelectionApproach2018]

-   Approximating the edit distance and bypassing the need for alignment [@corsoNeuralDistanceEmbeddings2021]

-   dimensionality reduction:

    -   traditionally apply PCA to some type of matrix, like distance matrix obtained from aln [@haschkaMNHNTreeToolsToolboxTree2021] or other matrices like gene expression

    -   Also can be used for clustering [@ben-hurDetectingStableClusters2003; @KmeansClusteringPrincipal; @casariSequencespaceToolFamily1995; @clampJalviewJavaAlignment2004]

    -   Work done to compute PCA directly on alignment [@konishiPrincipalComponentAnalysis2019]

### Others

-   task based: end-to-end training like aligning sequences, this is harder because it requires developing a custom differentiable scoring function based on the task.

    -   Predict P/P interactions [@townshendEndtoEndLearning3D2019]

    -   microRNA target prediction [@leeDeepTargetEndtoendLearning2016]

-   self-supervised, i.e. train on a proxy task and hope the model learns important features and useful stuff to continue on fully-unsupervised.

    -   Used to find useful info in disordered protein regions [@luDiscoveringMolecularFeatures2022]

    -   Protein language models: [@elnaggarProtTransCrackingLanguage2021]

-   semi-supervised, combined a small amount of labelled data with large amount of unlabeled data that the models can leverage.

    -   used to predict drug-protein interactions [@xiaSemisupervisedDrugproteinInteraction2010]

    -   and transmembrane protein topology [@tamposisSemisupervisedLearningHidden2019]

## How to learn ?

Machine learning regroups a multitude of techniques and methods to extract knowledge and make data-driven predictions. In this section we will quickly go over some of these supervised-learning methods, and go into more detail for techniques used in Chapter \@ref(HIV-paper).

### The general supervized learning paradigm

-   All these methods rely on data (here sequences and alignments)

    -   Usually we split available data into training data (on which the learning is done) and testing data (which we can use to asses the model performance). Sometimes we also have validation data used to evaluate the learning process.

    -   Important that each of these datasets are completely separate -\> avoid data leaks.

-   Learning -\> optimization process:

    -   A method has an associated cost, i.e. a performance measure that translates how bad the method is doing at predicting the target on the training data (or a score that measure the good)

    -   We minimize the cost (maximize the score) iteratively over the training data.

    -   The cost function is also called loss function

### Tests and statistical learning

-   Simple statistical methods have been used for a long time
-   Simplest possible method:
    -   Statistical testing (e.g. Fisher or Student test)

    -   You use 1 feature and see if if informative of the target, or build ensembles (c.f. Chapter \@ref(HIV-paper))

    -   Relatively poor predictive power.
-   More sophisticated is linear and logistic regression, for regression and classification respectively.
    -   Model: prediction is weighted sum of numerical features + intercept.

    -   In logistic regression this sum is run through logistic function to get threshold for classification

    -   The loss function is RMSE

    -   Gradient descent is used to optimize the cost, Also exact analytical solution in the simplest case
-   Naive Bayes classifier [@hastieElementsStatisticalLearning2009]
    -   Conditional probabilities

    -   Plug in whatever distribution to compute probabilities

    -   Train with maximum likelihood

    -   Violation of principle (i.e. features are rarely independent)

    -   Pretty good predictive power [@zhangOptimalityNaiveBayes]
-   KNN: simple but effective
    -   main method

    -   dependent on k and the distance metric

    -   Quadratic so data structures (e.g. kd-trees) for efficient querying or approximation methods like LSH.

### More complex methods, machine learning

-   SVM: mainly for classification
    -   Optimal separation hyperplane -\> linear separation [@vapnikEstimationDependencesBased1982]

    -   Can perform non-linear classification with the kernel trick in the early 90's [@boserTrainingAlgorithmOptimal1992; @cortesSupportvectorNetworks1995]

    -   adapted to regression [@druckerSupportVectorRegression1996]
-   RF:
    -   Introduced by Leo Breiman [@breimanRandomForests2001]

    -   Based on the CART decision tree method

    -   Based on the Gini index / purity index

    -   Select random features when building a tree

    -   Make the trees vote to get prediction

    -   More complex variants with boosting as well

### Deep Learning

-   Steiner et al...
-   plenty of other refs (DRMs + ML section from our minireview in Current opinions in virology 2021)
-   Complex architectures

## Preprocessing the alignment for machine learning

In order to do some learning we need to have the data in digestible form

Most of the methods will transform an aligned sequence into a vector, and therefore the alignment as an input matrix.

Some methods are designed to not required alignment, but I will present some anyway

### General purpose embeddings

review [@potdarComparativeStudyCategorical2017]

-   Simple labeling / ordinal coding, i.e. assigning an integer to each character (A=1, C=2, G=3, T=4)

    -   pb -\> assigning artificial order to a purely categorical variable, used in [@steinerDrugResistancePrediction2020a]

-   BItwise labeling -\> encode each character with $n$ bits, for nucleotides 2bits if no gaps (A=00, C=01, G=10, T=11) [@dufresneKmerFileFormat2022; @wrightUsingDECIPHERV22016] for AAs 5 bits necessary explored in [@zamaniAminoAcidEncoding2011]

    -   still a problem of order, for aas used in hashing and bloom filters ...

    -   Often used for hashing/indexing a sequence or compressing, but is also a used scheme in categorical variable encoding in machine learning.

-   One-Hot encoding, widely used: a variable with $d$ levels (for DNA $d=4$) is transformed to a length $d$ sparse binary vector, when the variable is equal to the $i^{th}$ level then the resulting vector has a 1 set in the $i^{th}$ position. (also called orthonormal [@singhEvolutionaryBasedOptimal2018])

    -   Used in Chapter \@ref(HIV-paper) and other papers [@budachPyssterClassificationBiological2018],

    -   one of the problems is the "curse of dimensionality" since the for a sequence of length $n$ we get $n\times d$ features, which can be quite large.

    -   Performance between OH and Ordinal can be quite similar [@choongEvaluationConvolutionaryNeural2017] but easily interpretable (important in Bio)

    -   Has been used for encoding protein sequences as early as 1988 [@qianPredictingSecondaryStructure1988]

```{r, generalEncodingCaption}
generalEncodingCaption <- "**Example of 3 general categorical encoding schemes**  
TODO
"
```

```{r, generalEncoding, label="generalEncoding", cache=FALSE, eval=knitr::is_html_output(), fig.cap=generalEncodingCaption, out.width="70%"}
knitr::include_graphics("figures/Encode-seqs/general_purpose.png")
```

```{=tex}
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figures/Encode-seqs/general_purpose.pdf}
\extcaption{Example of 3 general categorical encoding schemes}{TODO}
\label{fig:generalEncoding}
\end{figure}
```
-   Many general purpose categorical encodings, library in Python [@mcginnisScikitLearnContribCategoricalEncodingRelease2018]

### Biological sequence-specific embeddings

These embeddings were inspired by physico-chemical properties, many encodings in [@zamaniAminoAcidEncoding2011]:

-   AAIndex embedding, based on the the AAIndex database [@kawashima2008],

    -   each amino acid has a number real-valued physico-chemical properties so you can select a subset of them.

    -   A subset of properties identified as informative [@liPredictionProteinStructural2008]

    -   Available in the iFeature python package [@chenIFeaturePythonPackage2018]

    -   Also, combine features from AAIndex with PCA to get ensemble features and use top components [@nanniNewEncodingTechnique2011]

-   Groups:

    -   Based on the Venn diagram defined by Taylor [@taylorClassificationAminoAcid1986], grouping AAs by 9 properties (hydrophobic, polar, small, ...)

    -   We can get a 9-D vector for each residue

    -   An encoding based on that was used as early as 1987 to predict protein secondary structure [@zvelebilPredictionProteinSecondary1987]

    -   Later 5 more groups defined [@kremerMethodSystemComputer2009]

-   BLOMAP [@maetschkeBlomapEncodingAmino2005]:

    -   Non-linear projection of the BLOSUM62 substitution matrix to encode an amino acid in 5 dimensions

    -   Used to predict cleavage sites of HIV-1 protease [@singhEvolutionaryBasedOptimal2018]

-   Others:

    -   frequencies of amino acids -\> very coarse (but low dimensional)

    -   $k$-mer frequencies, often reffered to as $n$-gram encoding [@sahaNovelApproachFind2019], often $n=2$ since as $k$/$n$ grow the dimensions grow exponentially -\> with 20 AAs and $20^n$ dimensions per sequence.

    -   OETMAP, extension of BLOMAP [@gokNewFeatureEncoding2013]

    -   Codon graph encoding proposed in [@zamaniAminoAcidEncoding2011], where an AA is represented by a directed graph of all the codons that make up the AA. The 4x4 adjacency matrix is converted into a length 16 vector representing a single AA.

### Learned embeddings

-   (Variational) Auto-encoders:

    -   Bottlneck in deep neural neck, task is to predict input. Add noise in the hidden layers -\> remove noise or regularize to have smooth latent space and get embeddings
    -   Used for ancestral sequence reconstruction [@moretaAncestralProteinSequence2022] and estimating evolutionary distances [@corsoNeuralDistanceEmbeddings2021]
    -   VAEs used for sequence design as well [@wuProteinSequenceDesign2021; @stantonAcceleratingBayesianOptimization2022]

NLP:

-   From the field of natural language processing where very high dimensionality (470,000 words in the Merriam-Webster English dictionary [@HowManyWords], so naive one hot is out of the question), we need other ways to transform words into sequences.

-   Method of pre-training embedding methods

-   Word2Vec derivatives:

    -   word2Vec [@mikolovEfficientEstimationWord2013; @mikolovDistributedRepresentationsWords2013], take in a large corpus of text and learns a vector space from it. Then each word in the corpus can be assigned a vector, constraints mean that similar words have similar vectors (i.e. low distance in the vector space). And that the embeddings make sense grammatically (e.g. of the Paper $vec(Madrid) - vec(Spain)$ should be close to $vec(Paris)$ in the learned space.

        -   Context of a word = window of $k$ words centered around it

        -   The model is a neural network and the hidden layer corresponds to the embedding (similar to auto-encoders)

        -   2 ways to train it [@goldbergWord2vecExplainedDeriving2014]:

            -   CBOW (continuous bag of words) = predict word from context

            -   skip-gram = predict context from word

    -   dna2vec [@ngDna2vecConsistentVector2017]

        -   Used to predict methylation sites [@liangHyb4mCHybridDNA2vecbased2022]

    -   seq2vec [@kimothiDistributedRepresentationsBiological2016]

    -   BioVec/ProtVec/GeneVec [@asgariContinuousDistributedRepresentation2015]

        -   Seq2vec and ProtVec both used in classification [@kimothiMetricLearningBiological2017]

-   Transformers / NN-based language models:

    -   Also from NLP, more recent development,

        -   Some have seen a lot of success like BERT [@devlinBERTPretrainingDeep2019] and GPT-3 [@brownLanguageModelsAre2020]
        -   Based on the very popular Transformer architecture [@vaswaniAttentionAllYou2017], with attention maps. Embed features as a linear weighted sum of other features (learn weights).
        -   Allows for long range dependencies to be captured efficiently
        -   LLMs trained with MLM
        -   Replaced methods based on RNNs / LSTMs which have trouble capturing long range dependencies [@songPretrainingModelBiological2021].

    -   protein language models have been developed from this with the same idea.

        -   ProGen [@madaniProGenLanguageModeling2020] and ProGen2 [@eriknijkampProGen2ExploringBoundaries2022]

        -   ProtBERT [@elnaggarProtTransCrackingLanguage2021]

        -   DNABert [@jiDNABERTPretrainedBidirectional2021]

        -   They have interesting properties [@beplerLearningProteinLanguage2021]:

            -   Intuitively learn structure of proteins [@raoTransformerProteinLanguage2020; @rivesBiologicalStructureFunction2019]

            -   Learn mutational effects [@meierLanguageModelsEnable2021]

            -   Evolutionary characteristics [@hieEvolutionaryVelocityProtein2022]

        -   To counter the space limitations (i.e. sequence length limitations) induced by attention, other types of transformers used, with linear scale attention maps not quadratic [@choromanskiMaskedLanguageModeling2020]

    -   Used to embed sequences from MSAs and find homologies [@caiGenomewidePredictionSmall2020]

    -   MSA Transformer

-   Powerful but hard to interpret what the model actually learns. i.e. "black box" but some work is being done to interpret attention maps [@vigBERTologyMeetsBiology2021]

Other weirder embeddings: chaos game theory [@lochelChaosGameRepresentation2021]

## Conclusion

-   Alignment rich in info

-   plenty to learn with ML

-   the choice of embedding / vector representation of sequences in the alignment is not a simple task

-   Guides what you are able to learn

\printbibliography[segment=\therefsegment,heading=subbibintoc,title={References for chapter \thechapter}]
